# RealtimeTTS
[![PyPI](https://img.shields.io/pypi/v/RealtimeTTS)](https://pypi.org/project/RealtimeTTS/)
[![TÃ©lÃ©chargements](https://static.pepy.tech/badge/RealtimeTTS)](https://pepy.tech/project/KoljaB/RealtimeTTS)
[![Version GitHub](https://img.shields.io/github/release/KoljaB/RealtimeTTS.svg)](https://GitHub.com/KoljaB/RealtimeTTS/releases/)
[![GitHub commits](https://badgen.net/github/commits/KoljaB/RealtimeTTS)](https://GitHub.com/Naereen/KoljaB/RealtimeTTS/commit/)
[![GitHub forks](https://img.shields.io/github/forks/KoljaB/RealtimeTTS.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/KoljaB/RealtimeTTS/network/)
[![GitHub Ã©toiles](https://img.shields.io/github/stars/KoljaB/RealtimeTTS.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/KoljaB/RealtimeTTS/stargazers/)

*BibliothÃ¨que de synthÃ¨se vocale Ã  faible latence et facile Ã  utiliser pour les applications en temps rÃ©el*

## Ã€ propos du projet

RealtimeTTS est une bibliothÃ¨que de synthÃ¨se vocale (TTS) de pointe conÃ§ue pour les applications en temps rÃ©el. Elle se distingue par sa capacitÃ© Ã  convertir rapidement des flux de texte en sortie auditive de haute qualitÃ© avec une latence minimale.

> **ImportantÂ :** [Installation](#installation) a changÃ© pour permettre plus de personnalisation. Veuillez utiliser `pip install realtimetts[all]` au lieu de `pip install realtimetts` maintenant. Plus d'informations ici](#installation).

> **AstuceÂ :** *<strong>Consultez [Linguflex](https://github.com/KoljaB/Linguflex)</strong>, le projet original dont RealtimeTTS est issu. Il vous permet de contrÃ´ler votre environnement en parlant et est l'un des assistants open source les plus performants et les plus sophistiquÃ©s actuellement disponibles.*

https://github.com/KoljaB/RealtimeTTS/assets/7604638/87dcd9a5-3a4e-4f57-be45-837fc63237e7

## Principales fonctionnalitÃ©s

- **Faible latence**
- conversion texte-parole quasi instantanÃ©e
- compatible avec les sorties LLM
- **Audio de haute qualitÃ©**
- gÃ©nÃ¨re une parole claire et naturelle
- **Prise en charge de plusieurs moteurs TTS**
- prend en charge OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS et System TTS
- **Multilingue**
- **Robuste et fiable**Â :
- assure un fonctionnement continu grÃ¢ce Ã  un mÃ©canisme de secours
- bascule vers des moteurs alternatifs en cas de perturbations garantissant des performances et une fiabilitÃ© constantes, ce qui est essentiel pour une utilisation critique et professionnelle cas

> **Astuce**Â : *consultez [RealtimeSTT](https://github.com/KoljaB/RealtimeSTT), l'Ã©quivalent d'entrÃ©e de cette bibliothÃ¨que, pour les capacitÃ©s de conversion de la parole en texte. Ensemble, ils forment un puissant wrapper audio en temps rÃ©el autour de grands modÃ¨les de langage.*

## FAQ

Consultez la [page FAQ](./FAQ.md) pour obtenir des rÃ©ponses Ã  de nombreuses questions sur l'utilisation de RealtimeTTS.

## Mises Ã  jour

DerniÃ¨re versionÂ : v0.4.7

Voir [historique des versions](https://github.com/KoljaB/RealtimeTTS/releases).

## Pile technologique

Cette bibliothÃ¨que utiliseÂ :

- **Moteurs de synthÃ¨se vocale**
- **OpenAIEngine**Â : le systÃ¨me TTS d'OpenAI offre 6 voix au son naturel.
- **CoquiEngine**Â : TTS neuronal local de haute qualitÃ©.
- **AzureEngine**Â : la technologie TTS de pointe de Microsoft. 500 000 caractÃ¨res gratuits par mois.
- **ElevenlabsEngine** : offre les meilleures voix disponibles.
- **GTTSEngine** : utilisation gratuite et ne nÃ©cessite pas la configuration d'un GPU local.
- **SystemEngine** : moteur natif pour une configuration rapide.

- **Sentence Boundary Detection**
- **NLTK Sentence Tokenizer** : tokenizer de phrases de Natural Language Toolkit pour des tÃ¢ches de synthÃ¨se vocale simples en anglais ou lorsque la simplicitÃ© est prÃ©fÃ©rÃ©e.
- **Stanza Sentence Tokenizer** : tokenizer de phrases Stanza pour travailler avec du texte multilingue ou lorsqu'une prÃ©cision et des performances supÃ©rieures sont requises.

*En utilisant des composants Â« standards de l'industrie Â», RealtimeTTS offre une base technologique fiable et haut de gamme pour le dÃ©veloppement de solutions vocales avancÃ©es.*

## Installation

> **RemarqueÂ :** l'installation de base avec `pip install realtimetts` n'est plus recommandÃ©e, utilisez plutÃ´t `pip install realtimetts[all]`.

La bibliothÃ¨que RealtimeTTS fournit des options d'installation pour diverses dÃ©pendances pour votre cas d'utilisation. Voici les diffÃ©rentes maniÃ¨res d'installer RealtimeTTS en fonction de vos besoinsÂ :

### Installation complÃ¨te

Pour installer RealtimeTTS avec prise en charge de tous les moteurs TTSÂ :

```bash
pip install -U realtimetts[all]
```

### Installation personnalisÃ©e

RealtimeTTS permet une installation personnalisÃ©e avec des installations de bibliothÃ¨que minimales. Voici les options disponiblesÂ :
- **all**Â : installation complÃ¨te avec prise en charge de tous les moteurs.
- **system**Â : inclut des fonctionnalitÃ©s TTS spÃ©cifiques au systÃ¨me (par exemple, pyttsx3).
- **azure**Â : ajoute la prise en charge de la parole par Azure Cognitive Services.
- **elevenlabs**Â : inclut l'intÃ©gration avec l'API ElevenLabs.
- **openai**Â : pour les services vocaux OpenAI.
- **gtts**Â : prise en charge de la synthÃ¨se vocale par Google.
- **coqui**Â : installe le moteur TTS Coqui.
- **minimal**Â : installe uniquement les exigences de base sans moteur (uniquement nÃ©cessaire si vous souhaitez dÃ©velopper votre propre moteur)

Supposons que vous souhaitiez installer RealtimeTTS uniquement pour l'utilisation locale de Coqui TTS neuronal, vous devez alors utiliserÂ :

```bash
pip install realtimetts[coqui]
```

Par exemple, si vous souhaitez installer Real timeTTS avec prise en charge d'Azure Cognitive Services Speech, ElevenLabs et OpenAI uniquementÂ :

```bash
pip install realtimetts[azure,elevenlabs,openai]
```

### Installation d'un environnement virtuel

Pour ceux qui souhaitent effectuer une installation complÃ¨te dans un environnement virtuel, suivez ces Ã©tapesÂ :

```bash
python -m venv env_realtimetts
env_realtimetts\Scripts\activate.bat
python.exe -m pip install --upgrade pip
pip install -U realtimetts[all]
```

Plus d'informations sur [l'installation CUDA](#cuda-installation).

## Configuration requise pour le moteur

Les diffÃ©rents moteurs pris en charge par RealtimeTTS ont des exigences uniques. Assurez-vous de rÃ©pondre Ã  ces exigences en fonction du moteur que vous choisissez.

### SystemEngine
`SystemEngine` fonctionne immÃ©diatement avec les fonctionnalitÃ©s TTS intÃ©grÃ©es de votre systÃ¨me. Aucune configuration supplÃ©mentaire n'est nÃ©cessaire.

### GTTSEngine
Le `GTTSEngine` fonctionne immÃ©diatement Ã  l'aide de l'API de synthÃ¨se vocale de Google Translate. Aucune configuration supplÃ©mentaire n'est nÃ©cessaire.

### OpenAIEngine
Pour utiliser `OpenAIEngine`Â :
- dÃ©finissez la variable d'environnement OPENAI_API_KEY
- installez ffmpeg (voir [Installation CUDA](#cuda-installation) point 3)

### AzureEngine
Pour utiliser `AzureEngine`, vous aurez besoin deÂ :
- ClÃ© API de synthÃ¨se vocale Microsoft Azure (fournie via le paramÃ¨tre de constructeur AzureEngine Â«Â speech_keyÂ Â» ou dans la variable d'environnement AZURE_SPEECH_KEY)
- RÃ©gion de service Microsoft Azure.

Assurez-vous que ces informations d'identification sont disponibles et correctement configurÃ©es lors de l'initialisation de `AzureEngine`.

### ElevenlabsEngine
Pour `ElevenlabsEngine`, vous avez besoin deÂ :
- ClÃ© API Elevenlabs (fournie via le paramÃ¨tre constructeur ElevenlabsEngine Â«Â api_keyÂ Â» ou dans la variable d'environnement ELEVENLABS_API_KEY)
- `mpv` installÃ© sur votre systÃ¨me (essentiel pour le streaming audio mpeg, Elevenlabs ne fournit que du mpeg).

ğŸ”¹ **Installation de `mpv`Â :**
- **macOS**Â :
```bash
brew install mpv
```

- **Linux et Windows**Â : Visitez [mpv.io](https://mpv.io/) pour obtenir des instructions d'installation.

### CoquiEngine

Fournit une synthÃ¨se vocale neuronale locale de haute qualitÃ© avec clonage de la voix.

TÃ©lÃ©charge d'abord un modÃ¨le de synthÃ¨se vocale neuronale. Dans la plupart des cas, il doit Ãªtre suffisamment rapide pour la synthÃ¨se en temps rÃ©el Ã  l'aide du GPU. NÃ©cessite environ 4 Ã  5Â Go de VRAM.

- pour cloner une voix, soumettez le nom de fichier d'un fichier wave contenant la voix source comme paramÃ¨tre Â« voix Â» au constructeur CoquiEngine
- le clonage de voix fonctionne mieux avec un fichier WAV mono 16 bits 22050 Hz contenant un Ã©chantillon court (~5-30 sec)

Sur la plupart des systÃ¨mes, la prise en charge du GPU sera nÃ©cessaire pour fonctionner suffisamment rapidement en temps rÃ©el, sinon vous subirez des bÃ©gaiements.

## DÃ©marrage rapide

Voici un exemple d'utilisation de baseÂ :

```python
from RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine

engine = SystemEngine() # replace with your TTS engine
stream = TextToAudioStream(engine)
stream.feed("Hello world! How are you today?")
stream.play_async()
```

## Feed Text

Vous pouvez alimenter des chaÃ®nes individuellesÂ :

```python
stream.feed("Hello, this is a sentence.")
```

Vous pouvez Ã©galement alimenter des gÃ©nÃ©rateurs et des itÃ©rateurs de caractÃ¨res pour le streaming en temps rÃ©elÂ :

```python
def write(prompt: str):
for chunk in openai.ChatCompletion.create(
model="gpt-3.5-turbo",
messages=[{"role": "user", "content" : prompt}],
stream=True
):
if (text_chunk := chunk["choices"][0]["delta"].get("content")) is not None:
yield text_chunk

text_stream = write("Un discours relaxant en trois phrases.")

stream.feed(text_stream)
```

```python
char_iterator = iter("Diffusion en continu de ce caractÃ¨re par caractÃ¨re.")
stream.feed(char_iterator)
```

## Lecture

De maniÃ¨re asynchroneÂ :

```python
stream.play_async()
while stream.is_playing():
time.sleep(0.1)
```

De maniÃ¨re synchroneÂ :

```python
stream.play()
```

## Test de la bibliothÃ¨que

Le sous-rÃ©pertoire test contient un ensemble de scripts pour vous aider Ã  Ã©valuer et Ã  comprendre les capacitÃ©s de la bibliothÃ¨que RealtimeTTS.

Notez que la plupart des tests reposent toujours sur l'"ancienne" API OpenAI (<1.0.0). L'utilisation de la nouvelle API OpenAI est dÃ©montrÃ©e dans openai_1.0_test.py.

- **simple_test.py**
- **Description**Â : Une dÃ©monstration de style Â«Â hello worldÂ Â» de l'utilisation la plus simple de la bibliothÃ¨que.

- **complex_test.py**
- **Description**Â : Une dÃ©monstration complÃ¨te prÃ©sentant la plupart des fonctionnalitÃ©s fournies par la bibliothÃ¨que.

- **coqui_test.py**
- **Description**Â : Test du moteur TTS local de coqui.

- **translator.py**
- **DÃ©pendances**Â : ExÃ©cutez `pip install openai realtimestt`.
- **Description**Â : Traductions en temps rÃ©el dans six langues diffÃ©rentes.

- **openai_voice_interface.py**
- **DÃ©pendances**Â : ExÃ©cutez `pip install openai realtimestt`.
- **Description**Â : Interface utilisateur activÃ©e par mot de rÃ©veil et basÃ©e sur la voix pour l'API OpenAI.

- **advanced_talk.py**
- **DÃ©pendances**Â : ExÃ©cutez `pip install openai keyboard realtimestt`.
- **Description**Â : Choisissez le moteur TTS et la voix avant de dÃ©marrer la conversation AI.

- **minimalistic_talkbot.py**
- **DÃ©pendances**Â : ExÃ©cutez `pip install openai realtimestt`.
- **Description**Â : Un talkbot de base en 20 lignes de code.

- **simple_llm_test.py**
- **DÃ©pendances**Â : ExÃ©cutez `pip install op
  enai`.
- **Description**Â : DÃ©monstration simple de la maniÃ¨re d'intÃ©grer la bibliothÃ¨que Ã  de grands modÃ¨les de langage (LLM).

- **test_callbacks.py**
- **DÃ©pendances**Â : ExÃ©cutez `pip install openai`.
- **Description**Â : PrÃ©sente les rappels et vous permet de vÃ©rifier les temps de latence dans un environnement d'application rÃ©el.

## Pause, reprise et arrÃªt

Mettre en pause le flux audioÂ :

```python
stream.pause()
```

Reprendre un flux en pauseÂ :

```python
stream.resume()
```

ArrÃªter immÃ©diatement le fluxÂ :

```python
stream.stop()
```

## Exigences expliquÃ©es

- **Version Python**Â :
- **Obligatoire**Â : Python >= 3.9, < 3.13
- **Raison**Â : La bibliothÃ¨que dÃ©pend de la bibliothÃ¨que GitHub Â«Â TTSÂ Â» de coqui, qui nÃ©cessite des versions Python dans cette plage.

- **PyAudio**Â : pour crÃ©er un flux audio de sortie

- **stream2sentence**Â : pour diviser le flux de texte entrant en phrases

- **pyttsx3**Â : moteur de conversion texte-parole du systÃ¨me

- **pydub**Â : pour convertir les formats de morceaux audio

- **azure-cognitiveservices-speech**Â : moteur de conversion texte-parole Azure

- **elevenlabs**Â : moteur de conversion texte-parole Elevenlabs

- **coqui-TTS**Â : bibliothÃ¨que de synthÃ¨se vocale XTTS de Coqui pour une synthÃ¨se vocale neuronale locale de haute qualitÃ©

Un grand merci Ã  [Idiap Research Institute](https://github.com/idiap) pour avoir maintenu un [fork de coqui tts](https://github.com/idiap/coqui-ai-TTS).

- **openai**Â : pour interagir avec l'API TTS d'OpenAI

- **gtts**Â : conversion de texte en parole de Google Translate

## Configuration

### ParamÃ¨tres d'initialisation pour `TextToAudioStream`

Lorsque vous initialisez la classe `TextToAudioStream`, vous disposez de diverses options pour personnaliser son comportement. Voici les paramÃ¨tres disponiblesÂ :

#### `engine` (BaseEngine)
- **Type**Â : BaseEngine
- **Obligatoire**Â : Oui
- **Description**Â : Le moteur sous-jacent responsable de la synthÃ¨se texte-audio. Vous devez fournir une instance de `BaseEngine` ou de sa sous-classe pour activer la synthÃ¨se audio.

#### `on_text_stream_start` (callable)
- **Type**Â : Fonction appelable
- **Obligatoire**Â : Non
- **Description**Â : Cette fonction de rappel facultative est dÃ©clenchÃ©e lorsque le flux de texte dÃ©marre. Utilisez-la pour toute configuration ou journalisation dont vous pourriez avoir besoin.

#### `on_text_stream_stop` (callable)
- **Type**Â : Fonction appelable
- **Obligatoire**Â : Non
- **Description**Â : Cette fonction de rappel facultative est activÃ©e lorsque le flux de texte se termine. Vous pouvez l'utiliser pour les tÃ¢ches de nettoyage ou de journalisation.

#### `on_audio_stream_start` (callable)
- **Type**Â : Fonction appelable
- **Obligatoire**Â : Non
- **Description**Â : Cette fonction de rappel facultative est invoquÃ©e lorsque le flux audio dÃ©marre. Utile pour les mises Ã  jour de l'interface utilisateur ou la journalisation des Ã©vÃ©nements.

#### `on_audio_stream_stop` (callable)
- **Type**Â : Fonction appelable
- **Obligatoire**Â : Non
- **Description**Â : Cette fonction de rappel facultative est appelÃ©e lorsque le flux audio s'arrÃªte. IdÃ©al pour le nettoyage des ressources ou les tÃ¢ches de post-traitement.

#### `on_character` (callable)
- **Type**Â : Fonction appelable
- **Obligatoire**Â : Non
- **Description**Â : Cette fonction de rappel facultative est appelÃ©e lorsqu'un seul caractÃ¨re est traitÃ©.

#### `output_device_index` (int)
- **Type**Â : Entier
- **Obligatoire**Â : Non
- **Par dÃ©faut**Â : Aucun
- **Description**Â : SpÃ©cifie l'index du pÃ©riphÃ©rique de sortie Ã  utiliser. None utilise le pÃ©riphÃ©rique par dÃ©faut.

#### `tokenizer` (string)
- **Type**Â : String
- **Obligatoire**Â : Non
- **Par dÃ©faut**Â : nltk
- **Description**Â : Tokenizer Ã  utiliser pour le fractionnement de phrases (actuellement, Â«Â nltkÂ Â» et Â«Â stanzaÂ Â» sont pris en charge).

#### `language` (string)
- **Type**: String
- **Required**: No
- **Default**: fr
- **Description**: Langue Ã  utiliser pour le fractionnement des phrases.

#### `muted` (bool)
- **Type**: Bool
- **Required**: No
- **Default**: False
- **Description**: ParamÃ¨tre global muted. Si True, aucun flux pyAudio ne sera ouvert. DÃ©sactive la lecture audio via les haut-parleurs locaux (au cas oÃ¹ vous voudriez synthÃ©tiser dans un fichier ou traiter des morceaux audio) et remplace le paramÃ¨tre de mise en sourdine des paramÃ¨tres de lecture.

#### `level` (int)
- **Type**: Integer
- **Required**: No
- **Default**: `logging.WARNING`
- **Description**: DÃ©finit le niveau de journalisation pour le logger interne. Il peut s'agir de n'importe quelle constante entiÃ¨re du module `logging` intÃ©grÃ© de Python.

#### Exemple d'utilisationÂ :

```python
engine = YourEngine() # Remplacez par votre moteur
stream = TextToAudioStream(
engine=engine,
on_text_stream_start=my_text_start_func,
on_text_stream_stop=my_text_stop_func,
on_audio_stream_start=my_audio_start_func,
on_audio_stream_stop=my_audio_stop_func,
level=logging.INFO
)
```

### MÃ©thodes

#### `play` et `play_async`

Ces mÃ©thodes sont responsables de l'exÃ©cution de la synthÃ¨se texte-audio et de la lecture du flux audio. La diffÃ©rence est que `play` est une fonction de blocage, tandis que `play_async` s'exÃ©cute dans un thread sÃ©parÃ©, permettant Ã  d'autres opÃ©rations de se poursuivre.

##### ParamÃ¨tresÂ :

###### `fast_sentence_fragment` (bool)
- **Par dÃ©faut**Â : `True`
- **Description**Â : Lorsqu'elle est dÃ©finie sur `True`, la mÃ©thode privilÃ©gie la vitesse, en gÃ©nÃ©rant et en lisant les fragments de phrases plus rapidement. Ceci est utile pour les applications oÃ¹ la latence est importante.

###### `fast_sentence_fragment_allsentences` (bool)
- **Par dÃ©faut**Â : `False`
- **Description**Â : lorsqu'il est dÃ©fini sur Â«Â TrueÂ Â», applique le traitement rapide des fragments de phrases Ã  toutes les phrases, pas seulement Ã  la premiÃ¨re.

###### `fast_sentence_fragment_allsentences_multiple` (bool)
- **Par dÃ©faut**Â : Â«Â False`
- **Description**Â : lorsqu'il est dÃ©fini sur Â«Â TrueÂ Â», permet de gÃ©nÃ©rer plusieurs fragments de phrases au lieu d'un seul.

###### `buffer_threshold_seconds` (float)
- **Par dÃ©faut**Â : Â«Â 0.0Â Â»
- **Description**Â : spÃ©cifie le temps en secondes pour le seuil de mise en mÃ©moire tampon, qui a un impact sur la fluiditÃ© et la continuitÃ© de la lecture audio.

- **Fonctionnement**Â : avant de synthÃ©tiser une nouvelle phrase, le systÃ¨me vÃ©rifie s'il reste plus de matÃ©riel audio dans la mÃ©moire tampon que le temps spÃ©cifiÃ© par `buffer_threshold_seconds`. Si tel est le cas, il rÃ©cupÃ¨re une autre phrase du gÃ©nÃ©rateur de texte, en supposant qu'il peut rÃ©cupÃ©rer et synthÃ©tiser cette nouvelle phrase dans la fenÃªtre temporelle fournie par l'audio restant dans la mÃ©moire tampon. Ce processus permet au moteur de synthÃ¨se vocale d'avoir plus de contexte pour une meilleure synthÃ¨se, amÃ©liorant ainsi l'expÃ©rience utilisateur.

Une valeur plus Ã©levÃ©e garantit qu'il y a plus d'audio prÃ©-tamponnÃ©, rÃ©duisant ainsi la probabilitÃ© de silence ou de lacunes pendant la lecture. Si vous rencontrez des pauses ou des interruptions, pensez Ã  augmenter cette valeur.

###### `minimum_sentence_length` (int)
- **Par dÃ©faut**Â : `10`
- **Description**Â : DÃ©finit la longueur minimale de caractÃ¨res pour considÃ©rer une chaÃ®ne comme une phrase Ã  synthÃ©tiser. Cela affecte la maniÃ¨re dont les fragments de texte sont traitÃ©s et lus.

###### `minimum_first_fragment_length` (int)
- **Par dÃ©faut**Â : `10`
- **Description**Â : Le nombre minimal de caractÃ¨res requis pour le premier fragment de phrase avant de produire le rÃ©sultat.

###### `log_synthesized_text` (bool)
- **Default**: `False`
- **Description**: Lorsqu'il est activÃ©, enregistre les fragments de texte au fur et Ã  mesure qu'ils sont synthÃ©tisÃ©s en audio. Utile pour l'audit et le dÃ©bogage.

###### `reset_generated_text` (bool)
- **Default**: `True`
- **Description**: Si True, rÃ©initialise le texte gÃ©nÃ©rÃ© avant le traitement.

###### `output_wavfile` (str)
- **Default**: `None`
- **Description**: Si dÃ©fini, enregistre l'audio dans le fichier WAV spÃ©cifiÃ©.

###### `on_sentence_synthesized` (callable)
- **Default**: `None`
- **Description**: Une fonction de rappel qui est appelÃ©e aprÃ¨s la synthÃ¨se d'un seul fragment de phrase.

###### `before_sentence_synthesized` (callable)
- **Default**: `None`
- **Description**: Une fonction de rappel qui est appelÃ©e avant qu'un seul fragment de phrase ne soit synthÃ©tisÃ©.

###### `on_audio_chunk` (callable)
- **Default**: `None`
- **Description**: Fonction de rappel qui est appelÃ©e lorsqu'un seul fragment audio est prÃªt.

###### `tokenizer` (str)
- **Default**: `"nltk"`
- **Description**: GÃ©nÃ©rateur de tokens Ã  utiliser pour le fractionnement des phrases. Prend actuellement en charge "nltk" et "stanza".

###### `tokenize_sentences` (callable)
- **Default**: `None`
- **Description**: Une fonction personnalisÃ©e qui tokenise les phrases Ã  partir du texte d'entrÃ©e. Vous pouvez fournir votre propre tokenizer lÃ©ger si vous n'Ãªtes pas satisfait de nltk et stanza. Il doit prendre le texte comme une chaÃ®ne et renvoyer les phrases divisÃ©es comme une liste de chaÃ®nes.

###### `language` (str)
- **Default**: `"en"`
- **Description**: Langue Ã  utiliser pour le fractionnement des phrases.

###### `context_size` (int)
- **Default**: `12`
- **Description**: Le nombre de caractÃ¨res utilisÃ©s pour Ã©tablir le contexte pour la dÃ©tection des limites de phrases. Un contexte plus large amÃ©liore la prÃ©cision de la dÃ©tection des limites de phrases.

###### `context_size_look_overhead` (int)
- **Default**: `12`
- **Description**: Taille de contexte supplÃ©mentaire pour regarder en avant lors de la dÃ©tection des limites de phrases.

###### `muted` (bool)
- **Default**: `False`
- **Description**: Si True, dÃ©sactive la lecture audio via les haut-parleurs locaux. Utile lorsque vous souhaitez synthÃ©tiser dans un fichier ou traiter des morceaux audio sans les lire.

###### `sentence_fragment_delimiters` (str)
- **Default**: `".?!;:,\nâ€¦)]}ã€‚-"`
- **Description**: Une chaÃ®ne de caractÃ¨res considÃ©rÃ©s comme des dÃ©limiteurs de phrases.

###### `force_first_fragment_after_words` (int)
- **Default**: `15`
- **Description**: Le nombre de mots aprÃ¨s lequel le premier fragment de phrase est forcÃ© Ã  Ãªtre rendu.

### Installation de CUDA

Ces Ã©tapes sont recommandÃ©es pour ceux qui ont besoin de **meilleures performances** et qui ont un GPU NVIDIA compatible.

> **Remarque**Â : *pour vÃ©rifier si votre GPU NVIDIA prend en charge CUDA, visitez la [liste officielle des GPU CUDA](https://developer.nvidia.com/cuda-gpus).*

Pour utiliser une torche avec prise en charge via CUDA, veuillez suivre ces Ã©tapesÂ :

> **Remarque**Â : *les installations plus rÃ©centes de Pytorch [peuvent](https://stackoverflow.com/a/77069523) (non vÃ©rifiÃ©es) ne plus nÃ©cessiter l'installation de Toolkit (et Ã©ventuellement de cuDNN).*

1. **Installez NVIDIA CUDA Toolkit**Â :
Par exemple, pour installer Toolkit 12.X, veuillez
- Visitez [TÃ©lÃ©chargements NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads).
- SÃ©lectionnez votre systÃ¨me d'exploitation, l'architecture de votre systÃ¨me et la version de votre systÃ¨me d'exploitation.
- TÃ©lÃ©chargez et installez le logiciel.

ou pour installer Toolkit 11.8, veuillez
- Visitez [NVIDIA CUDA Toolkit Archive](https://developer.nvidia.com/cuda-11-8-0-download-archive).
- SÃ©lectionnez votre
- systÃ¨me d'exploitation, architecture systÃ¨me et version du systÃ¨me d'exploitation.
- TÃ©lÃ©chargez et installez le logiciel.

2. **Installez NVIDIA cuDNN**Â :

Par exemple, pour installer cuDNN 8.7.0 pour CUDA 11.x, veuillez
- Visitez [NVIDIA cuDNN Archive](https://developer.nvidia.com/rdp/cudnn-archive).
- Cliquez sur Â«Â TÃ©lÃ©charger cuDNN v8.7.0 (28 novembre 2022), pour CUDA 11.xÂ Â».
- TÃ©lÃ©chargez et installez le logiciel.

3. **Installez ffmpeg**Â :

Vous pouvez tÃ©lÃ©charger un programme d'installation pour votre systÃ¨me d'exploitation Ã  partir du [site Web ffmpeg](https://ffmpeg.org/download.html).

Ou utilisez un gestionnaire de paquetsÂ :

- **Sur Ubuntu ou Debian**Â :
```bash
sudo apt update && sudo apt install ffmpeg
```

- **Sur Arch Linux**Â :
```bash
sudo pacman -S ffmpeg
```

- **Sur MacOS avec Homebrew** ([https://brew.sh/](https://brew.sh/))Â :
```bash
brew install ffmpeg
```

- **Sur Windows avec Chocolatey** ([https://chocolatey.org/](https://chocolatey.org/))Â :
```bash
choco install ffmpeg
```

- **Sur Windows avec Scoop** ([https://scoop.sh/](https://scoop.sh/))Â :
```bash
scoop install ffmpeg
```

4. **Installer PyTorch avec prise en charge CUDA**Â :

Pour mettre Ã  niveau votre installation PyTorch afin d'activer la prise en charge GPU avec CUDA, suivez ces instructions en fonction de votre version spÃ©cifique de CUDA. Cela est utile si vous souhaitez amÃ©liorer les performances de RealtimeSTT avec les fonctionnalitÃ©s CUDA.

- **Pour CUDA 11.8Â :**

Pour mettre Ã  jour PyTorch et Torchaudio afin de prendre en charge CUDA 11.8, utilisez les commandes suivantesÂ :

```bash
pip install torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118
```

- **Pour CUDA 12.XÂ :**

Pour mettre Ã  jour PyTorch et Torchaudio afin de prendre en charge CUDA 12.X, exÃ©cutez la commande suivanteÂ :

```bash
pip install torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121
```

Remplacez Â«Â 2.3.1Â Â» par la version de PyTorch qui correspond Ã  votre systÃ¨me et Ã  vos exigences.

5. **Correction pour rÃ©soudre les problÃ¨mes de compatibilitÃ©**Â :
Si vous rencontrez des problÃ¨mes de compatibilitÃ© de bibliothÃ¨que, essayez de dÃ©finir ces bibliothÃ¨ques sur des versions fixesÂ :

```bash
pip install networkx==2.8.8
pip install typing_extensions==4.8.0
pip install fsspec==2023.6.0
pip install imageio==2.31.6
pip install networkx==2.8.8
pip install numpy==1.24.3
pip install requests==2.31.0
```

## ğŸ’– Remerciements

Un grand merci Ã  l'Ã©quipe derriÃ¨re [Coqui AI](https://coqui.ai/) - en particulier la brillante [Eren GÃ¶lge](https://github.com/erogol) - pour avoir Ã©tÃ© la premiÃ¨re Ã  nous fournir une synthÃ¨se locale de haute qualitÃ© avec une vitesse en temps rÃ©el et mÃªme une voix clonableÂ !

Merci Ã  [Pierre Nicolas Durette](https://github.com/pndurette) de nous avoir offert un tts gratuit Ã  utiliser sans GPU en utilisant Google Translate avec sa bibliothÃ¨que python gtts.

## Contribution

Les contributions sont toujours les bienvenues (par exemple, PR pour ajouter un nouveau moteur).

## Informations sur la licence

### â— Remarque importanteÂ :
Bien que la source de cette bibliothÃ¨que soit open source, l'utilisation de nombreux moteurs dont elle dÃ©pend ne l'est pasÂ : les fournisseurs de moteurs externes limitent souvent l'utilisation commerciale dans leurs plans gratuits. Cela signifie que les moteurs peuvent Ãªtre utilisÃ©s pour des projets non commerciaux, mais l'utilisation commerciale nÃ©cessite un plan payant.

### RÃ©sumÃ© des licences de moteurÂ :

#### CoquiEngine
- **Licence**Â : Open source uniquement pour les projets non commerciaux.
- **Utilisation commerciale**Â : NÃ©cessite un plan payant.
- **DÃ©tails**Â : [Licence CoquiEngine](https://coqui.ai/cpml)

#### ElevenlabsEngine
- **Licence**Â : Open source uniquement pour les projets non commerciaux.
- **Utilisation commerciale**Â : Disponible avec tous les forfaits payants.
- **DÃ©tails**Â : [Licence ElevenlabsEngine](https://help.elevenlabs.io/hc/en-us/articles/13313564601361-Puis-je-publier-le-contenu-que-je-gÃ©nÃ¨re-sur-la-plateforme-)

#### AzureEngine
- **Licence**Â : Open source uniquement pour les projets non commerciaux.
- **Utilisation commerciale**Â : Disponible Ã  partir du niveau standard.
- **DÃ©tails**Â : [Licence AzureEngine](https://learn.microsoft.com/en-us/answers/questions/1192398/can-i-use-azure-text-to-speech-for-commercial-usag)

#### SystemEngine
- **Licence**Â : Mozilla Public License 2.0 et GNU Lesser General Public License (LGPL) version 3.0.
- **Utilisation commerciale**Â : autorisÃ©e sous cette licence.
- **DÃ©tails**Â : [Licence SystemEngine](https://github.com/nateshmbhat/pyttsx3/blob/master/LICENSE)

#### GTTSEngine
- **Licence**Â : licence MIT
- **Utilisation commerciale**Â : elle est sous licence MIT, elle devrait donc thÃ©oriquement Ãªtre possible. Une certaine prudence peut Ãªtre nÃ©cessaire car elle utilise la fonctionnalitÃ© vocale non documentÃ©e de Google Translate.
- **DÃ©tails** : [Licence MIT GTTS](https://github.com/pndurette/gTTS/blob/main/LICENSE)

#### OpenAIEngine
- **Licence** : veuillez lire les [Conditions d'utilisation d'OpenAI](https://openai.com/policies/terms-of-use)

**Avertissement** : il s'agit d'un rÃ©sumÃ© des licences telles qu'elles sont comprises au moment de la rÃ©daction. Il ne s'agit pas d'un avis juridique. Veuillez lire et respecter les licences des diffÃ©rents fournisseurs de moteurs si vous
Vous prÃ©voyez de les utiliser dans un projet.

## Contributeurs

<a href="https://github.com/traceloop/openllmetry/graphs/contributors">
<img alt="contributeurs" src="https://contrib.rocks/image?repo=KoljaB/RealtimeTTS"/>
</a>

## Auteur

Kolja Beigel
E-mailÂ : kolja.beigel@web.de

<p align="center">
<a href="https://github.com/KoljaB/RealtimeTTS" target="_blank">
<img src="https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white" alt="GitHub">
</a>
&nbsp;&nbsp;&nbsp;
<a href="#realtimetts" target="_blank">
<img src="https://img.shields.io/badge/Retour%20en%20haut-000000?style=pour-le-badge" alt="Retour en haut">
</a>
</p>
