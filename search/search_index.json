{"config":{"lang":["en","fr","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"en/","title":"RealtimeTTS","text":"<p>EN | FR | ES</p> <p>Easy to use, low-latency text-to-speech library for realtime applications</p>"},{"location":"en/#about-the-project","title":"About the Project","text":"<p>RealtimeTTS is a state-of-the-art text-to-speech (TTS) library designed for real-time applications. It stands out in its ability to convert text streams fast into high-quality auditory output with minimal latency.</p>"},{"location":"en/#key-features","title":"Key Features","text":"<ul> <li>Low Latency: almost instantaneous text-to-speech conversion, compatible with LLM outputs</li> <li>High-Quality Audio: generates clear and natural-sounding speech</li> <li>Multiple TTS Engine Support: supports OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS and System TTS</li> <li>Multilingual</li> <li>Robust and Reliable: ensures continuous operation through a fallback mechanism, switches to alternative engines in case of disruptions guaranteeing consistent performance and reliability</li> </ul> <p>For installation instructions, usage examples, and API reference, please navigate through the documentation using the sidebar.</p>"},{"location":"en/api/","title":"English","text":""},{"location":"en/api/#configuration","title":"Configuration","text":""},{"location":"en/api/#initialization-parameters-for-texttoaudiostream","title":"Initialization Parameters for <code>TextToAudioStream</code>","text":"<p>When you initialize the <code>TextToAudioStream</code> class, you have various options to customize its behavior. Here are the available parameters:</p>"},{"location":"en/api/#engine-baseengine","title":"<code>engine</code> (BaseEngine)","text":"<ul> <li>Type: BaseEngine</li> <li>Required: Yes</li> <li>Description: The underlying engine responsible for text-to-audio synthesis. You must provide an instance of <code>BaseEngine</code> or its subclass to enable audio synthesis.</li> </ul>"},{"location":"en/api/#on_text_stream_start-callable","title":"<code>on_text_stream_start</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is triggered when the text stream begins. Use it for any setup or logging you may need.</li> </ul>"},{"location":"en/api/#on_text_stream_stop-callable","title":"<code>on_text_stream_stop</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is activated when the text stream ends. You can use this for cleanup tasks or logging.</li> </ul>"},{"location":"en/api/#on_audio_stream_start-callable","title":"<code>on_audio_stream_start</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is invoked when the audio stream starts. Useful for UI updates or event logging.</li> </ul>"},{"location":"en/api/#on_audio_stream_stop-callable","title":"<code>on_audio_stream_stop</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is called when the audio stream stops. Ideal for resource cleanup or post-processing tasks.</li> </ul>"},{"location":"en/api/#on_character-callable","title":"<code>on_character</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is called when a single character is processed.</li> </ul>"},{"location":"en/api/#output_device_index-int","title":"<code>output_device_index</code> (int)","text":"<ul> <li>Type: Integer</li> <li>Required: No</li> <li>Default: None</li> <li>Description: Specifies the output device index to use. None uses the default device.</li> </ul>"},{"location":"en/api/#tokenizer-string","title":"<code>tokenizer</code> (string)","text":"<ul> <li>Type: String</li> <li>Required: No</li> <li>Default: nltk</li> <li>Description: Tokenizer to use for sentence splitting (currently \"nltk\" and \"stanza\" are supported).</li> </ul>"},{"location":"en/api/#language-string","title":"<code>language</code> (string)","text":"<ul> <li>Type: String</li> <li>Required: No</li> <li>Default: en</li> <li>Description: Language to use for sentence splitting.</li> </ul>"},{"location":"en/api/#muted-bool","title":"<code>muted</code> (bool)","text":"<ul> <li>Type: Bool</li> <li>Required: No</li> <li>Default: False</li> <li>Description: Global muted parameter. If True, no pyAudio stream will be opened. Disables audio playback via local speakers (in case you want to synthesize to file or process audio chunks) and overrides the play parameters muted setting.</li> </ul>"},{"location":"en/api/#level-int","title":"<code>level</code> (int)","text":"<ul> <li>Type: Integer</li> <li>Required: No</li> <li>Default: <code>logging.WARNING</code></li> <li>Description: Sets the logging level for the internal logger. This can be any integer constant from Python's built-in <code>logging</code> module.</li> </ul>"},{"location":"en/api/#example-usage","title":"Example Usage:","text":"<pre><code>engine = YourEngine()  # Substitute with your engine\nstream = TextToAudioStream(\n    engine=engine,\n    on_text_stream_start=my_text_start_func,\n    on_text_stream_stop=my_text_stop_func,\n    on_audio_stream_start=my_audio_start_func,\n    on_audio_stream_stop=my_audio_stop_func,\n    level=logging.INFO\n)\n</code></pre>"},{"location":"en/api/#methods","title":"Methods","text":""},{"location":"en/api/#play-and-play_async","title":"<code>play</code> and <code>play_async</code>","text":"<p>These methods are responsible for executing the text-to-audio synthesis and playing the audio stream. The difference is that <code>play</code> is a blocking function, while <code>play_async</code> runs in a separate thread, allowing other operations to proceed.</p>"},{"location":"en/api/#parameters","title":"Parameters:","text":""},{"location":"en/api/#fast_sentence_fragment-bool","title":"<code>fast_sentence_fragment</code> (bool)","text":"<ul> <li>Default: <code>True</code></li> <li>Description: When set to <code>True</code>, the method will prioritize speed, generating and playing sentence fragments faster. This is useful for applications where latency matters.</li> </ul>"},{"location":"en/api/#fast_sentence_fragment_allsentences-bool","title":"<code>fast_sentence_fragment_allsentences</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: When set to <code>True</code>, applies the fast sentence fragment processing to all sentences, not just the first one.</li> </ul>"},{"location":"en/api/#fast_sentence_fragment_allsentences_multiple-bool","title":"<code>fast_sentence_fragment_allsentences_multiple</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: When set to <code>True</code>, allows yielding multiple sentence fragments instead of just a single one.</li> </ul>"},{"location":"en/api/#buffer_threshold_seconds-float","title":"<code>buffer_threshold_seconds</code> (float)","text":"<ul> <li>Default: <code>0.0</code></li> <li> <p>Description: Specifies the time in seconds for the buffering threshold, which impacts the smoothness and continuity of audio playback.</p> </li> <li> <p>How it Works: Before synthesizing a new sentence, the system checks if there is more audio material left in the buffer than the time specified by <code>buffer_threshold_seconds</code>. If so, it retrieves another sentence from the text generator, assuming that it can fetch and synthesize this new sentence within the time window provided by the remaining audio in the buffer. This process allows the text-to-speech engine to have more context for better synthesis, enhancing the user experience.</p> </li> </ul> <p>A higher value ensures that there's more pre-buffered audio, reducing the likelihood of silence or gaps during playback. If you experience breaks or pauses, consider increasing this value.</p>"},{"location":"en/api/#minimum_sentence_length-int","title":"<code>minimum_sentence_length</code> (int)","text":"<ul> <li>Default: <code>10</code></li> <li>Description: Sets the minimum character length to consider a string as a sentence to be synthesized. This affects how text chunks are processed and played.</li> </ul>"},{"location":"en/api/#minimum_first_fragment_length-int","title":"<code>minimum_first_fragment_length</code> (int)","text":"<ul> <li>Default: <code>10</code></li> <li>Description: The minimum number of characters required for the first sentence fragment before yielding.</li> </ul>"},{"location":"en/api/#log_synthesized_text-bool","title":"<code>log_synthesized_text</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: When enabled, logs the text chunks as they are synthesized into audio. Helpful for auditing and debugging.</li> </ul>"},{"location":"en/api/#reset_generated_text-bool","title":"<code>reset_generated_text</code> (bool)","text":"<ul> <li>Default: <code>True</code></li> <li>Description: If True, reset the generated text before processing.</li> </ul>"},{"location":"en/api/#output_wavfile-str","title":"<code>output_wavfile</code> (str)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: If set, save the audio to the specified WAV file.</li> </ul>"},{"location":"en/api/#on_sentence_synthesized-callable","title":"<code>on_sentence_synthesized</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: A callback function that gets called after a single sentence fragment was synthesized.</li> </ul>"},{"location":"en/api/#before_sentence_synthesized-callable","title":"<code>before_sentence_synthesized</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: A callback function that gets called before a single sentence fragment gets synthesized.</li> </ul>"},{"location":"en/api/#on_audio_chunk-callable","title":"<code>on_audio_chunk</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: Callback function that gets called when a single audio chunk is ready.</li> </ul>"},{"location":"en/api/#tokenizer-str","title":"<code>tokenizer</code> (str)","text":"<ul> <li>Default: <code>\"nltk\"</code></li> <li>Description: Tokenizer to use for sentence splitting. Currently supports \"nltk\" and \"stanza\".</li> </ul>"},{"location":"en/api/#tokenize_sentences-callable","title":"<code>tokenize_sentences</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: A custom function that tokenizes sentences from the input text. You can provide your own lightweight tokenizer if you are unhappy with nltk and stanza. It should take text as a string and return split sentences as a list of strings.</li> </ul>"},{"location":"en/api/#language-str","title":"<code>language</code> (str)","text":"<ul> <li>Default: <code>\"en\"</code></li> <li>Description: Language to use for sentence splitting.</li> </ul>"},{"location":"en/api/#context_size-int","title":"<code>context_size</code> (int)","text":"<ul> <li>Default: <code>12</code></li> <li>Description: The number of characters used to establish context for sentence boundary detection. A larger context improves the accuracy of detecting sentence boundaries.</li> </ul>"},{"location":"en/api/#context_size_look_overhead-int","title":"<code>context_size_look_overhead</code> (int)","text":"<ul> <li>Default: <code>12</code></li> <li>Description: Additional context size for looking ahead when detecting sentence boundaries.</li> </ul>"},{"location":"en/api/#muted-bool_1","title":"<code>muted</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: If True, disables audio playback via local speakers. Useful when you want to synthesize to a file or process audio chunks without playing them.</li> </ul>"},{"location":"en/api/#sentence_fragment_delimiters-str","title":"<code>sentence_fragment_delimiters</code> (str)","text":"<ul> <li>Default: <code>\".?!;:,\\n\u2026)]}\u3002-\"</code></li> <li>Description: A string of characters that are considered sentence delimiters.</li> </ul>"},{"location":"en/api/#force_first_fragment_after_words-int","title":"<code>force_first_fragment_after_words</code> (int)","text":"<ul> <li>Default: <code>15</code></li> <li>Description: The number of words after which the first sentence fragment is forced to be yielded.</li> </ul>"},{"location":"en/contributing/","title":"Contributing to RealtimeTTS","text":"<p>We welcome contributions to RealtimeTTS! Here are some ways you can contribute:</p> <ol> <li> <p>Reporting Bugs: If you find a bug, please open an issue on our GitHub repository.</p> </li> <li> <p>Suggesting Enhancements: Have ideas for new features or improvements? We'd love to hear them! Open an issue to suggest enhancements.</p> </li> <li> <p>Code Contributions: Want to add a new feature or fix a bug? Great! Please follow these steps:</p> </li> <li>Fork the repository</li> <li>Create a new branch for your feature</li> <li>Make your changes</li> <li> <p>Submit a pull request with a clear description of your changes</p> </li> <li> <p>Documentation: Help us improve our documentation by fixing typos, adding examples, or clarifying confusing sections.</p> </li> <li> <p>Adding New Engines: If you want to add support for a new TTS engine, please open an issue first to discuss the implementation.</p> </li> </ol> <p>Thank you for helping make RealtimeTTS better!</p>"},{"location":"en/faq/","title":"Frequently Asked Questions","text":"<p>For answers to frequently asked questions about RealtimeTTS, please refer to our FAQ page on GitHub.</p> <p>This page covers various topics including:</p> <ul> <li>Usage of different TTS engines</li> <li>Handling of multilingual text</li> <li>Performance optimization</li> <li>Troubleshooting common issues</li> </ul> <p>For more detailed information, please visit the link above.</p>"},{"location":"en/installation/","title":"English","text":"<p>Note: Basic Installation with <code>pip install realtimetts</code> is not recommended anymore, use <code>pip install realtimetts[all]</code> instead.</p> <p>The RealtimeTTS library provides installation options for various dependencies for your use case. Here are the different ways you can install RealtimeTTS depending on your needs:</p>"},{"location":"en/installation/#full-installation","title":"Full Installation","text":"<p>To install RealtimeTTS with support for all TTS engines:</p> <pre><code>pip install -U realtimetts[all]\n</code></pre>"},{"location":"en/installation/#custom-installation","title":"Custom Installation","text":"<p>RealtimeTTS allows for custom installation with minimal library installations. Here are the options available: - all: Full installation with every engine supported. - system: Includes system-specific TTS capabilities (e.g., pyttsx3). - azure: Adds Azure Cognitive Services Speech support. - elevenlabs: Includes integration with ElevenLabs API. - openai: For OpenAI voice services. - gtts: Google Text-to-Speech support. - coqui: Installs the Coqui TTS engine. - minimal: Installs only the base requirements with no engine (only needed if you want to develop an own engine)</p> <p>Say you want to install RealtimeTTS only for local neuronal Coqui TTS usage, then you should use:</p> <pre><code>pip install realtimetts[coqui]\n</code></pre> <p>For example, if you want to install RealtimeTTS with only Azure Cognitive Services Speech, ElevenLabs, and OpenAI support:</p> <pre><code>pip install realtimetts[azure,elevenlabs,openai]\n</code></pre>"},{"location":"en/installation/#virtual-environment-installation","title":"Virtual Environment Installation","text":"<p>For those who want to perform a full installation within a virtual environment, follow these steps:</p> <pre><code>python -m venv env_realtimetts\nenv_realtimetts\\Scripts\\activate.bat\npython.exe -m pip install --upgrade pip\npip install -U realtimetts[all]\n</code></pre> <p>More information about CUDA installation.</p>"},{"location":"en/installation/#engine-requirements","title":"Engine Requirements","text":"<p>Different engines supported by RealtimeTTS have unique requirements. Ensure you fulfill these requirements based on the engine you choose.</p>"},{"location":"en/installation/#systemengine","title":"SystemEngine","text":"<p>The <code>SystemEngine</code> works out of the box with your system's built-in TTS capabilities. No additional setup is needed.</p>"},{"location":"en/installation/#gttsengine","title":"GTTSEngine","text":"<p>The <code>GTTSEngine</code> works out of the box using Google Translate's text-to-speech API. No additional setup is needed.</p>"},{"location":"en/installation/#openaiengine","title":"OpenAIEngine","text":"<p>To use the <code>OpenAIEngine</code>: - set environment variable OPENAI_API_KEY - install ffmpeg (see CUDA installation point 3)</p>"},{"location":"en/installation/#azureengine","title":"AzureEngine","text":"<p>To use the <code>AzureEngine</code>, you will need: - Microsoft Azure Text-to-Speech API key (provided via AzureEngine constructor parameter \"speech_key\" or in the environment variable AZURE_SPEECH_KEY) - Microsoft Azure service region.</p> <p>Make sure you have these credentials available and correctly configured when initializing the <code>AzureEngine</code>.</p>"},{"location":"en/installation/#elevenlabsengine","title":"ElevenlabsEngine","text":"<p>For the <code>ElevenlabsEngine</code>, you need: - Elevenlabs API key (provided via ElevenlabsEngine constructor parameter \"api_key\" or in the environment variable ELEVENLABS_API_KEY) - <code>mpv</code> installed on your system (essential for streaming mpeg audio, Elevenlabs only delivers mpeg).</p> <p>\ud83d\udd39 Installing <code>mpv</code>:   - macOS:     <code>brew install mpv</code></p> <ul> <li>Linux and Windows: Visit mpv.io for installation instructions.</li> </ul>"},{"location":"en/installation/#coquiengine","title":"CoquiEngine","text":"<p>Delivers high quality, local, neural TTS with voice-cloning.</p> <p>Downloads a neural TTS model first. In most cases it be fast enough for Realtime using GPU synthesis. Needs around 4-5 GB VRAM.</p> <ul> <li>to clone a voice submit the filename of a wave file containing the source voice as \"voice\" parameter to the CoquiEngine constructor</li> <li>voice cloning works best with a 22050 Hz mono 16bit WAV file containing a short (~5-30 sec) sample</li> </ul> <p>On most systems GPU support will be needed to run fast enough for realtime, otherwise you will experience stuttering.</p>"},{"location":"en/installation/#cuda-installation","title":"CUDA installation","text":"<p>These steps are recommended for those who require better performance and have a compatible NVIDIA GPU.</p> <p>Note: to check if your NVIDIA GPU supports CUDA, visit the official CUDA GPUs list.</p> <p>To use a torch with support via CUDA please follow these steps:</p> <p>Note: newer pytorch installations may (unverified) not need Toolkit (and possibly cuDNN) installation anymore.</p> <ol> <li> <p>Install NVIDIA CUDA Toolkit:     For example, to install Toolkit 12.X, please</p> <ul> <li>Visit NVIDIA CUDA Downloads.</li> <li>Select your operating system, system architecture, and os version.</li> <li>Download and install the software.</li> </ul> <p>or to install Toolkit 11.8, please - Visit NVIDIA CUDA Toolkit Archive. - Select your operating system, system architecture, and os version. - Download and install the software.</p> </li> <li> <p>Install NVIDIA cuDNN:</p> <p>For example, to install cuDNN 8.7.0 for CUDA 11.x please - Visit NVIDIA cuDNN Archive. - Click on \"Download cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x\". - Download and install the software.</p> </li> <li> <p>Install ffmpeg:</p> <p>You can download an installer for your OS from the ffmpeg Website.</p> <p>Or use a package manager:</p> <ul> <li> <p>On Ubuntu or Debian:     <code>sudo apt update &amp;&amp; sudo apt install ffmpeg</code></p> </li> <li> <p>On Arch Linux:     <code>sudo pacman -S ffmpeg</code></p> </li> <li> <p>On MacOS using Homebrew (https://brew.sh/):     <code>brew install ffmpeg</code></p> </li> <li> <p>On Windows using Chocolatey (https://chocolatey.org/):     <code>choco install ffmpeg</code></p> </li> <li> <p>On Windows using Scoop (https://scoop.sh/):     <code>scoop install ffmpeg</code></p> </li> </ul> </li> <li> <p>Install PyTorch with CUDA support:</p> <p>To upgrade your PyTorch installation to enable GPU support with CUDA, follow these instructions based on your specific CUDA version. This is useful if you wish to enhance the performance of RealtimeSTT with CUDA capabilities.</p> <ul> <li> <p>For CUDA 11.8:</p> <p>To update PyTorch and Torchaudio to support CUDA 11.8, use the following commands:</p> <p><code>pip install torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p> </li> <li> <p>For CUDA 12.X:</p> <p>To update PyTorch and Torchaudio to support CUDA 12.X, execute the following:</p> <p><code>pip install torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code></p> </li> </ul> <p>Replace <code>2.3.1</code> with the version of PyTorch that matches your system and requirements.</p> </li> <li> <p>Fix for to resolve compatibility issues:     If you run into library compatibility issues, try setting these libraries to fixed versions:</p> </li> </ol> <p>``` </p> <pre><code>pip install networkx==2.8.8\n\npip install typing_extensions==4.8.0\n\npip install fsspec==2023.6.0\n\npip install imageio==2.31.6\n\npip install networkx==2.8.8\n\npip install numpy==1.24.3\n\npip install requests==2.31.0\n</code></pre> <p>```</p>"},{"location":"en/usage/","title":"Usage","text":""},{"location":"en/usage/#quick-start","title":"Quick Start","text":"<p>Here's a basic usage example:</p> <pre><code>from RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine\n\nengine = SystemEngine() # replace with your TTS engine\nstream = TextToAudioStream(engine)\nstream.feed(\"Hello world! How are you today?\")\nstream.play_async()\n</code></pre>"},{"location":"en/usage/#feed-text","title":"Feed Text","text":"<p>You can feed individual strings:</p> <pre><code>stream.feed(\"Hello, this is a sentence.\")\n</code></pre> <p>Or you can feed generators and character iterators for real-time streaming:</p> <pre><code>def write(prompt: str):\n    for chunk in openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\" : prompt}],\n        stream=True\n    ):\n        if (text_chunk := chunk[\"choices\"][0][\"delta\"].get(\"content\")) is not None:\n            yield text_chunk\n\ntext_stream = write(\"A three-sentence relaxing speech.\")\n\nstream.feed(text_stream)\n</code></pre> <pre><code>char_iterator = iter(\"Streaming this character by character.\")\nstream.feed(char_iterator)\n</code></pre>"},{"location":"en/usage/#playback","title":"Playback","text":"<p>Asynchronously:</p> <pre><code>stream.play_async()\nwhile stream.is_playing():\n    time.sleep(0.1)\n</code></pre> <p>Synchronously:</p> <pre><code>stream.play()\n</code></pre>"},{"location":"en/usage/#testing-the-library","title":"Testing the Library","text":"<p>The test subdirectory contains a set of scripts to help you evaluate and understand the capabilities of the RealtimeTTS library.</p> <p>Note that most of the tests still rely on the \"old\" OpenAI API (&lt;1.0.0). Usage of the new OpenAI API is demonstrated in openai_1.0_test.py.</p> <ul> <li> <p>simple_test.py</p> <ul> <li>Description: A \"hello world\" styled demonstration of the library's simplest usage.</li> </ul> </li> <li> <p>complex_test.py</p> <ul> <li>Description: A comprehensive demonstration showcasing most of the features provided by the library.</li> </ul> </li> <li> <p>coqui_test.py</p> <ul> <li>Description: Test of local coqui TTS engine.</li> </ul> </li> <li> <p>translator.py</p> <ul> <li>Dependencies: Run <code>pip install openai realtimestt</code>.</li> <li>Description: Real-time translations into six different languages.</li> </ul> </li> <li> <p>openai_voice_interface.py</p> <ul> <li>Dependencies: Run <code>pip install openai realtimestt</code>.</li> <li>Description: Wake word activated and voice based user interface to the OpenAI API.</li> </ul> </li> <li> <p>advanced_talk.py</p> <ul> <li>Dependencies: Run <code>pip install openai keyboard realtimestt</code>.</li> <li>Description: Choose TTS engine and voice before starting AI conversation.</li> </ul> </li> <li> <p>minimalistic_talkbot.py</p> <ul> <li>Dependencies: Run <code>pip install openai realtimestt</code>.</li> <li>Description: A basic talkbot in 20 lines of code.</li> </ul> </li> <li> <p>simple_llm_test.py</p> <ul> <li>Dependencies: Run <code>pip install openai</code>.</li> <li>Description: Simple demonstration of how to integrate the library with large language models (LLMs).</li> </ul> </li> <li> <p>test_callbacks.py</p> <ul> <li>Dependencies: Run <code>pip install openai</code>.</li> <li>Description: Showcases the callbacks and lets you check the latency times in a real-world application environment.</li> </ul> </li> </ul>"},{"location":"en/usage/#pause-resume-stop","title":"Pause, Resume &amp; Stop","text":"<p>Pause the audio stream:</p> <pre><code>stream.pause()\n</code></pre> <p>Resume a paused stream:</p> <pre><code>stream.resume()\n</code></pre> <p>Stop the stream immediately:</p> <pre><code>stream.stop()\n</code></pre>"},{"location":"en/usage/#requirements-explained","title":"Requirements Explained","text":"<ul> <li>Python Version:</li> <li>Required: Python &gt;= 3.9, &lt; 3.13</li> <li> <p>Reason: The library depends on the GitHub library \"TTS\" from coqui, which requires Python versions in this range.</p> </li> <li> <p>PyAudio: to create an output audio stream</p> </li> <li> <p>stream2sentence: to split the incoming text stream into sentences</p> </li> <li> <p>pyttsx3: System text-to-speech conversion engine</p> </li> <li> <p>pydub: to convert audio chunk formats</p> </li> <li> <p>azure-cognitiveservices-speech: Azure text-to-speech conversion engine</p> </li> <li> <p>elevenlabs: Elevenlabs text-to-speech conversion engine</p> </li> <li> <p>coqui-TTS: Coqui's XTTS text-to-speech library for high-quality local neural TTS</p> </li> </ul> <p>Shoutout to Idiap Research Institute for maintaining a fork of coqui tts.</p> <ul> <li> <p>openai: to interact with OpenAI's TTS API</p> </li> <li> <p>gtts: Google translate text-to-speech conversion</p> </li> </ul>"},{"location":"es/","title":"RealtimeTTS","text":"<p>EN | FR | ES</p> <p>*Biblioteca de conversi\u00f3n de texto en voz f\u00e1cil de usar y de baja latencia para aplicaciones en tiempo real.</p>"},{"location":"es/#acerca-del-proyecto","title":"Acerca del proyecto","text":"<p>RealtimeTTS es una biblioteca de texto a voz (TTS) de \u00faltima generaci\u00f3n dise\u00f1ada para aplicaciones en tiempo real. Destaca por su capacidad para convertir r\u00e1pidamente flujos de texto en salida auditiva de alta calidad con una latencia m\u00ednima.</p>"},{"location":"es/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ul> <li>Baja latencia: conversi\u00f3n de texto a voz casi instant\u00e1nea, compatible con salidas LLM.</li> <li>Audio de alta calidad**: genera un habla clara y natural.</li> <li>Compatible con m\u00faltiples motores TTS**: compatible con OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS y System TTS</li> <li>Multiling\u00fce</li> <li>Robusto y fiable**: garantiza un funcionamiento continuo gracias a un mecanismo de reserva que cambia a motores alternativos en caso de interrupciones, lo que garantiza un rendimiento y una fiabilidad constantes.</li> </ul> <p>Para obtener instrucciones de instalaci\u00f3n, ejemplos de uso y referencias de la API, navegue por la documentaci\u00f3n utilizando la barra lateral.</p>"},{"location":"es/api/","title":"TextToAudioStream - Documentaci\u00f3n en Espa\u00f1ol","text":""},{"location":"es/api/#configuracion","title":"Configuraci\u00f3n","text":""},{"location":"es/api/#parametros-de-inicializacion-para-texttoaudiostream","title":"Par\u00e1metros de Inicializaci\u00f3n para <code>TextToAudioStream</code>","text":"<p>Cuando inicializa la clase <code>TextToAudioStream</code>, tiene varias opciones para personalizar su comportamiento. Aqu\u00ed est\u00e1n los par\u00e1metros disponibles:</p>"},{"location":"es/api/#parametros-principales","title":"Par\u00e1metros Principales","text":""},{"location":"es/api/#engine-baseengine","title":"<code>engine</code> (BaseEngine)","text":"<ul> <li>Tipo: BaseEngine</li> <li>Requerido: S\u00ed</li> <li>Descripci\u00f3n: El motor subyacente responsable de la s\u00edntesis de texto a audio. Debe proporcionar una instancia de <code>BaseEngine</code> o su subclase para habilitar la s\u00edntesis de audio.</li> </ul>"},{"location":"es/api/#on_text_stream_start-callable","title":"<code>on_text_stream_start</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se activa cuando comienza el flujo de texto. Util\u00edcela para cualquier configuraci\u00f3n o registro que pueda necesitar.</li> </ul>"},{"location":"es/api/#on_text_stream_stop-callable","title":"<code>on_text_stream_stop</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se activa cuando finaliza el flujo de texto. Puede utilizarla para tareas de limpieza o registro.</li> </ul>"},{"location":"es/api/#on_audio_stream_start-callable","title":"<code>on_audio_stream_start</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se invoca cuando comienza el flujo de audio. \u00datil para actualizaciones de UI o registro de eventos.</li> </ul>"},{"location":"es/api/#on_audio_stream_stop-callable","title":"<code>on_audio_stream_stop</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se llama cuando se detiene el flujo de audio. Ideal para limpieza de recursos o tareas de post-procesamiento.</li> </ul>"},{"location":"es/api/#on_character-callable","title":"<code>on_character</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se llama cuando se procesa un solo car\u00e1cter.</li> </ul>"},{"location":"es/api/#output_device_index-int","title":"<code>output_device_index</code> (int)","text":"<ul> <li>Tipo: Entero</li> <li>Requerido: No</li> <li>Valor predeterminado: None</li> <li>Descripci\u00f3n: Especifica el \u00edndice del dispositivo de salida a utilizar. None usa el dispositivo predeterminado.</li> </ul>"},{"location":"es/api/#tokenizer-string","title":"<code>tokenizer</code> (string)","text":"<ul> <li>Tipo: String</li> <li>Requerido: No</li> <li>Valor predeterminado: nltk</li> <li>Descripci\u00f3n: Tokenizador a utilizar para la divisi\u00f3n de oraciones (actualmente se admiten \"nltk\" y \"stanza\").</li> </ul>"},{"location":"es/api/#language-string","title":"<code>language</code> (string)","text":"<ul> <li>Tipo: String</li> <li>Requerido: No</li> <li>Valor predeterminado: en</li> <li>Descripci\u00f3n: Idioma a utilizar para la divisi\u00f3n de oraciones.</li> </ul>"},{"location":"es/api/#muted-bool","title":"<code>muted</code> (bool)","text":"<ul> <li>Tipo: Bool</li> <li>Requerido: No</li> <li>Valor predeterminado: False</li> <li>Descripci\u00f3n: Par\u00e1metro global de silencio. Si es True, no se abrir\u00e1 ning\u00fan flujo pyAudio. Deshabilita la reproducci\u00f3n de audio a trav\u00e9s de los altavoces locales.</li> </ul>"},{"location":"es/api/#level-int","title":"<code>level</code> (int)","text":"<ul> <li>Tipo: Entero</li> <li>Requerido: No</li> <li>Valor predeterminado: <code>logging.WARNING</code></li> <li>Descripci\u00f3n: Establece el nivel de registro para el registrador interno. Puede ser cualquier constante entera del m\u00f3dulo <code>logging</code> incorporado de Python.</li> </ul>"},{"location":"es/api/#ejemplo-de-uso","title":"Ejemplo de Uso","text":"<pre><code>engine = YourEngine()  # Sustituya con su motor\nstream = TextToAudioStream(\n    engine=engine,\n    on_text_stream_start=my_text_start_func,\n    on_text_stream_stop=my_text_stop_func,\n    on_audio_stream_start=my_audio_start_func,\n    on_audio_stream_stop=my_audio_stop_func,\n    level=logging.INFO\n)\n</code></pre>"},{"location":"es/api/#metodos","title":"M\u00e9todos","text":""},{"location":"es/api/#play-y-play_async","title":"<code>play</code> y <code>play_async</code>","text":"<p>Estos m\u00e9todos son responsables de ejecutar la s\u00edntesis de texto a audio y reproducir el flujo de audio. La diferencia es que <code>play</code> es una funci\u00f3n bloqueante, mientras que <code>play_async</code> se ejecuta en un hilo separado, permitiendo que otras operaciones contin\u00faen.</p>"},{"location":"es/api/#parametros-de-reproduccion","title":"Par\u00e1metros de Reproducci\u00f3n","text":""},{"location":"es/api/#fast_sentence_fragment-bool","title":"<code>fast_sentence_fragment</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>True</code></li> <li>Descripci\u00f3n: Cuando se establece en <code>True</code>, el m\u00e9todo priorizar\u00e1 la velocidad, generando y reproduciendo fragmentos de oraciones m\u00e1s r\u00e1pidamente.</li> </ul>"},{"location":"es/api/#fast_sentence_fragment_allsentences-bool","title":"<code>fast_sentence_fragment_allsentences</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Cuando se establece en <code>True</code>, aplica el procesamiento r\u00e1pido de fragmentos de oraciones a todas las oraciones.</li> </ul>"},{"location":"es/api/#fast_sentence_fragment_allsentences_multiple-bool","title":"<code>fast_sentence_fragment_allsentences_multiple</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Cuando se establece en <code>True</code>, permite generar m\u00faltiples fragmentos de oraciones.</li> </ul>"},{"location":"es/api/#buffer_threshold_seconds-float","title":"<code>buffer_threshold_seconds</code> (float)","text":"<ul> <li>Valor predeterminado: <code>0.0</code></li> <li>Descripci\u00f3n: Especifica el tiempo en segundos para el umbral de b\u00fafer.</li> </ul> <p>C\u00f3mo funciona: Antes de sintetizar una nueva oraci\u00f3n, el sistema verifica si queda m\u00e1s material de audio en el b\u00fafer que el tiempo especificado. Un valor m\u00e1s alto asegura que haya m\u00e1s audio pre-almacenado en el b\u00fafer.</p>"},{"location":"es/api/#minimum_sentence_length-int","title":"<code>minimum_sentence_length</code> (int)","text":"<ul> <li>Valor predeterminado: <code>10</code></li> <li>Descripci\u00f3n: Establece la longitud m\u00ednima de caracteres para considerar una cadena como una oraci\u00f3n.</li> </ul>"},{"location":"es/api/#minimum_first_fragment_length-int","title":"<code>minimum_first_fragment_length</code> (int)","text":"<ul> <li>Valor predeterminado: <code>10</code></li> <li>Descripci\u00f3n: El n\u00famero m\u00ednimo de caracteres requeridos para el primer fragmento de oraci\u00f3n.</li> </ul>"},{"location":"es/api/#log_synthesized_text-bool","title":"<code>log_synthesized_text</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Cuando est\u00e1 habilitado, registra los fragmentos de texto sintetizados.</li> </ul>"},{"location":"es/api/#reset_generated_text-bool","title":"<code>reset_generated_text</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>True</code></li> <li>Descripci\u00f3n: Si es True, reinicia el texto generado antes del procesamiento.</li> </ul>"},{"location":"es/api/#output_wavfile-str","title":"<code>output_wavfile</code> (str)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Si se establece, guarda el audio en el archivo WAV especificado.</li> </ul>"},{"location":"es/api/#funciones-de-callback","title":"Funciones de Callback","text":""},{"location":"es/api/#on_sentence_synthesized-callable","title":"<code>on_sentence_synthesized</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Se llama despu\u00e9s de sintetizar un fragmento de oraci\u00f3n.</li> </ul>"},{"location":"es/api/#before_sentence_synthesized-callable","title":"<code>before_sentence_synthesized</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Se llama antes de sintetizar un fragmento de oraci\u00f3n.</li> </ul>"},{"location":"es/api/#on_audio_chunk-callable","title":"<code>on_audio_chunk</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Se llama cuando un fragmento de audio est\u00e1 listo.</li> </ul>"},{"location":"es/api/#configuracion-de-tokenizacion","title":"Configuraci\u00f3n de Tokenizaci\u00f3n","text":""},{"location":"es/api/#tokenizer-str","title":"<code>tokenizer</code> (str)","text":"<ul> <li>Valor predeterminado: <code>\"nltk\"</code></li> <li>Descripci\u00f3n: Tokenizador para la divisi\u00f3n de oraciones. Admite \"nltk\" y \"stanza\".</li> </ul>"},{"location":"es/api/#tokenize_sentences-callable","title":"<code>tokenize_sentences</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Funci\u00f3n personalizada para tokenizar oraciones del texto de entrada.</li> </ul>"},{"location":"es/api/#language-str","title":"<code>language</code> (str)","text":"<ul> <li>Valor predeterminado: <code>\"en\"</code></li> <li>Descripci\u00f3n: Idioma para la divisi\u00f3n de oraciones.</li> </ul>"},{"location":"es/api/#parametros-de-contexto","title":"Par\u00e1metros de Contexto","text":""},{"location":"es/api/#context_size-int","title":"<code>context_size</code> (int)","text":"<ul> <li>Valor predeterminado: <code>12</code></li> <li>Descripci\u00f3n: Caracteres utilizados para establecer el contexto de l\u00edmites de oraciones.</li> </ul>"},{"location":"es/api/#context_size_look_overhead-int","title":"<code>context_size_look_overhead</code> (int)","text":"<ul> <li>Valor predeterminado: <code>12</code></li> <li>Descripci\u00f3n: Tama\u00f1o de contexto adicional para mirar hacia adelante.</li> </ul>"},{"location":"es/api/#otros-parametros","title":"Otros Par\u00e1metros","text":""},{"location":"es/api/#muted-bool_1","title":"<code>muted</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Deshabilita la reproducci\u00f3n de audio local si es True.</li> </ul>"},{"location":"es/api/#sentence_fragment_delimiters-str","title":"<code>sentence_fragment_delimiters</code> (str)","text":"<ul> <li>Valor predeterminado: <code>\".?!;:,\\n\u2026)]}\u3002-\"</code></li> <li>Descripci\u00f3n: Caracteres considerados como delimitadores de oraciones.</li> </ul>"},{"location":"es/api/#force_first_fragment_after_words-int","title":"<code>force_first_fragment_after_words</code> (int)","text":"<ul> <li>Valor predeterminado: <code>15</code></li> <li>Descripci\u00f3n: N\u00famero de palabras despu\u00e9s de las cuales se fuerza el primer fragmento.</li> </ul>"},{"location":"es/contributing/","title":"Contribuir a RealtimeTTS","text":"<p>Agradecemos cualquier contribuci\u00f3n a RealtimeTTS. Aqu\u00ed tienes algunas formas de contribuir:</p> <ol> <li> <p>Informar de errores: Si encuentras un error, por favor abre una incidencia en nuestro repositorio GitHub.</p> </li> <li> <p>Sugerir mejoras: \u00bfTienes ideas para nuevas funciones o mejoras? Nos encantar\u00eda escucharlas. Abre una incidencia para sugerir mejoras.</p> </li> <li> <p>Contribuciones de c\u00f3digo: \u00bfQuieres a\u00f1adir una nueva funci\u00f3n o corregir un error? \u00a1Perfecto! Sigue estos pasos:</p> </li> <li>Abre el repositorio</li> <li>Crea una nueva rama para tu funci\u00f3n</li> <li>Realice los cambios</li> <li> <p>Env\u00eda un pull request con una descripci\u00f3n clara de tus cambios</p> </li> <li> <p>Documentaci\u00f3n: Ay\u00fadanos a mejorar nuestra documentaci\u00f3n corrigiendo erratas, a\u00f1adiendo ejemplos o aclarando secciones confusas.</p> </li> <li> <p>A\u00f1adir nuevos motores: Si quieres a\u00f1adir soporte para un nuevo motor TTS, por favor abre una incidencia primero para discutir la implementaci\u00f3n.</p> </li> </ol> <p>Gracias por ayudarnos a mejorar RealtimeTTS.</p>"},{"location":"es/faq/","title":"Preguntas frecuentes","text":"<p>Para obtener respuestas a las preguntas m\u00e1s frecuentes sobre RealtimeTTS, consulta nuestra p\u00e1gina de preguntas frecuentes en GitHub.</p> <p>Esta p\u00e1gina cubre varios temas, entre ellos</p> <ul> <li>Uso de diferentes motores TTS</li> <li>Tratamiento de textos multiling\u00fces</li> <li>Optimizaci\u00f3n del rendimiento</li> <li>Soluci\u00f3n de problemas comunes</li> </ul> <p>Para obtener informaci\u00f3n m\u00e1s detallada, visite el enlace anterior.</p>"},{"location":"es/installation/","title":"Espa\u00f1ol","text":"<p>Nota: Ya no se recomienda la instalaci\u00f3n b\u00e1sica con <code>pip install realtimetts</code>, use <code>pip install realtimetts[all]</code> en su lugar.</p> <p>La biblioteca RealtimeTTS proporciona opciones de instalaci\u00f3n para varias dependencias seg\u00fan su caso de uso. Aqu\u00ed est\u00e1n las diferentes formas en que puede instalar RealtimeTTS seg\u00fan sus necesidades:</p>"},{"location":"es/installation/#instalacion-completa","title":"Instalaci\u00f3n Completa","text":"<p>Para instalar RealtimeTTS con soporte para todos los motores de TTS:</p> <pre><code>pip install -U realtimetts[all]\n</code></pre>"},{"location":"es/installation/#instalacion-personalizada","title":"Instalaci\u00f3n Personalizada","text":"<p>RealtimeTTS permite una instalaci\u00f3n personalizada con instalaciones m\u00ednimas de bibliotecas. Estas son las opciones disponibles: - all: Instalaci\u00f3n completa con todos los motores soportados. - system: Incluye capacidades de TTS espec\u00edficas del sistema (por ejemplo, pyttsx3). - azure: Agrega soporte para Azure Cognitive Services Speech. - elevenlabs: Incluye integraci\u00f3n con la API de ElevenLabs. - openai: Para servicios de voz de OpenAI. - gtts: Soporte para Google Text-to-Speech. - coqui: Instala el motor Coqui TTS. - minimal: Instala solo los requisitos base sin motor (solo necesario si desea desarrollar un motor propio)</p> <p>Por ejemplo, si desea instalar RealtimeTTS solo para uso local de Coqui TTS neuronal, debe usar:</p> <pre><code>pip install realtimetts[coqui]\n</code></pre> <p>Si desea instalar RealtimeTTS solo con Azure Cognitive Services Speech, ElevenLabs y soporte de OpenAI:</p> <pre><code>pip install realtimetts[azure,elevenlabs,openai]\n</code></pre>"},{"location":"es/installation/#instalacion-en-entorno-virtual","title":"Instalaci\u00f3n en Entorno Virtual","text":"<p>Para aquellos que deseen realizar una instalaci\u00f3n completa dentro de un entorno virtual, sigan estos pasos:</p> <pre><code>python -m venv env_realtimetts\nenv_realtimetts\\Scripts\\activate.bat\npython.exe -m pip install --upgrade pip\npip install -U realtimetts[all]\n</code></pre> <p>M\u00e1s informaci\u00f3n sobre instalaci\u00f3n de CUDA.</p>"},{"location":"es/installation/#requisitos-de-los-motores","title":"Requisitos de los Motores","text":"<p>Los diferentes motores soportados por RealtimeTTS tienen requisitos \u00fanicos. Aseg\u00farese de cumplir con estos requisitos seg\u00fan el motor que elija.</p>"},{"location":"es/installation/#systemengine","title":"SystemEngine","text":"<p>El <code>SystemEngine</code> funciona de inmediato con las capacidades de TTS incorporadas en su sistema. No se necesita configuraci\u00f3n adicional.</p>"},{"location":"es/installation/#gttsengine","title":"GTTSEngine","text":"<p>El <code>GTTSEngine</code> funciona de inmediato usando la API de texto a voz de Google Translate. No se necesita configuraci\u00f3n adicional.</p>"},{"location":"es/installation/#openaiengine","title":"OpenAIEngine","text":"<p>Para usar el <code>OpenAIEngine</code>: - configure la variable de entorno OPENAI_API_KEY - instale ffmpeg (ver instalaci\u00f3n de CUDA punto 3)</p>"},{"location":"es/installation/#azureengine","title":"AzureEngine","text":"<p>Para usar el <code>AzureEngine</code>, necesitar\u00e1: - Clave API de Microsoft Azure Text-to-Speech (proporcionada a trav\u00e9s del par\u00e1metro \"speech_key\" del constructor AzureEngine o en la variable de entorno AZURE_SPEECH_KEY) - Regi\u00f3n de servicio de Microsoft Azure.</p> <p>Aseg\u00farese de tener estas credenciales disponibles y correctamente configuradas al inicializar el <code>AzureEngine</code>.</p>"},{"location":"es/installation/#elevenlabsengine","title":"ElevenlabsEngine","text":"<p>Para el <code>ElevenlabsEngine</code>, necesita: - Clave API de Elevenlabs (proporcionada a trav\u00e9s del par\u00e1metro \"api_key\" del constructor ElevenlabsEngine o en la variable de entorno ELEVENLABS_API_KEY) - <code>mpv</code> instalado en su sistema (esencial para transmitir audio mpeg, Elevenlabs solo entrega mpeg).</p> <p>\ud83d\udd39 Instalaci\u00f3n de <code>mpv</code>:   - macOS:     <code>brew install mpv</code></p> <ul> <li>Linux y Windows: Visite mpv.io para instrucciones de instalaci\u00f3n.</li> </ul>"},{"location":"es/installation/#coquiengine","title":"CoquiEngine","text":"<p>Proporciona TTS neuronal local de alta calidad con clonaci\u00f3n de voz.</p> <p>Descarga primero un modelo neuronal TTS. En la mayor\u00eda de los casos, ser\u00e1 lo suficientemente r\u00e1pido para tiempo real usando s\u00edntesis GPU. Necesita alrededor de 4-5 GB de VRAM.</p> <ul> <li>para clonar una voz, env\u00ede el nombre del archivo de un archivo wave que contenga la voz fuente como par\u00e1metro \"voice\" al constructor CoquiEngine</li> <li>la clonaci\u00f3n de voz funciona mejor con un archivo WAV mono de 16 bits a 22050 Hz que contenga una muestra corta (~5-30 seg)</li> </ul> <p>En la mayor\u00eda de los sistemas, se necesitar\u00e1 soporte de GPU para ejecutarse lo suficientemente r\u00e1pido en tiempo real, de lo contrario experimentar\u00e1 tartamudeo.</p>"},{"location":"es/installation/#instalacion-de-cuda","title":"Instalaci\u00f3n de CUDA","text":"<p>Estos pasos son recomendados para aquellos que requieren mejor rendimiento y tienen una GPU NVIDIA compatible.</p> <p>Nota: para verificar si su GPU NVIDIA es compatible con CUDA, visite la lista oficial de GPUs CUDA.</p> <p>Para usar torch con soporte v\u00eda CUDA, siga estos pasos:</p> <p>Nota: las instalaciones m\u00e1s nuevas de pytorch pueden (no verificado) no necesitar la instalaci\u00f3n de Toolkit (y posiblemente cuDNN).</p> <ol> <li> <p>Instalar NVIDIA CUDA Toolkit:     Por ejemplo, para instalar Toolkit 12.X, por favor</p> <ul> <li>Visite NVIDIA CUDA Downloads.</li> <li>Seleccione su sistema operativo, arquitectura del sistema y versi\u00f3n del sistema operativo.</li> <li>Descargue e instale el software.</li> </ul> <p>o para instalar Toolkit 11.8, por favor - Visite NVIDIA CUDA Toolkit Archive. - Seleccione su sistema operativo, arquitectura del sistema y versi\u00f3n del sistema operativo. - Descargue e instale el software.</p> </li> <li> <p>Instalar NVIDIA cuDNN:</p> <p>Por ejemplo, para instalar cuDNN 8.7.0 para CUDA 11.x por favor - Visite NVIDIA cuDNN Archive. - Haga clic en \"Download cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x\". - Descargue e instale el software.</p> </li> <li> <p>Instalar ffmpeg:</p> <p>Puede descargar un instalador para su sistema operativo desde el sitio web de ffmpeg.</p> <p>O usar un gestor de paquetes:</p> <ul> <li> <p>En Ubuntu o Debian:     <code>sudo apt update &amp;&amp; sudo apt install ffmpeg</code></p> </li> <li> <p>En Arch Linux:     <code>sudo pacman -S ffmpeg</code></p> </li> <li> <p>En MacOS usando Homebrew (https://brew.sh/):     <code>brew install ffmpeg</code></p> </li> <li> <p>En Windows usando Chocolatey (https://chocolatey.org/):     <code>choco install ffmpeg</code></p> </li> <li> <p>En Windows usando Scoop (https://scoop.sh/):     <code>scoop install ffmpeg</code></p> </li> </ul> </li> <li> <p>Instalar PyTorch con soporte CUDA:</p> <p>Para actualizar su instalaci\u00f3n de PyTorch y habilitar el soporte de GPU con CUDA, siga estas instrucciones seg\u00fan su versi\u00f3n espec\u00edfica de CUDA. Esto es \u00fatil si desea mejorar el rendimiento de RealtimeSTT con capacidades CUDA.</p> <ul> <li> <p>Para CUDA 11.8:</p> <p>Para actualizar PyTorch y Torchaudio para soportar CUDA 11.8, use los siguientes comandos:</p> <p><code>pip install torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p> </li> <li> <p>Para CUDA 12.X:</p> <p>Para actualizar PyTorch y Torchaudio para soportar CUDA 12.X, ejecute lo siguiente:</p> <p><code>pip install torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code></p> </li> </ul> <p>Reemplace <code>2.3.1</code> con la versi\u00f3n de PyTorch que coincida con su sistema y requisitos.</p> </li> <li> <p>Soluci\u00f3n para resolver problemas de compatibilidad:     Si encuentra problemas de compatibilidad de bibliotecas, intente establecer estas bibliotecas en versiones fijas:</p> <p><code>pip install networkx==2.8.8 pip install typing_extensions==4.8.0 pip install fsspec==2023.6.0 pip install imageio==2.31.6 pip install networkx==2.8.8 pip install numpy==1.24.3 pip install requests==2.31.0</code></p> </li> </ol>"},{"location":"es/usage/","title":"Uso","text":""},{"location":"es/usage/#inicio-rapido","title":"Inicio R\u00e1pido","text":"<p>Aqu\u00ed hay un ejemplo b\u00e1sico de uso:</p> <pre><code>from RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine\n\nengine = SystemEngine() # replace with your TTS engine\nstream = TextToAudioStream(engine)\nstream.feed(\"Hello world! How are you today?\")\nstream.play_async()\n</code></pre>"},{"location":"es/usage/#alimentar-texto","title":"Alimentar Texto","text":"<p>Puede alimentar cadenas individuales:</p> <pre><code>stream.feed(\"Hello, this is a sentence.\")\n</code></pre> <p>O puede alimentar generadores e iteradores de caracteres para la transmisi\u00f3n en tiempo real:</p> <pre><code>def write(prompt: str):\n    for chunk in openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\" : prompt}],\n        stream=True\n    ):\n        if (text_chunk := chunk[\"choices\"][0][\"delta\"].get(\"content\")) is not None:\n            yield text_chunk\n\ntext_stream = write(\"A three-sentence relaxing speech.\")\n\nstream.feed(text_stream)\n</code></pre> <pre><code>char_iterator = iter(\"Streaming this character by character.\")\nstream.feed(char_iterator)\n</code></pre>"},{"location":"es/usage/#reproduccion","title":"Reproducci\u00f3n","text":"<p>De forma as\u00edncrona:</p> <pre><code>stream.play_async()\nwhile stream.is_playing():\n    time.sleep(0.1)\n</code></pre> <p>De forma s\u00edncrona:</p> <pre><code>stream.play()\n</code></pre>"},{"location":"es/usage/#prueba-de-la-biblioteca","title":"Prueba de la Biblioteca","text":"<p>El subdirectorio de pruebas contiene un conjunto de scripts para ayudarte a evaluar y comprender las capacidades de la biblioteca RealtimeTTS.</p> <p>Ten en cuenta que la mayor\u00eda de las pruebas a\u00fan dependen de la API \"antigua\" de OpenAI (&lt;1.0.0). El uso de la nueva API de OpenAI se demuestra en openai_1.0_test.py.</p> <ul> <li> <p>simple_test.py</p> <ul> <li>Descripci\u00f3n: Una demostraci\u00f3n tipo \"hola mundo\" del uso m\u00e1s simple de la biblioteca.</li> </ul> </li> <li> <p>complex_test.py</p> <ul> <li>Descripci\u00f3n: Una demostraci\u00f3n completa que muestra la mayor\u00eda de las caracter\u00edsticas proporcionadas por la biblioteca.</li> </ul> </li> <li> <p>coqui_test.py</p> <ul> <li>Descripci\u00f3n: Prueba del motor local coqui TTS.</li> </ul> </li> <li> <p>translator.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai realtimestt</code>.</li> <li>Descripci\u00f3n: Traducciones en tiempo real a seis idiomas diferentes.</li> </ul> </li> <li> <p>openai_voice_interface.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai realtimestt</code>.</li> <li>Descripci\u00f3n: Interfaz de usuario activada por palabra clave y basada en voz para la API de OpenAI.</li> </ul> </li> <li> <p>advanced_talk.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai keyboard realtimestt</code>.</li> <li>Descripci\u00f3n: Elija el motor TTS y la voz antes de iniciar la conversaci\u00f3n con IA.</li> </ul> </li> <li> <p>minimalistic_talkbot.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai realtimestt</code>.</li> <li>Descripci\u00f3n: Un talkbot b\u00e1sico en 20 l\u00edneas de c\u00f3digo.</li> </ul> </li> <li> <p>simple_llm_test.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai</code>.</li> <li>Descripci\u00f3n: Demostraci\u00f3n simple de c\u00f3mo integrar la biblioteca con modelos de lenguaje grande (LLMs).</li> </ul> </li> <li> <p>test_callbacks.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai</code>.</li> <li>Descripci\u00f3n: Muestra los callbacks y te permite verificar los tiempos de latencia en un entorno de aplicaci\u00f3n del mundo real.</li> </ul> </li> </ul>"},{"location":"es/usage/#pausar-reanudar-y-detener","title":"Pausar, Reanudar y Detener","text":"<p>Pausar el flujo de audio:</p> <pre><code>stream.pause()\n</code></pre> <p>Reanudar un flujo pausado:</p> <pre><code>stream.resume()\n</code></pre> <p>Detener el flujo inmediatamente:</p> <pre><code>stream.stop()\n</code></pre>"},{"location":"es/usage/#requisitos-explicados","title":"Requisitos Explicados","text":"<ul> <li>Versi\u00f3n de Python:</li> <li>Requerido: Python &gt;= 3.9, &lt; 3.13</li> <li> <p>Raz\u00f3n: La biblioteca depende de la biblioteca GitHub \"TTS\" de coqui, que requiere versiones de Python en este rango.</p> </li> <li> <p>PyAudio: para crear un flujo de audio de salida</p> </li> <li> <p>stream2sentence: para dividir el flujo de texto entrante en oraciones</p> </li> <li> <p>pyttsx3: Motor de conversi\u00f3n de texto a voz del sistema</p> </li> <li> <p>pydub: para convertir formatos de fragmentos de audio</p> </li> <li> <p>azure-cognitiveservices-speech: Motor de conversi\u00f3n de texto a voz de Azure</p> </li> <li> <p>elevenlabs: Motor de conversi\u00f3n de texto a voz de Elevenlabs</p> </li> <li> <p>coqui-TTS: Biblioteca de texto a voz XTTS de Coqui para TTS neuronal local de alta calidad</p> </li> </ul> <p>Agradecimiento especial al Instituto de Investigaci\u00f3n Idiap por mantener un fork de coqui tts.</p> <ul> <li> <p>openai: para interactuar con la API TTS de OpenAI</p> </li> <li> <p>gtts: Conversi\u00f3n de texto a voz de Google translate</p> </li> </ul>"},{"location":"fr/","title":"RealtimeTTS","text":"<p>EN | FR | ES</p> <p>Biblioth\u00e8que de synth\u00e8se vocale \u00e0 faible latence et facile \u00e0 utiliser pour les applications en temps r\u00e9el</p>"},{"location":"fr/#a-propos-du-projet","title":"\u00c0 propos du projet","text":"<p>RealtimeTTS est une biblioth\u00e8que de synth\u00e8se vocale (TTS) de pointe con\u00e7ue pour les applications en temps r\u00e9el. Elle se distingue par sa capacit\u00e9 \u00e0 convertir des flux de texte en sortie auditive de haute qualit\u00e9 avec une latence minimale.</p>"},{"location":"fr/#caracteristiques-cles","title":"Caract\u00e9ristiques cl\u00e9s","text":"<ul> <li>Faible latence : conversion text-to-speech quasi-instantan\u00e9e, compatible avec les sorties LLM</li> <li>Audio de haute qualit\u00e9 : g\u00e9n\u00e8re un discours clair et naturel</li> <li>Support de plusieurs moteurs TTS : prend en charge OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS et System TTS</li> <li>Multilingue</li> <li>Robuste et fiable : garantit une op\u00e9ration continue gr\u00e2ce \u00e0 un m\u00e9canisme de fallback, bascule vers des moteurs alternatifs en cas de perturbations, garantissant une performance et une fiabilit\u00e9 coh\u00e9rentes</li> </ul> <p>Pour les instructions d'installation, les exemples d'utilisation et la r\u00e9f\u00e9rence de l'API, veuillez naviguer \u00e0 travers la documentation \u00e0 l'aide du sidebar.</p>"},{"location":"fr/api/","title":"Fran\u00e7ais","text":""},{"location":"fr/api/#configuration","title":"Configuration","text":""},{"location":"fr/api/#parametres-dinitialisation-pour-texttoaudiostream","title":"Param\u00e8tres d'initialisation pour `TextToAudioStream","text":"<p>Lorsque vous initialisez la classe <code>TextToAudioStream</code>, vous disposez de diverses options pour personnaliser son comportement. Voici les param\u00e8tres disponibles :</p>"},{"location":"fr/api/#baseengine","title":"`(BaseEngine)","text":"<ul> <li>Type: BaseEngine</li> <li>Obligatoire: Oui</li> <li>Description : Le moteur sous-jacent responsable de la synth\u00e8se texte-audio. Vous devez fournir une instance de <code>ine</code> ou sa sous-classe pour permettre la synth\u00e8se audio.</li> </ul>"},{"location":"fr/api/#_text_stream_start-appelable","title":"<code>_text_stream_start</code> (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est d\u00e9clench\u00e9e lorsque le flux de texte commence. Utilisez-le pour toute configuration ou journalisation dont vous pourriez avoir besoin.</li> </ul>"},{"location":"fr/api/#_text_stream_stop-appelable","title":"<code>_text_stream_stop</code> (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est activ\u00e9e \u00e0 la fin du flux de texte. Vous pouvez l'utiliser pour des t\u00e2ches de nettoyage ou de journalisation.</li> </ul>"},{"location":"fr/api/#_audio_stream_start-appelable","title":"_audio_stream_start` (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel facultative est invoqu\u00e9e au d\u00e9marrage du flux audio. Utile pour les mises \u00e0 jour de l'interface utilisateur ou la journalisation des \u00e9v\u00e9nements.</li> </ul>"},{"location":"fr/api/#_audio_stream_stop-appelable","title":"<code>_audio_stream_stop</code> (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est appel\u00e9e lorsque le flux audio s'arr\u00eate. Id\u00e9al pour les t\u00e2ches de nettoyage des ressources ou de post-traitement.</li> </ul>"},{"location":"fr/api/#on_character-appelable","title":"on_character` (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est appel\u00e9e lorsqu'un seul caract\u00e8re est trait\u00e9.</li> </ul>"},{"location":"fr/api/#_device_index-int","title":"<code>_device_index</code> (int)","text":"<ul> <li>Type: Entier</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: Aucun</li> <li>Description : Sp\u00e9cifie l'index du p\u00e9riph\u00e9rique de sortie \u00e0 utiliser. Aucun n'utilise le p\u00e9riph\u00e9rique par d\u00e9faut.</li> </ul>"},{"location":"fr/api/#tokenizerchaine","title":"<code>(tokenizer</code>(cha\u00eene)","text":"<ul> <li>Type: Cha\u00eene</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: nltk</li> <li>Description : Tokenizer \u00e0 utiliser pour le fractionnement des phrases (actuellement \u00ab nltk \u00bb et \u00ab stroza \u00bb sont pris en charge).</li> </ul>"},{"location":"fr/api/#languagechaine","title":"`language(cha\u00eene)","text":"<ul> <li>Type: Cha\u00eene</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: fr</li> <li>Description : Langue \u00e0 utiliser pour le fractionnement des phrases.</li> </ul>"},{"location":"fr/api/#mutedbool","title":"<code>muted</code>(bool)","text":"<ul> <li>Type: Bool</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: Faux</li> <li>Description : Param\u00e8tre global coup\u00e9. Si True, aucun flux pyAudio ne sera ouvert. D\u00e9sactive la lecture audio via des haut-parleurs locaux (au cas o\u00f9 vous souhaitez synth\u00e9tiser dans un fichier ou traiter des morceaux audio) et remplace le param\u00e8tre de mise en sourdine des param\u00e8tres de lecture.</li> </ul>"},{"location":"fr/api/#level-int","title":"<code>level</code> (int)","text":"<ul> <li>Type: Entier</li> <li>Obligatoire: Non</li> <li>D\u00e9faut:<code>logging.AVERTISSEMENT</code></li> <li>Description : D\u00e9finit le niveau de journalisation pour l'enregistreur interne. Cela peut \u00eatre n'importe quelle constante enti\u00e8re du module <code>ging</code> int\u00e9gr\u00e9 de Python.</li> </ul>"},{"location":"fr/api/#exemple-dutilisation","title":"Exemple d'utilisation :","text":"<p><code>``(`python moteur = YourEngine () # Remplacez-vous par votre moteur flux = TextToAudioStream(     moteur=engine,     on_text_stream_start=my_text_start_func,     on_text_stream_stop=my_text_stop_func,     on_audio_stream_start=my_audio_start_func,     on_audio_stream_stop=my_audio_stop_func,     niveau=logging.INFO )</code></p>"},{"location":"fr/api/#methodes","title":"M\u00e9thodes","text":""},{"location":"fr/api/#play-etplay_async","title":"<code>play et</code>play_async`","text":"<p>Ces m\u00e9thodes sont responsables de l'ex\u00e9cution de la synth\u00e8se texte-audio et de la lecture du flux audio. La diff\u00e9rence est que <code>play</code> est une fonction de blocage, tandis que <code>play_async</code> s'ex\u00e9cute dans un thread s\u00e9par\u00e9, permettant \u00e0 d'autres op\u00e9rations de se poursuivre.</p>"},{"location":"fr/api/#parametres","title":"Param\u00e8tres :","text":""},{"location":"fr/api/#fast_sentence_fragment-bool","title":"fast<code>_sentence_fragment</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>True</code></li> <li>Description : Lorsqu'elle est d\u00e9finie sur <code>True</code>, la m\u00e9thode donnera la priorit\u00e9 \u00e0 la vitesse, g\u00e9n\u00e9rant et jouant plus rapidement des fragments de phrases. Ceci est utile pour les applications o\u00f9 la latence est importante.</li> </ul>"},{"location":"fr/api/#fast_sentence_fragment_allsentencesbool","title":"fast<code>_sentence_fragment_allsentences</code>(bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Lorsqu'il est d\u00e9fini sur <code>True</code>, applique le traitement rapide des fragments de phrase \u00e0 toutes les phrases, pas seulement \u00e0 la premi\u00e8re.</li> </ul>"},{"location":"fr/api/#fast_sentence_fragment_allsentences_multiple-bool","title":"fast<code>_sentence_fragment_allsentences_multiple</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Lorsqu'il est d\u00e9fini sur <code>True</code>, permet de produire plusieurs fragments de phrase au lieu d'un seul.</li> </ul>"},{"location":"fr/api/#_threshold_seconds-flotteur","title":"<code>_threshold_seconds</code> (flotteur)","text":"<ul> <li>Par d\u00e9faut: <code>0.0</code></li> <li> <p>Description : Sp\u00e9cifie le temps en secondes pour le seuil de mise en m\u00e9moire tampon, ce qui a un impact sur la douceur et la continuit\u00e9 de la lecture audio.</p> </li> <li> <p>Comment \u00e7a marche : Avant de synth\u00e9tiser une nouvelle phrase, le syst\u00e8me v\u00e9rifie s'il reste plus de mat\u00e9riel audio dans le tampon que le temps sp\u00e9cifi\u00e9 par <code>buffer_threshold_seconds</code>. Si tel est le cas, il r\u00e9cup\u00e8re une autre phrase du g\u00e9n\u00e9rateur de texte, en supposant qu'il peut r\u00e9cup\u00e9rer et synth\u00e9tiser cette nouvelle phrase dans la fen\u00eatre temporelle fournie par l'audio restant dans le tampon. Ce processus permet au moteur de synth\u00e8se vocale d'avoir plus de contexte pour une meilleure synth\u00e8se, am\u00e9liorant ainsi l'exp\u00e9rience utilisateur.</p> </li> </ul> <p>Une valeur plus \u00e9lev\u00e9e garantit qu'il y a plus d'audio pr\u00e9-tamponn\u00e9, r\u00e9duisant ainsi le risque de silence ou de lacunes pendant la lecture. Si vous rencontrez des pauses ou des pauses, envisagez d'augmenter cette valeur.</p>"},{"location":"fr/api/#_sentence_length-int","title":"<code>_sentence_length</code> (int)","text":"<ul> <li>Par d\u00e9faut: <code>10</code></li> <li>Description : D\u00e9finit la longueur minimale des caract\u00e8res pour consid\u00e9rer une cha\u00eene comme une phrase \u00e0 synth\u00e9tiser. Cela affecte la fa\u00e7on dont les morceaux de texte sont trait\u00e9s et lus.</li> </ul>"},{"location":"fr/api/#_first_fragment_lengthint","title":"<code>_first_fragment_length</code>(int)","text":"<ul> <li>Par d\u00e9faut: <code>10</code></li> <li>Description : Le nombre minimum de caract\u00e8res requis pour le premier fragment de phrase avant de c\u00e9der.</li> </ul>"},{"location":"fr/api/#_synthesized_text-bool","title":"<code>_synthesized_text</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Lorsqu'il est activ\u00e9, enregistre les morceaux de texte au fur et \u00e0 mesure de leur synth\u00e8se en audio. Utile pour l'audit et le d\u00e9bogage.</li> </ul>"},{"location":"fr/api/#reset_generated_text-bool","title":"#reset_generated_text` (bool)","text":"<ul> <li>Par d\u00e9faut: <code>True</code></li> <li>Description : Si Vrai, r\u00e9initialisez le texte g\u00e9n\u00e9r\u00e9 avant le traitement.</li> </ul>"},{"location":"fr/api/#_wavfile-str","title":"<code>_wavfile</code> (str)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Si d\u00e9fini, enregistrez l'audio dans le fichier WAV sp\u00e9cifi\u00e9.</li> </ul>"},{"location":"fr/api/#_sentence_synthesized-appelable","title":"`_sentence_synthesized (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Une fonction de rappel appel\u00e9e apr\u00e8s un seul fragment de phrase a \u00e9t\u00e9 synth\u00e9tis\u00e9e.</li> </ul>"},{"location":"fr/api/#before_sentence_synthesized-appelable","title":"before`_sentence_synthesized (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Une fonction de rappel qui est appel\u00e9e avant qu'un seul fragment de phrase ne soit synth\u00e9tis\u00e9.</li> </ul>"},{"location":"fr/api/#_audio_chunk-appelable","title":"<code>_audio_chunk</code> (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Fonction de rappel qui est appel\u00e9e lorsqu'un seul morceau audio est pr\u00eat.</li> </ul>"},{"location":"fr/api/#str","title":"```(str)","text":"<ul> <li>Par d\u00e9faut:<code>\"nltk\"</code></li> <li>Description : Tokenizer \u00e0 utiliser pour le fractionnement des phrases. Prend actuellement en charge \u00ab nltk \u00bb et \u00ab stroza \u00bb.</li> </ul>"},{"location":"fr/api/#_sentences-appelable","title":"<code>_sentences</code> (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Une fonction personnalis\u00e9e qui tokenise les phrases du texte saisi. Vous pouvez fournir votre propre tokenizer l\u00e9ger si vous n'\u00eates pas satisfait de nltk et stanza. Il doit prendre du texte comme cha\u00eene et renvoyer des phrases divis\u00e9es comme liste de cha\u00eenes.</li> </ul>"},{"location":"fr/api/#angustr","title":"<code>angu</code>(str)","text":"<ul> <li>Par d\u00e9faut:<code>\"en\"</code></li> <li>Description : Langue \u00e0 utiliser pour le fractionnement des phrases.</li> </ul>"},{"location":"fr/api/#_sizeint","title":"<code>_size</code>(int)","text":"<ul> <li>Par d\u00e9faut: <code>12</code></li> <li>Description : Le nombre de caract\u00e8res utilis\u00e9s pour \u00e9tablir le contexte pour la d\u00e9tection des limites de phrase. Un contexte plus large am\u00e9liore la pr\u00e9cision de la d\u00e9tection des limites des phrases.</li> </ul>"},{"location":"fr/api/#_size_look_overhead-int","title":"<code>_size_look_overhead</code> (int)","text":"<ul> <li>Par d\u00e9faut: <code>12</code></li> <li>Description : Taille de contexte suppl\u00e9mentaire pour regarder vers l'avenir lors de la d\u00e9tection des limites des phrases.</li> </ul>"},{"location":"fr/api/#mute-bool","title":"<code>mute</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Si vrai, d\u00e9sactive la lecture audio via des haut-parleurs locaux. Utile lorsque vous souhaitez synth\u00e9tiser dans un fichier ou traiter des morceaux audio sans les lire.</li> </ul>"},{"location":"fr/api/#ence_fragment_delimiters-str","title":"<code>ence_fragment_delimiters</code> (str)","text":"<ul> <li>Par d\u00e9faut:<code>\"?!;::\\n...)]}-</code></li> <li>Description : Une cha\u00eene de caract\u00e8res qui sont consid\u00e9r\u00e9s comme des d\u00e9limiteurs de phrases.</li> </ul>"},{"location":"fr/api/#_first_fragment_after_words-int","title":"<code>_first_fragment_after_</code>words (int)","text":"<ul> <li>Par d\u00e9faut: <code>15</code></li> <li>Description : Le nombre de mots apr\u00e8s lesquels le fragment de la premi\u00e8re phrase est forc\u00e9 d'\u00eatre donn\u00e9.</li> </ul>"},{"location":"fr/contributing/","title":"Contribuer \u00e0 RealtimeTTS","text":"<p>Nous accueillons les contributions \u00e0 RealtimeTTS ! Voici quelques fa\u00e7ons dont vous pouvez contribuer :</p> <ol> <li> <p>Reporting Bugs : Si vous trouvez un bug, veuillez ouvrir un probl\u00e8me sur notre r\u00e9f\u00e9rentiel GitHub.</p> </li> <li> <p>** Suggestion d'am\u00e9liorations** : Vous avez des id\u00e9es de nouvelles fonctionnalit\u00e9s ou d'am\u00e9liorations ? Nous serions ravis de les entendre ! Ouvrez un num\u00e9ro pour sugg\u00e9rer des am\u00e9liorations.</p> </li> <li> <p>Code Contributions : Vous voulez ajouter une nouvelle fonctionnalit\u00e9 ou corriger un bug ? Super ! Veuillez suivre ces \u00e9tapes :</p> </li> <li>Fourcher le d\u00e9p\u00f4t</li> <li>Cr\u00e9ez une nouvelle branche pour votre fonctionnalit\u00e9</li> <li>Faites vos changements</li> <li> <p>Soumettez une demande pull avec une description claire de vos modifications</p> </li> <li> <p>Documentation : Aidez-nous \u00e0 am\u00e9liorer notre documentation en corrigeant les fautes de frappe, en ajoutant des exemples ou en clarifiant les sections d\u00e9routantes.</p> </li> <li> <p>Ajout de nouveaux moteurs : Si vous souhaitez ajouter la prise en charge d'un nouveau moteur TTS, veuillez d'abord ouvrir un num\u00e9ro pour discuter de l'impl\u00e9mentation.</p> </li> </ol> <p>Merci d'avoir contribu\u00e9 \u00e0 rendre RealtimeTTS meilleur !</p>"},{"location":"fr/faq/","title":"Foire aux questions","text":"<p>Pour les r\u00e9ponses aux questions fr\u00e9quemment pos\u00e9es sur RealtimeTTS, veuillez vous r\u00e9f\u00e9rer \u00e0 notre page FAQ sur GitHub.</p> <p>Cette page couvre divers sujets dont</p> <ul> <li>Utilisation de diff\u00e9rents moteurs TTS</li> <li>Manipulation de textes multilingues</li> <li>Optimisation des performances</li> <li>D\u00e9pannage des probl\u00e8mes courants</li> </ul> <p>Pour des informations plus d\u00e9taill\u00e9es, veuillez consulter le lien ci-dessus.</p>"},{"location":"fr/installation/","title":"Fran\u00e7ais","text":"<p>Remarque: Installation de base avec <code>pip install realtimetts</code>s n'est plus recommand\u00e9, utilisez <code>pip install realtimetts[all]</code> \u00e0 la place.</p> <p>La biblioth\u00e8que RealtimeTTS offre des options d'installation pour diverses d\u00e9pendances pour votre cas d'utilisation. Voici les diff\u00e9rentes fa\u00e7ons dont vous pouvez installer RealtimeTTS en fonction de vos besoins :</p>"},{"location":"fr/installation/#installation-complete","title":"Installation compl\u00e8te","text":"<p>Pour installer RealtimeTTS avec prise en charge de tous les moteurs TTS :</p> <p><code>pip install -U realtimetts [tous]</code></p>"},{"location":"fr/installation/#installation-personnalisee","title":"Installation personnalis\u00e9e","text":"<p>RealtimeTTS permet une installation personnalis\u00e9e avec un minimum d'installations de biblioth\u00e8que. Voici les options disponibles : - all : Installation compl\u00e8te avec chaque moteur pris en charge. - ** syst\u00e8me : Inclut les capacit\u00e9s TTS sp\u00e9cifiques au syst\u00e8me (par exemple, pyttsx3). - azure : ajoute le support vocal Azure Cognitive Services. - elevenlabs : Comprend l'int\u00e9gration avec l'API ElevenLabs. - openai : Pour les services vocaux OpenAI. - gtts : Prise en charge de Google Text-to-Speech. - coqui : Installe le moteur Coqui TTS. - minimal** : installe uniquement les exigences de base sans moteur (n\u00e9cessaire uniquement si vous souhaitez d\u00e9velopper votre propre moteur)</p> <p>Supposons que vous souhaitiez installer RealtimeTTS uniquement pour l'utilisation neuronale locale de Coqui TTS, vous devez alors utiliser :</p> <p><code>pip installez realtimetts [coqui]</code></p> <p>Par exemple, si vous souhaitez installer RealtimeTTS avec uniquement Azure Cognitive Services Speech, ElevenLabs et la prise en charge d'OpenAI :</p> <p><code>pip installez realtimetts[azure,elevenlabs,openai]</code></p>"},{"location":"fr/installation/#installation-de-lenvironnement-virtuel","title":"Installation de l'environnement virtuel","text":"<p>Pour ceux qui souhaitent effectuer une installation compl\u00e8te dans un environnement virtuel, proc\u00e9dez comme suit</p> <p><code>python - m venv env_realtimetts env_realtimetts\\Scripts\\activate.bat python.exe - m pip install - upgrade pip pip install -U realtimetts [tous]</code></p> <p>Plus d'informations sur installation CUDA.</p>"},{"location":"fr/installation/#exigences-du-moteur","title":"Exigences du moteur","text":"<p>Diff\u00e9rents moteurs pris en charge par RealtimeTTS ont des exigences uniques. Assurez-vous de remplir ces exigences en fonction du moteur que vous choisissez.</p>"},{"location":"fr/installation/#moteur-systeme","title":"Moteur syst\u00e8me","text":"<p>Le `SystemEngine fonctionne d\u00e8s le d\u00e9part avec les capacit\u00e9s TTS int\u00e9gr\u00e9es de votre syst\u00e8me. Aucune configuration suppl\u00e9mentaire n'est n\u00e9cessaire.</p>"},{"location":"fr/installation/#gttsengine","title":"GTTSEngine","text":"<p>Le <code>GTTSEngine</code> fonctionne d\u00e8s le d\u00e9part \u00e0 l'aide de l'API de synth\u00e8se vocale de Google Translate. Aucune configuration suppl\u00e9mentaire n'est n\u00e9cessaire.</p>"},{"location":"fr/installation/#openaiengine","title":"OpenAIEngine","text":"<p>Pour utiliser le ``(OpenAIE): - d\u00e9finir la variable d'environnement OPENAI_API_KEY - installer ffmpeg (voir installation CUDA point 3)</p>"},{"location":"fr/installation/#azureengine","title":"AzureEngine","text":"<p>Pour utiliser le <code>ine</code>, vous aurez besoin de : - Cl\u00e9 API Microsoft Azure Text-to-Speech (fournie via le param\u00e8tre constructeur AzureEngine \u00ab speech_key \u00bb ou dans la variable d'environnement AZURE_SPEECH_KEY) - R\u00e9gion de service Microsoft Azure.</p> <p>Assurez-vous d'avoir ces informations d'identification disponibles et correctement configur\u00e9es lors de l'initialisation du <code>AzureEngine</code>.</p>"},{"location":"fr/installation/#elevenlabsengine","title":"ElevenlabsEngine","text":"<p>Pour le <code>ElevenlabsEngine</code>, vous avez besoin de: - Cl\u00e9 API Elevenlabs (fournie via le param\u00e8tre constructeur ElevenlabsEngine \u00ab api_key \u00bb ou dans la variable d'environnement ELEVENLABS_API_KEY) - <code>mpv</code> installed on your system (essential for streaming mpeg audio, Elevenlabs ne d\u00e9livre que mpeg).</p>"},{"location":"fr/installation/#elevenlabsengine_1","title":"ElevenlabsEngine","text":"<p>Pour le <code>ElevenlabsEngine</code>, vous avez besoin de: - Cl\u00e9 API Elevenlabs (fournie via le param\u00e8tre constructeur ElevenlabsEngine \u00ab api_key \u00bb ou dans la variable d'environnement ELEVENLABS_API_KEY) - <code>mpv</code> installed on your system (essential for streaming mpeg audio, Elevenlabs ne d\u00e9livre que mpeg).</p> <p>\ud83d\udd39 Installation <code>v</code>:   - macOS:     <code>infuser installer mpv</code></p> <ul> <li>Linux et Windows : Visitez mpv.io pour les instructions d'installation.</li> </ul>"},{"location":"fr/installation/#coquiengine","title":"CoquiEngine","text":"<p>Offre un TTS neuronal local de haute qualit\u00e9 avec clonage vocal.</p> <p>T\u00e9l\u00e9charge d'abord un mod\u00e8le TTS neuronal. Dans la plupart des cas, il est suffisamment rapide pour le temps r\u00e9el utilisant la synth\u00e8se GPU. N\u00e9cessite environ 4 \u00e0 5 Go de VRAM.</p> <ul> <li>pour cloner une voix, soumettez le nom de fichier d'un fichier d'onde contenant la voix source comme param\u00e8tre \u00ab voix \u00bb au constructeur CoquiEngine</li> <li>le clonage vocal fonctionne mieux avec un fichier WAV mono 16 bits de 22 050 Hz contenant un \u00e9chantillon court (~5 \u00e0 30 secondes)</li> </ul> <p>Sur la plupart des syst\u00e8mes, la prise en charge du GPU sera n\u00e9cessaire pour fonctionner suffisamment rapidement en temps r\u00e9el, sinon vous ferez l'exp\u00e9rience du b\u00e9gaiement.</p>"},{"location":"fr/installation/#installation-cuda","title":"Installation CUDA","text":"<p>Ces \u00e9tapes sont recommand\u00e9es pour ceux qui ont besoin de ** meilleures performances ** et disposent d'un GPU NVIDIA compatible.</p> <p>Remarque : pour v\u00e9rifier si votre GPU NVIDIA prend en charge CUDA, visitez la liste officielle des GPU CUDA.</p> <p>Pour utiliser une torche avec support via CUDA, veuillez suivre ces \u00e9tapes :</p> <p>Remarque : les installations de pythorque plus r\u00e9centes peuvent (non v\u00e9rifi\u00e9) n'ont plus besoin d'installation de Toolkit (et \u00e9ventuellement de cuDNN).</p> <ol> <li> <p>Installer NVIDIA CUDA Toolkit:     Par exemple, pour installer Toolkit 12.X, s'il te pla\u00eet</p> <ul> <li>Visitez NVIDIA CUDA T\u00e9l\u00e9chargements.</li> <li>S\u00e9lectionnez votre syst\u00e8me d'exploitation, votre architecture syst\u00e8me et votre version os.</li> <li>T\u00e9l\u00e9chargez et installez le logiciel.</li> </ul> <p>ou pour installer Toolkit 11.8, s'il vous pla\u00eet - Visitez Archive de la bo\u00eete \u00e0 outils CUDA NVIDIA. - S\u00e9lectionnez votre syst\u00e8me d'exploitation, votre architecture syst\u00e8me et votre version os. - T\u00e9l\u00e9chargez et installez le logiciel.</p> </li> <li> <p>Installer NVIDIA cuDNN:</p> <p>Par exemple, pour installer cuDNN 8.7.0 pour CUDA 11. x s'il vous pla\u00eet - Visitez NVIDIA cuDNN Archive. - Cliquez sur \u00ab T\u00e9l\u00e9charger cuDNN v8.7.0 (28 novembre 2022), pour CUDA 11.x \u00bb. - T\u00e9l\u00e9chargez et installez le logiciel.</p> </li> <li> <p>Installer ffmpeg:</p> <p>Vous pouvez t\u00e9l\u00e9charger un programme d'installation pour votre syst\u00e8me d'exploitation \u00e0 partir du site Web deffmpeg.</p> <p>Ou utilisez un gestionnaire de packages :</p> <ul> <li> <p>Sur Ubuntu ou Debian:     <code>sudo apt update &amp; &amp; sudo apt install ffmpeg</code></p> </li> <li> <p>Sur Arch Linux:     <code>sudo pacman -S ffmpeg</code></p> </li> <li> <p>Sur MacOS utilisant Homebrew (https://brew.sh/):     <code>infuser installer ffmpeg</code></p> </li> <li> <p>Sur Windows utilisant Chocolatey (https://chocolatey.org/):     <code>choco installer ffmpeg</code></p> </li> <li> <p>Sur Windows utilisant Scoop (https://scoop.sh/):     <code>scoop installer ffmpeg</code></p> </li> </ul> </li> <li> <p>Installez PyTorch avec le support CUDA :</p> <p>Pour mettre \u00e0 niveau votre installation PyTorch afin d'activer le support GPU avec CUDA, suivez ces instructions en fonction de votre version CUDA sp\u00e9cifique. Ceci est utile si vous souhaitez am\u00e9liorer les performances de RealtimeSTT avec les capacit\u00e9s CUDA.</p> <ul> <li> <p>Pour CUDA 11.8:</p> <p>Pour mettre \u00e0 jour PyTorch et Torchaudio afin de prendre en charge CUDA 11.8, utilisez les commandes suivantes :</p> <p><code>pip installe torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p> </li> <li> <p>Pour CUDA 12.X:</p> <p>Pour mettre \u00e0 jour PyTorch et Torchaudio pour prendre en charge CUDA 12.X, ex\u00e9cutez ce qui suit :</p> <p><code>pip installe torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code></p> </li> </ul> <p>Remplacer <code></code> ` of PyTorch that matching your system and requirements.</p> </li> <li> <p>** Correction pour r\u00e9soudre les probl\u00e8mes de compatibilit\u00e9** :     Si vous rencontrez des probl\u00e8mes de compatibilit\u00e9 de biblioth\u00e8que, essayez de d\u00e9finir ces biblioth\u00e8ques sur des versions fixes :</p> </li> </ol> <p>``` </p> <pre><code>pip install networkx==2.8.8\n\npip install typing_extensions==4.8.0\n\npip install fsspec==2023.6.0\n\npip install imageio==2.31.6\n\npip install networkx==2.8.8\n\npip install numpy==1.24.3\n\npip install requests==2.31.0\n</code></pre> <p>```</p>"},{"location":"fr/usage/","title":"Utilisation","text":""},{"location":"fr/usage/#demarrage-rapide","title":"D\u00e9marrage rapide","text":"<p>Voici un exemple d'utilisation de base :</p> <p><code>```(</code>python depuis RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine</p> <p>moteur = SystemEngine () # remplacer par votre moteur TTS flux = TextToAudioStream(moteur) stream.feed(\"Bonjour le monde! Comment \u00e7a va aujourd'hui ?\") stream.play_async() ``</p>"},{"location":"fr/usage/#flux-texte","title":"Flux Texte","text":"<p>Vous pouvez alimenter des cha\u00eenes individuelles :</p> <p><code>``(`python stream.feed(\u00ab Bonjour, c'est une phrase. \u00bb)</code></p> <p>Ou vous pouvez alimenter des g\u00e9n\u00e9rateurs et des it\u00e9rateurs de caract\u00e8res pour le streaming en temps r\u00e9el :</p> <p><code>```(</code>python def write (prompt : str) :     pour chunk en openai.ChatCompletion.create(         mod\u00e8le=\"gpt-3.5-turbo\",         messages=[{\"role\": \"utilisateur\", \"contenu\" : prompt}],         stream=True     ):         si (text_chunk := chunk[\u00ab choix \u00bb][0][\u00ab delta \u00bb].get(\u00ab contenu \u00bb)) n'est pas Aucun :             produire du texte_chunk</p> <p>text_stream = write (\u00ab Un discours relaxant en trois phrases \u00bb)</p> <p>stream.feed(text_stream) ``</p> <p><code>``(`python char_iterator = iter (\u00ab Diffusion de ce personnage par personnage \u00bb) stream.feed (char_iterator)</code></p>"},{"location":"fr/usage/#layback","title":"Layback","text":"<p>Asynchrone:</p> <p><code>``(`python stream.play_async() pendant que stream.is_playing():     temps.sommeil(0,1)</code></p> <p>Synchronis\u00e9:</p> <p><code>``(`python stream.play()</code></p>"},{"location":"fr/usage/#tester-la-bibliotheque","title":"Tester la biblioth\u00e8que","text":"<p>Le sous-r\u00e9pertoire de test contient un ensemble de scripts pour vous aider \u00e0 \u00e9valuer et comprendre les capacit\u00e9s de la biblioth\u00e8que RealtimeTTS.</p> <p>Notez que la plupart des tests reposent toujours sur l'\u00ab ancienne \u00bb API OpenAI (&lt;1.0.0). L'utilisation de la nouvelle API OpenAI est d\u00e9montr\u00e9e dans openai_1.0_test.py.</p> <ul> <li> <p>simple_test.py</p> <ul> <li>Description : Une d\u00e9monstration de style \u00ab hello world \u00bb de l'usage le plus simple de la biblioth\u00e8que.</li> </ul> </li> <li> <p>complex_test.py</p> <ul> <li>Description : Une d\u00e9monstration compl\u00e8te pr\u00e9sentant la plupart des fonctionnalit\u00e9s fournies par la biblioth\u00e8que.</li> </ul> </li> <li> <p>coqui_test.py</p> <ul> <li>Description : Test du moteur local coqui TTS.</li> </ul> </li> <li> <p>traducteur.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai realtimestt</code>.</li> <li>Description : Traductions en temps r\u00e9el dans six langues diff\u00e9rentes.</li> </ul> </li> <li> <p>openai_voice_interface.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai realtimestt</code>.</li> <li>Description : Interface utilisateur activ\u00e9e par mot de r\u00e9veil et bas\u00e9e sur la voix vers l'API OpenAI.</li> </ul> </li> <li> <p>advanced_talk.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai keyboard realtimestt</code>.</li> <li>Description : Choisissez le moteur et la voix TTS avant de d\u00e9marrer la conversation sur l'IA.</li> </ul> </li> <li> <p>_talkbot.py minimaliste</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai realtimestt</code>.</li> <li>Description : Un talkbot basique en 20 lignes de code.</li> </ul> </li> <li> <p>simple_llm_test.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai</code>.</li> <li>Description : D\u00e9monstration simple de la fa\u00e7on d'int\u00e9grer la biblioth\u00e8que avec de grands mod\u00e8les de langage (LLM).</li> </ul> </li> <li> <p>test_callbacks.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai</code>.</li> <li>Description : pr\u00e9sente les rappels et vous permet de v\u00e9rifier les temps de latence dans un environnement d'application r\u00e9el.</li> </ul> </li> </ul>"},{"location":"fr/usage/#mettre-en-pause-reprendre-et-arreter","title":"Mettre en pause, reprendre et arr\u00eater","text":"<p>Mettre en pause le flux audio :</p> <p><code>``(`python stream.pause()</code></p> <p>Reprendre un flux en pause :</p> <p><code>``(`python stream.reprendre()</code></p> <p>Arr\u00eatez imm\u00e9diatement le flux :</p> <p><code>``(`python stream.stop()</code></p>"},{"location":"fr/usage/#exigences-expliquees","title":"Exigences expliqu\u00e9es","text":"<ul> <li>Version Python:</li> <li>Obligatoire: Python &gt;= 3.9, &lt; 3.13</li> <li> <p>Raison : La biblioth\u00e8que d\u00e9pend de la biblioth\u00e8que GitHub \u00ab TTS \u00bb de coqui, qui n\u00e9cessite des versions Python dans cette gamme.</p> </li> <li> <p>PyAudio : pour cr\u00e9er un flux audio de sortie</p> </li> <li> <p>stream2sent : pour diviser le flux de texte entrant en phrases</p> </li> <li> <p>pyttsx3 : Moteur de conversion texte-parole du syst\u00e8me</p> </li> <li> <p>pydub : pour convertir les formats de morceaux audio</p> </li> <li> <p>azure-cognitiveservices-speech : Moteur de conversion texte-parole azur</p> </li> <li> <p>elevenlabs : Moteur de conversion texte-parole Elevenlabs</p> </li> <li> <p>coqui-TTS : Biblioth\u00e8que de synth\u00e8se vocale XTTS de Coqui pour un TTS neuronal local de haute qualit\u00e9</p> </li> </ul> <p>Criez \u00e0 Idiap Research Institute pour entretenir une fourche de coqui tts.</p> <ul> <li> <p>openai : pour interagir avec l'API TTS d'OpenAI</p> </li> <li> <p>gtts : Google traduit la conversion texte-parole</p> </li> </ul>"},{"location":"fr/en/","title":"RealtimeTTS","text":"<p>EN | FR | ES</p> <p>Easy to use, low-latency text-to-speech library for realtime applications</p>"},{"location":"fr/en/#about-the-project","title":"About the Project","text":"<p>RealtimeTTS is a state-of-the-art text-to-speech (TTS) library designed for real-time applications. It stands out in its ability to convert text streams fast into high-quality auditory output with minimal latency.</p>"},{"location":"fr/en/#key-features","title":"Key Features","text":"<ul> <li>Low Latency: almost instantaneous text-to-speech conversion, compatible with LLM outputs</li> <li>High-Quality Audio: generates clear and natural-sounding speech</li> <li>Multiple TTS Engine Support: supports OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS and System TTS</li> <li>Multilingual</li> <li>Robust and Reliable: ensures continuous operation through a fallback mechanism, switches to alternative engines in case of disruptions guaranteeing consistent performance and reliability</li> </ul> <p>For installation instructions, usage examples, and API reference, please navigate through the documentation using the sidebar.</p>"},{"location":"fr/en/api/","title":"English","text":""},{"location":"fr/en/api/#configuration","title":"Configuration","text":""},{"location":"fr/en/api/#initialization-parameters-for-texttoaudiostream","title":"Initialization Parameters for <code>TextToAudioStream</code>","text":"<p>When you initialize the <code>TextToAudioStream</code> class, you have various options to customize its behavior. Here are the available parameters:</p>"},{"location":"fr/en/api/#engine-baseengine","title":"<code>engine</code> (BaseEngine)","text":"<ul> <li>Type: BaseEngine</li> <li>Required: Yes</li> <li>Description: The underlying engine responsible for text-to-audio synthesis. You must provide an instance of <code>BaseEngine</code> or its subclass to enable audio synthesis.</li> </ul>"},{"location":"fr/en/api/#on_text_stream_start-callable","title":"<code>on_text_stream_start</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is triggered when the text stream begins. Use it for any setup or logging you may need.</li> </ul>"},{"location":"fr/en/api/#on_text_stream_stop-callable","title":"<code>on_text_stream_stop</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is activated when the text stream ends. You can use this for cleanup tasks or logging.</li> </ul>"},{"location":"fr/en/api/#on_audio_stream_start-callable","title":"<code>on_audio_stream_start</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is invoked when the audio stream starts. Useful for UI updates or event logging.</li> </ul>"},{"location":"fr/en/api/#on_audio_stream_stop-callable","title":"<code>on_audio_stream_stop</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is called when the audio stream stops. Ideal for resource cleanup or post-processing tasks.</li> </ul>"},{"location":"fr/en/api/#on_character-callable","title":"<code>on_character</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is called when a single character is processed.</li> </ul>"},{"location":"fr/en/api/#output_device_index-int","title":"<code>output_device_index</code> (int)","text":"<ul> <li>Type: Integer</li> <li>Required: No</li> <li>Default: None</li> <li>Description: Specifies the output device index to use. None uses the default device.</li> </ul>"},{"location":"fr/en/api/#tokenizer-string","title":"<code>tokenizer</code> (string)","text":"<ul> <li>Type: String</li> <li>Required: No</li> <li>Default: nltk</li> <li>Description: Tokenizer to use for sentence splitting (currently \"nltk\" and \"stanza\" are supported).</li> </ul>"},{"location":"fr/en/api/#language-string","title":"<code>language</code> (string)","text":"<ul> <li>Type: String</li> <li>Required: No</li> <li>Default: en</li> <li>Description: Language to use for sentence splitting.</li> </ul>"},{"location":"fr/en/api/#muted-bool","title":"<code>muted</code> (bool)","text":"<ul> <li>Type: Bool</li> <li>Required: No</li> <li>Default: False</li> <li>Description: Global muted parameter. If True, no pyAudio stream will be opened. Disables audio playback via local speakers (in case you want to synthesize to file or process audio chunks) and overrides the play parameters muted setting.</li> </ul>"},{"location":"fr/en/api/#level-int","title":"<code>level</code> (int)","text":"<ul> <li>Type: Integer</li> <li>Required: No</li> <li>Default: <code>logging.WARNING</code></li> <li>Description: Sets the logging level for the internal logger. This can be any integer constant from Python's built-in <code>logging</code> module.</li> </ul>"},{"location":"fr/en/api/#example-usage","title":"Example Usage:","text":"<pre><code>engine = YourEngine()  # Substitute with your engine\nstream = TextToAudioStream(\n    engine=engine,\n    on_text_stream_start=my_text_start_func,\n    on_text_stream_stop=my_text_stop_func,\n    on_audio_stream_start=my_audio_start_func,\n    on_audio_stream_stop=my_audio_stop_func,\n    level=logging.INFO\n)\n</code></pre>"},{"location":"fr/en/api/#methods","title":"Methods","text":""},{"location":"fr/en/api/#play-and-play_async","title":"<code>play</code> and <code>play_async</code>","text":"<p>These methods are responsible for executing the text-to-audio synthesis and playing the audio stream. The difference is that <code>play</code> is a blocking function, while <code>play_async</code> runs in a separate thread, allowing other operations to proceed.</p>"},{"location":"fr/en/api/#parameters","title":"Parameters:","text":""},{"location":"fr/en/api/#fast_sentence_fragment-bool","title":"<code>fast_sentence_fragment</code> (bool)","text":"<ul> <li>Default: <code>True</code></li> <li>Description: When set to <code>True</code>, the method will prioritize speed, generating and playing sentence fragments faster. This is useful for applications where latency matters.</li> </ul>"},{"location":"fr/en/api/#fast_sentence_fragment_allsentences-bool","title":"<code>fast_sentence_fragment_allsentences</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: When set to <code>True</code>, applies the fast sentence fragment processing to all sentences, not just the first one.</li> </ul>"},{"location":"fr/en/api/#fast_sentence_fragment_allsentences_multiple-bool","title":"<code>fast_sentence_fragment_allsentences_multiple</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: When set to <code>True</code>, allows yielding multiple sentence fragments instead of just a single one.</li> </ul>"},{"location":"fr/en/api/#buffer_threshold_seconds-float","title":"<code>buffer_threshold_seconds</code> (float)","text":"<ul> <li>Default: <code>0.0</code></li> <li> <p>Description: Specifies the time in seconds for the buffering threshold, which impacts the smoothness and continuity of audio playback.</p> </li> <li> <p>How it Works: Before synthesizing a new sentence, the system checks if there is more audio material left in the buffer than the time specified by <code>buffer_threshold_seconds</code>. If so, it retrieves another sentence from the text generator, assuming that it can fetch and synthesize this new sentence within the time window provided by the remaining audio in the buffer. This process allows the text-to-speech engine to have more context for better synthesis, enhancing the user experience.</p> </li> </ul> <p>A higher value ensures that there's more pre-buffered audio, reducing the likelihood of silence or gaps during playback. If you experience breaks or pauses, consider increasing this value.</p>"},{"location":"fr/en/api/#minimum_sentence_length-int","title":"<code>minimum_sentence_length</code> (int)","text":"<ul> <li>Default: <code>10</code></li> <li>Description: Sets the minimum character length to consider a string as a sentence to be synthesized. This affects how text chunks are processed and played.</li> </ul>"},{"location":"fr/en/api/#minimum_first_fragment_length-int","title":"<code>minimum_first_fragment_length</code> (int)","text":"<ul> <li>Default: <code>10</code></li> <li>Description: The minimum number of characters required for the first sentence fragment before yielding.</li> </ul>"},{"location":"fr/en/api/#log_synthesized_text-bool","title":"<code>log_synthesized_text</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: When enabled, logs the text chunks as they are synthesized into audio. Helpful for auditing and debugging.</li> </ul>"},{"location":"fr/en/api/#reset_generated_text-bool","title":"<code>reset_generated_text</code> (bool)","text":"<ul> <li>Default: <code>True</code></li> <li>Description: If True, reset the generated text before processing.</li> </ul>"},{"location":"fr/en/api/#output_wavfile-str","title":"<code>output_wavfile</code> (str)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: If set, save the audio to the specified WAV file.</li> </ul>"},{"location":"fr/en/api/#on_sentence_synthesized-callable","title":"<code>on_sentence_synthesized</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: A callback function that gets called after a single sentence fragment was synthesized.</li> </ul>"},{"location":"fr/en/api/#before_sentence_synthesized-callable","title":"<code>before_sentence_synthesized</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: A callback function that gets called before a single sentence fragment gets synthesized.</li> </ul>"},{"location":"fr/en/api/#on_audio_chunk-callable","title":"<code>on_audio_chunk</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: Callback function that gets called when a single audio chunk is ready.</li> </ul>"},{"location":"fr/en/api/#tokenizer-str","title":"<code>tokenizer</code> (str)","text":"<ul> <li>Default: <code>\"nltk\"</code></li> <li>Description: Tokenizer to use for sentence splitting. Currently supports \"nltk\" and \"stanza\".</li> </ul>"},{"location":"fr/en/api/#tokenize_sentences-callable","title":"<code>tokenize_sentences</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: A custom function that tokenizes sentences from the input text. You can provide your own lightweight tokenizer if you are unhappy with nltk and stanza. It should take text as a string and return split sentences as a list of strings.</li> </ul>"},{"location":"fr/en/api/#language-str","title":"<code>language</code> (str)","text":"<ul> <li>Default: <code>\"en\"</code></li> <li>Description: Language to use for sentence splitting.</li> </ul>"},{"location":"fr/en/api/#context_size-int","title":"<code>context_size</code> (int)","text":"<ul> <li>Default: <code>12</code></li> <li>Description: The number of characters used to establish context for sentence boundary detection. A larger context improves the accuracy of detecting sentence boundaries.</li> </ul>"},{"location":"fr/en/api/#context_size_look_overhead-int","title":"<code>context_size_look_overhead</code> (int)","text":"<ul> <li>Default: <code>12</code></li> <li>Description: Additional context size for looking ahead when detecting sentence boundaries.</li> </ul>"},{"location":"fr/en/api/#muted-bool_1","title":"<code>muted</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: If True, disables audio playback via local speakers. Useful when you want to synthesize to a file or process audio chunks without playing them.</li> </ul>"},{"location":"fr/en/api/#sentence_fragment_delimiters-str","title":"<code>sentence_fragment_delimiters</code> (str)","text":"<ul> <li>Default: <code>\".?!;:,\\n\u2026)]}\u3002-\"</code></li> <li>Description: A string of characters that are considered sentence delimiters.</li> </ul>"},{"location":"fr/en/api/#force_first_fragment_after_words-int","title":"<code>force_first_fragment_after_words</code> (int)","text":"<ul> <li>Default: <code>15</code></li> <li>Description: The number of words after which the first sentence fragment is forced to be yielded.</li> </ul>"},{"location":"fr/en/contributing/","title":"Contributing to RealtimeTTS","text":"<p>We welcome contributions to RealtimeTTS! Here are some ways you can contribute:</p> <ol> <li> <p>Reporting Bugs: If you find a bug, please open an issue on our GitHub repository.</p> </li> <li> <p>Suggesting Enhancements: Have ideas for new features or improvements? We'd love to hear them! Open an issue to suggest enhancements.</p> </li> <li> <p>Code Contributions: Want to add a new feature or fix a bug? Great! Please follow these steps:</p> </li> <li>Fork the repository</li> <li>Create a new branch for your feature</li> <li>Make your changes</li> <li> <p>Submit a pull request with a clear description of your changes</p> </li> <li> <p>Documentation: Help us improve our documentation by fixing typos, adding examples, or clarifying confusing sections.</p> </li> <li> <p>Adding New Engines: If you want to add support for a new TTS engine, please open an issue first to discuss the implementation.</p> </li> </ol> <p>Thank you for helping make RealtimeTTS better!</p>"},{"location":"fr/en/faq/","title":"Frequently Asked Questions","text":"<p>For answers to frequently asked questions about RealtimeTTS, please refer to our FAQ page on GitHub.</p> <p>This page covers various topics including:</p> <ul> <li>Usage of different TTS engines</li> <li>Handling of multilingual text</li> <li>Performance optimization</li> <li>Troubleshooting common issues</li> </ul> <p>For more detailed information, please visit the link above.</p>"},{"location":"fr/en/installation/","title":"English","text":"<p>Note: Basic Installation with <code>pip install realtimetts</code> is not recommended anymore, use <code>pip install realtimetts[all]</code> instead.</p> <p>The RealtimeTTS library provides installation options for various dependencies for your use case. Here are the different ways you can install RealtimeTTS depending on your needs:</p>"},{"location":"fr/en/installation/#full-installation","title":"Full Installation","text":"<p>To install RealtimeTTS with support for all TTS engines:</p> <pre><code>pip install -U realtimetts[all]\n</code></pre>"},{"location":"fr/en/installation/#custom-installation","title":"Custom Installation","text":"<p>RealtimeTTS allows for custom installation with minimal library installations. Here are the options available: - all: Full installation with every engine supported. - system: Includes system-specific TTS capabilities (e.g., pyttsx3). - azure: Adds Azure Cognitive Services Speech support. - elevenlabs: Includes integration with ElevenLabs API. - openai: For OpenAI voice services. - gtts: Google Text-to-Speech support. - coqui: Installs the Coqui TTS engine. - minimal: Installs only the base requirements with no engine (only needed if you want to develop an own engine)</p> <p>Say you want to install RealtimeTTS only for local neuronal Coqui TTS usage, then you should use:</p> <pre><code>pip install realtimetts[coqui]\n</code></pre> <p>For example, if you want to install RealtimeTTS with only Azure Cognitive Services Speech, ElevenLabs, and OpenAI support:</p> <pre><code>pip install realtimetts[azure,elevenlabs,openai]\n</code></pre>"},{"location":"fr/en/installation/#virtual-environment-installation","title":"Virtual Environment Installation","text":"<p>For those who want to perform a full installation within a virtual environment, follow these steps:</p> <pre><code>python -m venv env_realtimetts\nenv_realtimetts\\Scripts\\activate.bat\npython.exe -m pip install --upgrade pip\npip install -U realtimetts[all]\n</code></pre> <p>More information about CUDA installation.</p>"},{"location":"fr/en/installation/#engine-requirements","title":"Engine Requirements","text":"<p>Different engines supported by RealtimeTTS have unique requirements. Ensure you fulfill these requirements based on the engine you choose.</p>"},{"location":"fr/en/installation/#systemengine","title":"SystemEngine","text":"<p>The <code>SystemEngine</code> works out of the box with your system's built-in TTS capabilities. No additional setup is needed.</p>"},{"location":"fr/en/installation/#gttsengine","title":"GTTSEngine","text":"<p>The <code>GTTSEngine</code> works out of the box using Google Translate's text-to-speech API. No additional setup is needed.</p>"},{"location":"fr/en/installation/#openaiengine","title":"OpenAIEngine","text":"<p>To use the <code>OpenAIEngine</code>: - set environment variable OPENAI_API_KEY - install ffmpeg (see CUDA installation point 3)</p>"},{"location":"fr/en/installation/#azureengine","title":"AzureEngine","text":"<p>To use the <code>AzureEngine</code>, you will need: - Microsoft Azure Text-to-Speech API key (provided via AzureEngine constructor parameter \"speech_key\" or in the environment variable AZURE_SPEECH_KEY) - Microsoft Azure service region.</p> <p>Make sure you have these credentials available and correctly configured when initializing the <code>AzureEngine</code>.</p>"},{"location":"fr/en/installation/#elevenlabsengine","title":"ElevenlabsEngine","text":"<p>For the <code>ElevenlabsEngine</code>, you need: - Elevenlabs API key (provided via ElevenlabsEngine constructor parameter \"api_key\" or in the environment variable ELEVENLABS_API_KEY) - <code>mpv</code> installed on your system (essential for streaming mpeg audio, Elevenlabs only delivers mpeg).</p> <p>\ud83d\udd39 Installing <code>mpv</code>:   - macOS:     <code>brew install mpv</code></p> <ul> <li>Linux and Windows: Visit mpv.io for installation instructions.</li> </ul>"},{"location":"fr/en/installation/#coquiengine","title":"CoquiEngine","text":"<p>Delivers high quality, local, neural TTS with voice-cloning.</p> <p>Downloads a neural TTS model first. In most cases it be fast enough for Realtime using GPU synthesis. Needs around 4-5 GB VRAM.</p> <ul> <li>to clone a voice submit the filename of a wave file containing the source voice as \"voice\" parameter to the CoquiEngine constructor</li> <li>voice cloning works best with a 22050 Hz mono 16bit WAV file containing a short (~5-30 sec) sample</li> </ul> <p>On most systems GPU support will be needed to run fast enough for realtime, otherwise you will experience stuttering.</p>"},{"location":"fr/en/installation/#cuda-installation","title":"CUDA installation","text":"<p>These steps are recommended for those who require better performance and have a compatible NVIDIA GPU.</p> <p>Note: to check if your NVIDIA GPU supports CUDA, visit the official CUDA GPUs list.</p> <p>To use a torch with support via CUDA please follow these steps:</p> <p>Note: newer pytorch installations may (unverified) not need Toolkit (and possibly cuDNN) installation anymore.</p> <ol> <li> <p>Install NVIDIA CUDA Toolkit:     For example, to install Toolkit 12.X, please</p> <ul> <li>Visit NVIDIA CUDA Downloads.</li> <li>Select your operating system, system architecture, and os version.</li> <li>Download and install the software.</li> </ul> <p>or to install Toolkit 11.8, please - Visit NVIDIA CUDA Toolkit Archive. - Select your operating system, system architecture, and os version. - Download and install the software.</p> </li> <li> <p>Install NVIDIA cuDNN:</p> <p>For example, to install cuDNN 8.7.0 for CUDA 11.x please - Visit NVIDIA cuDNN Archive. - Click on \"Download cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x\". - Download and install the software.</p> </li> <li> <p>Install ffmpeg:</p> <p>You can download an installer for your OS from the ffmpeg Website.</p> <p>Or use a package manager:</p> <ul> <li> <p>On Ubuntu or Debian:     <code>sudo apt update &amp;&amp; sudo apt install ffmpeg</code></p> </li> <li> <p>On Arch Linux:     <code>sudo pacman -S ffmpeg</code></p> </li> <li> <p>On MacOS using Homebrew (https://brew.sh/):     <code>brew install ffmpeg</code></p> </li> <li> <p>On Windows using Chocolatey (https://chocolatey.org/):     <code>choco install ffmpeg</code></p> </li> <li> <p>On Windows using Scoop (https://scoop.sh/):     <code>scoop install ffmpeg</code></p> </li> </ul> </li> <li> <p>Install PyTorch with CUDA support:</p> <p>To upgrade your PyTorch installation to enable GPU support with CUDA, follow these instructions based on your specific CUDA version. This is useful if you wish to enhance the performance of RealtimeSTT with CUDA capabilities.</p> <ul> <li> <p>For CUDA 11.8:</p> <p>To update PyTorch and Torchaudio to support CUDA 11.8, use the following commands:</p> <p><code>pip install torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p> </li> <li> <p>For CUDA 12.X:</p> <p>To update PyTorch and Torchaudio to support CUDA 12.X, execute the following:</p> <p><code>pip install torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code></p> </li> </ul> <p>Replace <code>2.3.1</code> with the version of PyTorch that matches your system and requirements.</p> </li> <li> <p>Fix for to resolve compatibility issues:     If you run into library compatibility issues, try setting these libraries to fixed versions:</p> </li> </ol> <p>``` </p> <pre><code>pip install networkx==2.8.8\n\npip install typing_extensions==4.8.0\n\npip install fsspec==2023.6.0\n\npip install imageio==2.31.6\n\npip install networkx==2.8.8\n\npip install numpy==1.24.3\n\npip install requests==2.31.0\n</code></pre> <p>```</p>"},{"location":"fr/en/usage/","title":"Usage","text":""},{"location":"fr/en/usage/#quick-start","title":"Quick Start","text":"<p>Here's a basic usage example:</p> <pre><code>from RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine\n\nengine = SystemEngine() # replace with your TTS engine\nstream = TextToAudioStream(engine)\nstream.feed(\"Hello world! How are you today?\")\nstream.play_async()\n</code></pre>"},{"location":"fr/en/usage/#feed-text","title":"Feed Text","text":"<p>You can feed individual strings:</p> <pre><code>stream.feed(\"Hello, this is a sentence.\")\n</code></pre> <p>Or you can feed generators and character iterators for real-time streaming:</p> <pre><code>def write(prompt: str):\n    for chunk in openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\" : prompt}],\n        stream=True\n    ):\n        if (text_chunk := chunk[\"choices\"][0][\"delta\"].get(\"content\")) is not None:\n            yield text_chunk\n\ntext_stream = write(\"A three-sentence relaxing speech.\")\n\nstream.feed(text_stream)\n</code></pre> <pre><code>char_iterator = iter(\"Streaming this character by character.\")\nstream.feed(char_iterator)\n</code></pre>"},{"location":"fr/en/usage/#playback","title":"Playback","text":"<p>Asynchronously:</p> <pre><code>stream.play_async()\nwhile stream.is_playing():\n    time.sleep(0.1)\n</code></pre> <p>Synchronously:</p> <pre><code>stream.play()\n</code></pre>"},{"location":"fr/en/usage/#testing-the-library","title":"Testing the Library","text":"<p>The test subdirectory contains a set of scripts to help you evaluate and understand the capabilities of the RealtimeTTS library.</p> <p>Note that most of the tests still rely on the \"old\" OpenAI API (&lt;1.0.0). Usage of the new OpenAI API is demonstrated in openai_1.0_test.py.</p> <ul> <li> <p>simple_test.py</p> <ul> <li>Description: A \"hello world\" styled demonstration of the library's simplest usage.</li> </ul> </li> <li> <p>complex_test.py</p> <ul> <li>Description: A comprehensive demonstration showcasing most of the features provided by the library.</li> </ul> </li> <li> <p>coqui_test.py</p> <ul> <li>Description: Test of local coqui TTS engine.</li> </ul> </li> <li> <p>translator.py</p> <ul> <li>Dependencies: Run <code>pip install openai realtimestt</code>.</li> <li>Description: Real-time translations into six different languages.</li> </ul> </li> <li> <p>openai_voice_interface.py</p> <ul> <li>Dependencies: Run <code>pip install openai realtimestt</code>.</li> <li>Description: Wake word activated and voice based user interface to the OpenAI API.</li> </ul> </li> <li> <p>advanced_talk.py</p> <ul> <li>Dependencies: Run <code>pip install openai keyboard realtimestt</code>.</li> <li>Description: Choose TTS engine and voice before starting AI conversation.</li> </ul> </li> <li> <p>minimalistic_talkbot.py</p> <ul> <li>Dependencies: Run <code>pip install openai realtimestt</code>.</li> <li>Description: A basic talkbot in 20 lines of code.</li> </ul> </li> <li> <p>simple_llm_test.py</p> <ul> <li>Dependencies: Run <code>pip install openai</code>.</li> <li>Description: Simple demonstration of how to integrate the library with large language models (LLMs).</li> </ul> </li> <li> <p>test_callbacks.py</p> <ul> <li>Dependencies: Run <code>pip install openai</code>.</li> <li>Description: Showcases the callbacks and lets you check the latency times in a real-world application environment.</li> </ul> </li> </ul>"},{"location":"fr/en/usage/#pause-resume-stop","title":"Pause, Resume &amp; Stop","text":"<p>Pause the audio stream:</p> <pre><code>stream.pause()\n</code></pre> <p>Resume a paused stream:</p> <pre><code>stream.resume()\n</code></pre> <p>Stop the stream immediately:</p> <pre><code>stream.stop()\n</code></pre>"},{"location":"fr/en/usage/#requirements-explained","title":"Requirements Explained","text":"<ul> <li>Python Version:</li> <li>Required: Python &gt;= 3.9, &lt; 3.13</li> <li> <p>Reason: The library depends on the GitHub library \"TTS\" from coqui, which requires Python versions in this range.</p> </li> <li> <p>PyAudio: to create an output audio stream</p> </li> <li> <p>stream2sentence: to split the incoming text stream into sentences</p> </li> <li> <p>pyttsx3: System text-to-speech conversion engine</p> </li> <li> <p>pydub: to convert audio chunk formats</p> </li> <li> <p>azure-cognitiveservices-speech: Azure text-to-speech conversion engine</p> </li> <li> <p>elevenlabs: Elevenlabs text-to-speech conversion engine</p> </li> <li> <p>coqui-TTS: Coqui's XTTS text-to-speech library for high-quality local neural TTS</p> </li> </ul> <p>Shoutout to Idiap Research Institute for maintaining a fork of coqui tts.</p> <ul> <li> <p>openai: to interact with OpenAI's TTS API</p> </li> <li> <p>gtts: Google translate text-to-speech conversion</p> </li> </ul>"},{"location":"fr/es/","title":"RealtimeTTS","text":"<p>EN | FR | ES</p> <p>*Biblioteca de conversi\u00f3n de texto en voz f\u00e1cil de usar y de baja latencia para aplicaciones en tiempo real.</p>"},{"location":"fr/es/#acerca-del-proyecto","title":"Acerca del proyecto","text":"<p>RealtimeTTS es una biblioteca de texto a voz (TTS) de \u00faltima generaci\u00f3n dise\u00f1ada para aplicaciones en tiempo real. Destaca por su capacidad para convertir r\u00e1pidamente flujos de texto en salida auditiva de alta calidad con una latencia m\u00ednima.</p>"},{"location":"fr/es/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ul> <li>Baja latencia: conversi\u00f3n de texto a voz casi instant\u00e1nea, compatible con salidas LLM.</li> <li>Audio de alta calidad**: genera un habla clara y natural.</li> <li>Compatible con m\u00faltiples motores TTS**: compatible con OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS y System TTS</li> <li>Multiling\u00fce</li> <li>Robusto y fiable**: garantiza un funcionamiento continuo gracias a un mecanismo de reserva que cambia a motores alternativos en caso de interrupciones, lo que garantiza un rendimiento y una fiabilidad constantes.</li> </ul> <p>Para obtener instrucciones de instalaci\u00f3n, ejemplos de uso y referencias de la API, navegue por la documentaci\u00f3n utilizando la barra lateral.</p>"},{"location":"fr/es/api/","title":"TextToAudioStream - Documentaci\u00f3n en Espa\u00f1ol","text":""},{"location":"fr/es/api/#configuracion","title":"Configuraci\u00f3n","text":""},{"location":"fr/es/api/#parametros-de-inicializacion-para-texttoaudiostream","title":"Par\u00e1metros de Inicializaci\u00f3n para <code>TextToAudioStream</code>","text":"<p>Cuando inicializa la clase <code>TextToAudioStream</code>, tiene varias opciones para personalizar su comportamiento. Aqu\u00ed est\u00e1n los par\u00e1metros disponibles:</p>"},{"location":"fr/es/api/#parametros-principales","title":"Par\u00e1metros Principales","text":""},{"location":"fr/es/api/#engine-baseengine","title":"<code>engine</code> (BaseEngine)","text":"<ul> <li>Tipo: BaseEngine</li> <li>Requerido: S\u00ed</li> <li>Descripci\u00f3n: El motor subyacente responsable de la s\u00edntesis de texto a audio. Debe proporcionar una instancia de <code>BaseEngine</code> o su subclase para habilitar la s\u00edntesis de audio.</li> </ul>"},{"location":"fr/es/api/#on_text_stream_start-callable","title":"<code>on_text_stream_start</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se activa cuando comienza el flujo de texto. Util\u00edcela para cualquier configuraci\u00f3n o registro que pueda necesitar.</li> </ul>"},{"location":"fr/es/api/#on_text_stream_stop-callable","title":"<code>on_text_stream_stop</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se activa cuando finaliza el flujo de texto. Puede utilizarla para tareas de limpieza o registro.</li> </ul>"},{"location":"fr/es/api/#on_audio_stream_start-callable","title":"<code>on_audio_stream_start</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se invoca cuando comienza el flujo de audio. \u00datil para actualizaciones de UI o registro de eventos.</li> </ul>"},{"location":"fr/es/api/#on_audio_stream_stop-callable","title":"<code>on_audio_stream_stop</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se llama cuando se detiene el flujo de audio. Ideal para limpieza de recursos o tareas de post-procesamiento.</li> </ul>"},{"location":"fr/es/api/#on_character-callable","title":"<code>on_character</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se llama cuando se procesa un solo car\u00e1cter.</li> </ul>"},{"location":"fr/es/api/#output_device_index-int","title":"<code>output_device_index</code> (int)","text":"<ul> <li>Tipo: Entero</li> <li>Requerido: No</li> <li>Valor predeterminado: None</li> <li>Descripci\u00f3n: Especifica el \u00edndice del dispositivo de salida a utilizar. None usa el dispositivo predeterminado.</li> </ul>"},{"location":"fr/es/api/#tokenizer-string","title":"<code>tokenizer</code> (string)","text":"<ul> <li>Tipo: String</li> <li>Requerido: No</li> <li>Valor predeterminado: nltk</li> <li>Descripci\u00f3n: Tokenizador a utilizar para la divisi\u00f3n de oraciones (actualmente se admiten \"nltk\" y \"stanza\").</li> </ul>"},{"location":"fr/es/api/#language-string","title":"<code>language</code> (string)","text":"<ul> <li>Tipo: String</li> <li>Requerido: No</li> <li>Valor predeterminado: en</li> <li>Descripci\u00f3n: Idioma a utilizar para la divisi\u00f3n de oraciones.</li> </ul>"},{"location":"fr/es/api/#muted-bool","title":"<code>muted</code> (bool)","text":"<ul> <li>Tipo: Bool</li> <li>Requerido: No</li> <li>Valor predeterminado: False</li> <li>Descripci\u00f3n: Par\u00e1metro global de silencio. Si es True, no se abrir\u00e1 ning\u00fan flujo pyAudio. Deshabilita la reproducci\u00f3n de audio a trav\u00e9s de los altavoces locales.</li> </ul>"},{"location":"fr/es/api/#level-int","title":"<code>level</code> (int)","text":"<ul> <li>Tipo: Entero</li> <li>Requerido: No</li> <li>Valor predeterminado: <code>logging.WARNING</code></li> <li>Descripci\u00f3n: Establece el nivel de registro para el registrador interno. Puede ser cualquier constante entera del m\u00f3dulo <code>logging</code> incorporado de Python.</li> </ul>"},{"location":"fr/es/api/#ejemplo-de-uso","title":"Ejemplo de Uso","text":"<pre><code>engine = YourEngine()  # Sustituya con su motor\nstream = TextToAudioStream(\n    engine=engine,\n    on_text_stream_start=my_text_start_func,\n    on_text_stream_stop=my_text_stop_func,\n    on_audio_stream_start=my_audio_start_func,\n    on_audio_stream_stop=my_audio_stop_func,\n    level=logging.INFO\n)\n</code></pre>"},{"location":"fr/es/api/#metodos","title":"M\u00e9todos","text":""},{"location":"fr/es/api/#play-y-play_async","title":"<code>play</code> y <code>play_async</code>","text":"<p>Estos m\u00e9todos son responsables de ejecutar la s\u00edntesis de texto a audio y reproducir el flujo de audio. La diferencia es que <code>play</code> es una funci\u00f3n bloqueante, mientras que <code>play_async</code> se ejecuta en un hilo separado, permitiendo que otras operaciones contin\u00faen.</p>"},{"location":"fr/es/api/#parametros-de-reproduccion","title":"Par\u00e1metros de Reproducci\u00f3n","text":""},{"location":"fr/es/api/#fast_sentence_fragment-bool","title":"<code>fast_sentence_fragment</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>True</code></li> <li>Descripci\u00f3n: Cuando se establece en <code>True</code>, el m\u00e9todo priorizar\u00e1 la velocidad, generando y reproduciendo fragmentos de oraciones m\u00e1s r\u00e1pidamente.</li> </ul>"},{"location":"fr/es/api/#fast_sentence_fragment_allsentences-bool","title":"<code>fast_sentence_fragment_allsentences</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Cuando se establece en <code>True</code>, aplica el procesamiento r\u00e1pido de fragmentos de oraciones a todas las oraciones.</li> </ul>"},{"location":"fr/es/api/#fast_sentence_fragment_allsentences_multiple-bool","title":"<code>fast_sentence_fragment_allsentences_multiple</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Cuando se establece en <code>True</code>, permite generar m\u00faltiples fragmentos de oraciones.</li> </ul>"},{"location":"fr/es/api/#buffer_threshold_seconds-float","title":"<code>buffer_threshold_seconds</code> (float)","text":"<ul> <li>Valor predeterminado: <code>0.0</code></li> <li>Descripci\u00f3n: Especifica el tiempo en segundos para el umbral de b\u00fafer.</li> </ul> <p>C\u00f3mo funciona: Antes de sintetizar una nueva oraci\u00f3n, el sistema verifica si queda m\u00e1s material de audio en el b\u00fafer que el tiempo especificado. Un valor m\u00e1s alto asegura que haya m\u00e1s audio pre-almacenado en el b\u00fafer.</p>"},{"location":"fr/es/api/#minimum_sentence_length-int","title":"<code>minimum_sentence_length</code> (int)","text":"<ul> <li>Valor predeterminado: <code>10</code></li> <li>Descripci\u00f3n: Establece la longitud m\u00ednima de caracteres para considerar una cadena como una oraci\u00f3n.</li> </ul>"},{"location":"fr/es/api/#minimum_first_fragment_length-int","title":"<code>minimum_first_fragment_length</code> (int)","text":"<ul> <li>Valor predeterminado: <code>10</code></li> <li>Descripci\u00f3n: El n\u00famero m\u00ednimo de caracteres requeridos para el primer fragmento de oraci\u00f3n.</li> </ul>"},{"location":"fr/es/api/#log_synthesized_text-bool","title":"<code>log_synthesized_text</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Cuando est\u00e1 habilitado, registra los fragmentos de texto sintetizados.</li> </ul>"},{"location":"fr/es/api/#reset_generated_text-bool","title":"<code>reset_generated_text</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>True</code></li> <li>Descripci\u00f3n: Si es True, reinicia el texto generado antes del procesamiento.</li> </ul>"},{"location":"fr/es/api/#output_wavfile-str","title":"<code>output_wavfile</code> (str)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Si se establece, guarda el audio en el archivo WAV especificado.</li> </ul>"},{"location":"fr/es/api/#funciones-de-callback","title":"Funciones de Callback","text":""},{"location":"fr/es/api/#on_sentence_synthesized-callable","title":"<code>on_sentence_synthesized</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Se llama despu\u00e9s de sintetizar un fragmento de oraci\u00f3n.</li> </ul>"},{"location":"fr/es/api/#before_sentence_synthesized-callable","title":"<code>before_sentence_synthesized</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Se llama antes de sintetizar un fragmento de oraci\u00f3n.</li> </ul>"},{"location":"fr/es/api/#on_audio_chunk-callable","title":"<code>on_audio_chunk</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Se llama cuando un fragmento de audio est\u00e1 listo.</li> </ul>"},{"location":"fr/es/api/#configuracion-de-tokenizacion","title":"Configuraci\u00f3n de Tokenizaci\u00f3n","text":""},{"location":"fr/es/api/#tokenizer-str","title":"<code>tokenizer</code> (str)","text":"<ul> <li>Valor predeterminado: <code>\"nltk\"</code></li> <li>Descripci\u00f3n: Tokenizador para la divisi\u00f3n de oraciones. Admite \"nltk\" y \"stanza\".</li> </ul>"},{"location":"fr/es/api/#tokenize_sentences-callable","title":"<code>tokenize_sentences</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Funci\u00f3n personalizada para tokenizar oraciones del texto de entrada.</li> </ul>"},{"location":"fr/es/api/#language-str","title":"<code>language</code> (str)","text":"<ul> <li>Valor predeterminado: <code>\"en\"</code></li> <li>Descripci\u00f3n: Idioma para la divisi\u00f3n de oraciones.</li> </ul>"},{"location":"fr/es/api/#parametros-de-contexto","title":"Par\u00e1metros de Contexto","text":""},{"location":"fr/es/api/#context_size-int","title":"<code>context_size</code> (int)","text":"<ul> <li>Valor predeterminado: <code>12</code></li> <li>Descripci\u00f3n: Caracteres utilizados para establecer el contexto de l\u00edmites de oraciones.</li> </ul>"},{"location":"fr/es/api/#context_size_look_overhead-int","title":"<code>context_size_look_overhead</code> (int)","text":"<ul> <li>Valor predeterminado: <code>12</code></li> <li>Descripci\u00f3n: Tama\u00f1o de contexto adicional para mirar hacia adelante.</li> </ul>"},{"location":"fr/es/api/#otros-parametros","title":"Otros Par\u00e1metros","text":""},{"location":"fr/es/api/#muted-bool_1","title":"<code>muted</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Deshabilita la reproducci\u00f3n de audio local si es True.</li> </ul>"},{"location":"fr/es/api/#sentence_fragment_delimiters-str","title":"<code>sentence_fragment_delimiters</code> (str)","text":"<ul> <li>Valor predeterminado: <code>\".?!;:,\\n\u2026)]}\u3002-\"</code></li> <li>Descripci\u00f3n: Caracteres considerados como delimitadores de oraciones.</li> </ul>"},{"location":"fr/es/api/#force_first_fragment_after_words-int","title":"<code>force_first_fragment_after_words</code> (int)","text":"<ul> <li>Valor predeterminado: <code>15</code></li> <li>Descripci\u00f3n: N\u00famero de palabras despu\u00e9s de las cuales se fuerza el primer fragmento.</li> </ul>"},{"location":"fr/es/contributing/","title":"Contribuir a RealtimeTTS","text":"<p>Agradecemos cualquier contribuci\u00f3n a RealtimeTTS. Aqu\u00ed tienes algunas formas de contribuir:</p> <ol> <li> <p>Informar de errores: Si encuentras un error, por favor abre una incidencia en nuestro repositorio GitHub.</p> </li> <li> <p>Sugerir mejoras: \u00bfTienes ideas para nuevas funciones o mejoras? Nos encantar\u00eda escucharlas. Abre una incidencia para sugerir mejoras.</p> </li> <li> <p>Contribuciones de c\u00f3digo: \u00bfQuieres a\u00f1adir una nueva funci\u00f3n o corregir un error? \u00a1Perfecto! Sigue estos pasos:</p> </li> <li>Abre el repositorio</li> <li>Crea una nueva rama para tu funci\u00f3n</li> <li>Realice los cambios</li> <li> <p>Env\u00eda un pull request con una descripci\u00f3n clara de tus cambios</p> </li> <li> <p>Documentaci\u00f3n: Ay\u00fadanos a mejorar nuestra documentaci\u00f3n corrigiendo erratas, a\u00f1adiendo ejemplos o aclarando secciones confusas.</p> </li> <li> <p>A\u00f1adir nuevos motores: Si quieres a\u00f1adir soporte para un nuevo motor TTS, por favor abre una incidencia primero para discutir la implementaci\u00f3n.</p> </li> </ol> <p>Gracias por ayudarnos a mejorar RealtimeTTS.</p>"},{"location":"fr/es/faq/","title":"Preguntas frecuentes","text":"<p>Para obtener respuestas a las preguntas m\u00e1s frecuentes sobre RealtimeTTS, consulta nuestra p\u00e1gina de preguntas frecuentes en GitHub.</p> <p>Esta p\u00e1gina cubre varios temas, entre ellos</p> <ul> <li>Uso de diferentes motores TTS</li> <li>Tratamiento de textos multiling\u00fces</li> <li>Optimizaci\u00f3n del rendimiento</li> <li>Soluci\u00f3n de problemas comunes</li> </ul> <p>Para obtener informaci\u00f3n m\u00e1s detallada, visite el enlace anterior.</p>"},{"location":"fr/es/installation/","title":"Espa\u00f1ol","text":"<p>Nota: Ya no se recomienda la instalaci\u00f3n b\u00e1sica con <code>pip install realtimetts</code>, use <code>pip install realtimetts[all]</code> en su lugar.</p> <p>La biblioteca RealtimeTTS proporciona opciones de instalaci\u00f3n para varias dependencias seg\u00fan su caso de uso. Aqu\u00ed est\u00e1n las diferentes formas en que puede instalar RealtimeTTS seg\u00fan sus necesidades:</p>"},{"location":"fr/es/installation/#instalacion-completa","title":"Instalaci\u00f3n Completa","text":"<p>Para instalar RealtimeTTS con soporte para todos los motores de TTS:</p> <pre><code>pip install -U realtimetts[all]\n</code></pre>"},{"location":"fr/es/installation/#instalacion-personalizada","title":"Instalaci\u00f3n Personalizada","text":"<p>RealtimeTTS permite una instalaci\u00f3n personalizada con instalaciones m\u00ednimas de bibliotecas. Estas son las opciones disponibles: - all: Instalaci\u00f3n completa con todos los motores soportados. - system: Incluye capacidades de TTS espec\u00edficas del sistema (por ejemplo, pyttsx3). - azure: Agrega soporte para Azure Cognitive Services Speech. - elevenlabs: Incluye integraci\u00f3n con la API de ElevenLabs. - openai: Para servicios de voz de OpenAI. - gtts: Soporte para Google Text-to-Speech. - coqui: Instala el motor Coqui TTS. - minimal: Instala solo los requisitos base sin motor (solo necesario si desea desarrollar un motor propio)</p> <p>Por ejemplo, si desea instalar RealtimeTTS solo para uso local de Coqui TTS neuronal, debe usar:</p> <pre><code>pip install realtimetts[coqui]\n</code></pre> <p>Si desea instalar RealtimeTTS solo con Azure Cognitive Services Speech, ElevenLabs y soporte de OpenAI:</p> <pre><code>pip install realtimetts[azure,elevenlabs,openai]\n</code></pre>"},{"location":"fr/es/installation/#instalacion-en-entorno-virtual","title":"Instalaci\u00f3n en Entorno Virtual","text":"<p>Para aquellos que deseen realizar una instalaci\u00f3n completa dentro de un entorno virtual, sigan estos pasos:</p> <pre><code>python -m venv env_realtimetts\nenv_realtimetts\\Scripts\\activate.bat\npython.exe -m pip install --upgrade pip\npip install -U realtimetts[all]\n</code></pre> <p>M\u00e1s informaci\u00f3n sobre instalaci\u00f3n de CUDA.</p>"},{"location":"fr/es/installation/#requisitos-de-los-motores","title":"Requisitos de los Motores","text":"<p>Los diferentes motores soportados por RealtimeTTS tienen requisitos \u00fanicos. Aseg\u00farese de cumplir con estos requisitos seg\u00fan el motor que elija.</p>"},{"location":"fr/es/installation/#systemengine","title":"SystemEngine","text":"<p>El <code>SystemEngine</code> funciona de inmediato con las capacidades de TTS incorporadas en su sistema. No se necesita configuraci\u00f3n adicional.</p>"},{"location":"fr/es/installation/#gttsengine","title":"GTTSEngine","text":"<p>El <code>GTTSEngine</code> funciona de inmediato usando la API de texto a voz de Google Translate. No se necesita configuraci\u00f3n adicional.</p>"},{"location":"fr/es/installation/#openaiengine","title":"OpenAIEngine","text":"<p>Para usar el <code>OpenAIEngine</code>: - configure la variable de entorno OPENAI_API_KEY - instale ffmpeg (ver instalaci\u00f3n de CUDA punto 3)</p>"},{"location":"fr/es/installation/#azureengine","title":"AzureEngine","text":"<p>Para usar el <code>AzureEngine</code>, necesitar\u00e1: - Clave API de Microsoft Azure Text-to-Speech (proporcionada a trav\u00e9s del par\u00e1metro \"speech_key\" del constructor AzureEngine o en la variable de entorno AZURE_SPEECH_KEY) - Regi\u00f3n de servicio de Microsoft Azure.</p> <p>Aseg\u00farese de tener estas credenciales disponibles y correctamente configuradas al inicializar el <code>AzureEngine</code>.</p>"},{"location":"fr/es/installation/#elevenlabsengine","title":"ElevenlabsEngine","text":"<p>Para el <code>ElevenlabsEngine</code>, necesita: - Clave API de Elevenlabs (proporcionada a trav\u00e9s del par\u00e1metro \"api_key\" del constructor ElevenlabsEngine o en la variable de entorno ELEVENLABS_API_KEY) - <code>mpv</code> instalado en su sistema (esencial para transmitir audio mpeg, Elevenlabs solo entrega mpeg).</p> <p>\ud83d\udd39 Instalaci\u00f3n de <code>mpv</code>:   - macOS:     <code>brew install mpv</code></p> <ul> <li>Linux y Windows: Visite mpv.io para instrucciones de instalaci\u00f3n.</li> </ul>"},{"location":"fr/es/installation/#coquiengine","title":"CoquiEngine","text":"<p>Proporciona TTS neuronal local de alta calidad con clonaci\u00f3n de voz.</p> <p>Descarga primero un modelo neuronal TTS. En la mayor\u00eda de los casos, ser\u00e1 lo suficientemente r\u00e1pido para tiempo real usando s\u00edntesis GPU. Necesita alrededor de 4-5 GB de VRAM.</p> <ul> <li>para clonar una voz, env\u00ede el nombre del archivo de un archivo wave que contenga la voz fuente como par\u00e1metro \"voice\" al constructor CoquiEngine</li> <li>la clonaci\u00f3n de voz funciona mejor con un archivo WAV mono de 16 bits a 22050 Hz que contenga una muestra corta (~5-30 seg)</li> </ul> <p>En la mayor\u00eda de los sistemas, se necesitar\u00e1 soporte de GPU para ejecutarse lo suficientemente r\u00e1pido en tiempo real, de lo contrario experimentar\u00e1 tartamudeo.</p>"},{"location":"fr/es/installation/#instalacion-de-cuda","title":"Instalaci\u00f3n de CUDA","text":"<p>Estos pasos son recomendados para aquellos que requieren mejor rendimiento y tienen una GPU NVIDIA compatible.</p> <p>Nota: para verificar si su GPU NVIDIA es compatible con CUDA, visite la lista oficial de GPUs CUDA.</p> <p>Para usar torch con soporte v\u00eda CUDA, siga estos pasos:</p> <p>Nota: las instalaciones m\u00e1s nuevas de pytorch pueden (no verificado) no necesitar la instalaci\u00f3n de Toolkit (y posiblemente cuDNN).</p> <ol> <li> <p>Instalar NVIDIA CUDA Toolkit:     Por ejemplo, para instalar Toolkit 12.X, por favor</p> <ul> <li>Visite NVIDIA CUDA Downloads.</li> <li>Seleccione su sistema operativo, arquitectura del sistema y versi\u00f3n del sistema operativo.</li> <li>Descargue e instale el software.</li> </ul> <p>o para instalar Toolkit 11.8, por favor - Visite NVIDIA CUDA Toolkit Archive. - Seleccione su sistema operativo, arquitectura del sistema y versi\u00f3n del sistema operativo. - Descargue e instale el software.</p> </li> <li> <p>Instalar NVIDIA cuDNN:</p> <p>Por ejemplo, para instalar cuDNN 8.7.0 para CUDA 11.x por favor - Visite NVIDIA cuDNN Archive. - Haga clic en \"Download cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x\". - Descargue e instale el software.</p> </li> <li> <p>Instalar ffmpeg:</p> <p>Puede descargar un instalador para su sistema operativo desde el sitio web de ffmpeg.</p> <p>O usar un gestor de paquetes:</p> <ul> <li> <p>En Ubuntu o Debian:     <code>sudo apt update &amp;&amp; sudo apt install ffmpeg</code></p> </li> <li> <p>En Arch Linux:     <code>sudo pacman -S ffmpeg</code></p> </li> <li> <p>En MacOS usando Homebrew (https://brew.sh/):     <code>brew install ffmpeg</code></p> </li> <li> <p>En Windows usando Chocolatey (https://chocolatey.org/):     <code>choco install ffmpeg</code></p> </li> <li> <p>En Windows usando Scoop (https://scoop.sh/):     <code>scoop install ffmpeg</code></p> </li> </ul> </li> <li> <p>Instalar PyTorch con soporte CUDA:</p> <p>Para actualizar su instalaci\u00f3n de PyTorch y habilitar el soporte de GPU con CUDA, siga estas instrucciones seg\u00fan su versi\u00f3n espec\u00edfica de CUDA. Esto es \u00fatil si desea mejorar el rendimiento de RealtimeSTT con capacidades CUDA.</p> <ul> <li> <p>Para CUDA 11.8:</p> <p>Para actualizar PyTorch y Torchaudio para soportar CUDA 11.8, use los siguientes comandos:</p> <p><code>pip install torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p> </li> <li> <p>Para CUDA 12.X:</p> <p>Para actualizar PyTorch y Torchaudio para soportar CUDA 12.X, ejecute lo siguiente:</p> <p><code>pip install torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code></p> </li> </ul> <p>Reemplace <code>2.3.1</code> con la versi\u00f3n de PyTorch que coincida con su sistema y requisitos.</p> </li> <li> <p>Soluci\u00f3n para resolver problemas de compatibilidad:     Si encuentra problemas de compatibilidad de bibliotecas, intente establecer estas bibliotecas en versiones fijas:</p> <p><code>pip install networkx==2.8.8 pip install typing_extensions==4.8.0 pip install fsspec==2023.6.0 pip install imageio==2.31.6 pip install networkx==2.8.8 pip install numpy==1.24.3 pip install requests==2.31.0</code></p> </li> </ol>"},{"location":"fr/es/usage/","title":"Uso","text":""},{"location":"fr/es/usage/#inicio-rapido","title":"Inicio R\u00e1pido","text":"<p>Aqu\u00ed hay un ejemplo b\u00e1sico de uso:</p> <pre><code>from RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine\n\nengine = SystemEngine() # replace with your TTS engine\nstream = TextToAudioStream(engine)\nstream.feed(\"Hello world! How are you today?\")\nstream.play_async()\n</code></pre>"},{"location":"fr/es/usage/#alimentar-texto","title":"Alimentar Texto","text":"<p>Puede alimentar cadenas individuales:</p> <pre><code>stream.feed(\"Hello, this is a sentence.\")\n</code></pre> <p>O puede alimentar generadores e iteradores de caracteres para la transmisi\u00f3n en tiempo real:</p> <pre><code>def write(prompt: str):\n    for chunk in openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\" : prompt}],\n        stream=True\n    ):\n        if (text_chunk := chunk[\"choices\"][0][\"delta\"].get(\"content\")) is not None:\n            yield text_chunk\n\ntext_stream = write(\"A three-sentence relaxing speech.\")\n\nstream.feed(text_stream)\n</code></pre> <pre><code>char_iterator = iter(\"Streaming this character by character.\")\nstream.feed(char_iterator)\n</code></pre>"},{"location":"fr/es/usage/#reproduccion","title":"Reproducci\u00f3n","text":"<p>De forma as\u00edncrona:</p> <pre><code>stream.play_async()\nwhile stream.is_playing():\n    time.sleep(0.1)\n</code></pre> <p>De forma s\u00edncrona:</p> <pre><code>stream.play()\n</code></pre>"},{"location":"fr/es/usage/#prueba-de-la-biblioteca","title":"Prueba de la Biblioteca","text":"<p>El subdirectorio de pruebas contiene un conjunto de scripts para ayudarte a evaluar y comprender las capacidades de la biblioteca RealtimeTTS.</p> <p>Ten en cuenta que la mayor\u00eda de las pruebas a\u00fan dependen de la API \"antigua\" de OpenAI (&lt;1.0.0). El uso de la nueva API de OpenAI se demuestra en openai_1.0_test.py.</p> <ul> <li> <p>simple_test.py</p> <ul> <li>Descripci\u00f3n: Una demostraci\u00f3n tipo \"hola mundo\" del uso m\u00e1s simple de la biblioteca.</li> </ul> </li> <li> <p>complex_test.py</p> <ul> <li>Descripci\u00f3n: Una demostraci\u00f3n completa que muestra la mayor\u00eda de las caracter\u00edsticas proporcionadas por la biblioteca.</li> </ul> </li> <li> <p>coqui_test.py</p> <ul> <li>Descripci\u00f3n: Prueba del motor local coqui TTS.</li> </ul> </li> <li> <p>translator.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai realtimestt</code>.</li> <li>Descripci\u00f3n: Traducciones en tiempo real a seis idiomas diferentes.</li> </ul> </li> <li> <p>openai_voice_interface.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai realtimestt</code>.</li> <li>Descripci\u00f3n: Interfaz de usuario activada por palabra clave y basada en voz para la API de OpenAI.</li> </ul> </li> <li> <p>advanced_talk.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai keyboard realtimestt</code>.</li> <li>Descripci\u00f3n: Elija el motor TTS y la voz antes de iniciar la conversaci\u00f3n con IA.</li> </ul> </li> <li> <p>minimalistic_talkbot.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai realtimestt</code>.</li> <li>Descripci\u00f3n: Un talkbot b\u00e1sico en 20 l\u00edneas de c\u00f3digo.</li> </ul> </li> <li> <p>simple_llm_test.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai</code>.</li> <li>Descripci\u00f3n: Demostraci\u00f3n simple de c\u00f3mo integrar la biblioteca con modelos de lenguaje grande (LLMs).</li> </ul> </li> <li> <p>test_callbacks.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai</code>.</li> <li>Descripci\u00f3n: Muestra los callbacks y te permite verificar los tiempos de latencia en un entorno de aplicaci\u00f3n del mundo real.</li> </ul> </li> </ul>"},{"location":"fr/es/usage/#pausar-reanudar-y-detener","title":"Pausar, Reanudar y Detener","text":"<p>Pausar el flujo de audio:</p> <pre><code>stream.pause()\n</code></pre> <p>Reanudar un flujo pausado:</p> <pre><code>stream.resume()\n</code></pre> <p>Detener el flujo inmediatamente:</p> <pre><code>stream.stop()\n</code></pre>"},{"location":"fr/es/usage/#requisitos-explicados","title":"Requisitos Explicados","text":"<ul> <li>Versi\u00f3n de Python:</li> <li>Requerido: Python &gt;= 3.9, &lt; 3.13</li> <li> <p>Raz\u00f3n: La biblioteca depende de la biblioteca GitHub \"TTS\" de coqui, que requiere versiones de Python en este rango.</p> </li> <li> <p>PyAudio: para crear un flujo de audio de salida</p> </li> <li> <p>stream2sentence: para dividir el flujo de texto entrante en oraciones</p> </li> <li> <p>pyttsx3: Motor de conversi\u00f3n de texto a voz del sistema</p> </li> <li> <p>pydub: para convertir formatos de fragmentos de audio</p> </li> <li> <p>azure-cognitiveservices-speech: Motor de conversi\u00f3n de texto a voz de Azure</p> </li> <li> <p>elevenlabs: Motor de conversi\u00f3n de texto a voz de Elevenlabs</p> </li> <li> <p>coqui-TTS: Biblioteca de texto a voz XTTS de Coqui para TTS neuronal local de alta calidad</p> </li> </ul> <p>Agradecimiento especial al Instituto de Investigaci\u00f3n Idiap por mantener un fork de coqui tts.</p> <ul> <li> <p>openai: para interactuar con la API TTS de OpenAI</p> </li> <li> <p>gtts: Conversi\u00f3n de texto a voz de Google translate</p> </li> </ul>"},{"location":"fr/fr/","title":"RealtimeTTS","text":"<p>EN | FR | ES</p> <p>Biblioth\u00e8que de synth\u00e8se vocale \u00e0 faible latence et facile \u00e0 utiliser pour les applications en temps r\u00e9el</p>"},{"location":"fr/fr/#a-propos-du-projet","title":"\u00c0 propos du projet","text":"<p>RealtimeTTS est une biblioth\u00e8que de synth\u00e8se vocale (TTS) de pointe con\u00e7ue pour les applications en temps r\u00e9el. Elle se distingue par sa capacit\u00e9 \u00e0 convertir des flux de texte en sortie auditive de haute qualit\u00e9 avec une latence minimale.</p>"},{"location":"fr/fr/#caracteristiques-cles","title":"Caract\u00e9ristiques cl\u00e9s","text":"<ul> <li>Faible latence : conversion text-to-speech quasi-instantan\u00e9e, compatible avec les sorties LLM</li> <li>Audio de haute qualit\u00e9 : g\u00e9n\u00e8re un discours clair et naturel</li> <li>Support de plusieurs moteurs TTS : prend en charge OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS et System TTS</li> <li>Multilingue</li> <li>Robuste et fiable : garantit une op\u00e9ration continue gr\u00e2ce \u00e0 un m\u00e9canisme de fallback, bascule vers des moteurs alternatifs en cas de perturbations, garantissant une performance et une fiabilit\u00e9 coh\u00e9rentes</li> </ul> <p>Pour les instructions d'installation, les exemples d'utilisation et la r\u00e9f\u00e9rence de l'API, veuillez naviguer \u00e0 travers la documentation \u00e0 l'aide du sidebar.</p>"},{"location":"fr/fr/api/","title":"Fran\u00e7ais","text":""},{"location":"fr/fr/api/#configuration","title":"Configuration","text":""},{"location":"fr/fr/api/#parametres-dinitialisation-pour-texttoaudiostream","title":"Param\u00e8tres d'initialisation pour `TextToAudioStream","text":"<p>Lorsque vous initialisez la classe <code>TextToAudioStream</code>, vous disposez de diverses options pour personnaliser son comportement. Voici les param\u00e8tres disponibles :</p>"},{"location":"fr/fr/api/#baseengine","title":"`(BaseEngine)","text":"<ul> <li>Type: BaseEngine</li> <li>Obligatoire: Oui</li> <li>Description : Le moteur sous-jacent responsable de la synth\u00e8se texte-audio. Vous devez fournir une instance de <code>ine</code> ou sa sous-classe pour permettre la synth\u00e8se audio.</li> </ul>"},{"location":"fr/fr/api/#_text_stream_start-appelable","title":"<code>_text_stream_start</code> (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est d\u00e9clench\u00e9e lorsque le flux de texte commence. Utilisez-le pour toute configuration ou journalisation dont vous pourriez avoir besoin.</li> </ul>"},{"location":"fr/fr/api/#_text_stream_stop-appelable","title":"<code>_text_stream_stop</code> (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est activ\u00e9e \u00e0 la fin du flux de texte. Vous pouvez l'utiliser pour des t\u00e2ches de nettoyage ou de journalisation.</li> </ul>"},{"location":"fr/fr/api/#_audio_stream_start-appelable","title":"_audio_stream_start` (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel facultative est invoqu\u00e9e au d\u00e9marrage du flux audio. Utile pour les mises \u00e0 jour de l'interface utilisateur ou la journalisation des \u00e9v\u00e9nements.</li> </ul>"},{"location":"fr/fr/api/#_audio_stream_stop-appelable","title":"<code>_audio_stream_stop</code> (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est appel\u00e9e lorsque le flux audio s'arr\u00eate. Id\u00e9al pour les t\u00e2ches de nettoyage des ressources ou de post-traitement.</li> </ul>"},{"location":"fr/fr/api/#on_character-appelable","title":"on_character` (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est appel\u00e9e lorsqu'un seul caract\u00e8re est trait\u00e9.</li> </ul>"},{"location":"fr/fr/api/#_device_index-int","title":"<code>_device_index</code> (int)","text":"<ul> <li>Type: Entier</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: Aucun</li> <li>Description : Sp\u00e9cifie l'index du p\u00e9riph\u00e9rique de sortie \u00e0 utiliser. Aucun n'utilise le p\u00e9riph\u00e9rique par d\u00e9faut.</li> </ul>"},{"location":"fr/fr/api/#tokenizerchaine","title":"<code>(tokenizer</code>(cha\u00eene)","text":"<ul> <li>Type: Cha\u00eene</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: nltk</li> <li>Description : Tokenizer \u00e0 utiliser pour le fractionnement des phrases (actuellement \u00ab nltk \u00bb et \u00ab stroza \u00bb sont pris en charge).</li> </ul>"},{"location":"fr/fr/api/#languagechaine","title":"`language(cha\u00eene)","text":"<ul> <li>Type: Cha\u00eene</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: fr</li> <li>Description : Langue \u00e0 utiliser pour le fractionnement des phrases.</li> </ul>"},{"location":"fr/fr/api/#mutedbool","title":"<code>muted</code>(bool)","text":"<ul> <li>Type: Bool</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: Faux</li> <li>Description : Param\u00e8tre global coup\u00e9. Si True, aucun flux pyAudio ne sera ouvert. D\u00e9sactive la lecture audio via des haut-parleurs locaux (au cas o\u00f9 vous souhaitez synth\u00e9tiser dans un fichier ou traiter des morceaux audio) et remplace le param\u00e8tre de mise en sourdine des param\u00e8tres de lecture.</li> </ul>"},{"location":"fr/fr/api/#level-int","title":"<code>level</code> (int)","text":"<ul> <li>Type: Entier</li> <li>Obligatoire: Non</li> <li>D\u00e9faut:<code>logging.AVERTISSEMENT</code></li> <li>Description : D\u00e9finit le niveau de journalisation pour l'enregistreur interne. Cela peut \u00eatre n'importe quelle constante enti\u00e8re du module <code>ging</code> int\u00e9gr\u00e9 de Python.</li> </ul>"},{"location":"fr/fr/api/#exemple-dutilisation","title":"Exemple d'utilisation :","text":"<p><code>``(`python moteur = YourEngine () # Remplacez-vous par votre moteur flux = TextToAudioStream(     moteur=engine,     on_text_stream_start=my_text_start_func,     on_text_stream_stop=my_text_stop_func,     on_audio_stream_start=my_audio_start_func,     on_audio_stream_stop=my_audio_stop_func,     niveau=logging.INFO )</code></p>"},{"location":"fr/fr/api/#methodes","title":"M\u00e9thodes","text":""},{"location":"fr/fr/api/#play-etplay_async","title":"<code>play et</code>play_async`","text":"<p>Ces m\u00e9thodes sont responsables de l'ex\u00e9cution de la synth\u00e8se texte-audio et de la lecture du flux audio. La diff\u00e9rence est que <code>play</code> est une fonction de blocage, tandis que <code>play_async</code> s'ex\u00e9cute dans un thread s\u00e9par\u00e9, permettant \u00e0 d'autres op\u00e9rations de se poursuivre.</p>"},{"location":"fr/fr/api/#parametres","title":"Param\u00e8tres :","text":""},{"location":"fr/fr/api/#fast_sentence_fragment-bool","title":"fast<code>_sentence_fragment</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>True</code></li> <li>Description : Lorsqu'elle est d\u00e9finie sur <code>True</code>, la m\u00e9thode donnera la priorit\u00e9 \u00e0 la vitesse, g\u00e9n\u00e9rant et jouant plus rapidement des fragments de phrases. Ceci est utile pour les applications o\u00f9 la latence est importante.</li> </ul>"},{"location":"fr/fr/api/#fast_sentence_fragment_allsentencesbool","title":"fast<code>_sentence_fragment_allsentences</code>(bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Lorsqu'il est d\u00e9fini sur <code>True</code>, applique le traitement rapide des fragments de phrase \u00e0 toutes les phrases, pas seulement \u00e0 la premi\u00e8re.</li> </ul>"},{"location":"fr/fr/api/#fast_sentence_fragment_allsentences_multiple-bool","title":"fast<code>_sentence_fragment_allsentences_multiple</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Lorsqu'il est d\u00e9fini sur <code>True</code>, permet de produire plusieurs fragments de phrase au lieu d'un seul.</li> </ul>"},{"location":"fr/fr/api/#_threshold_seconds-flotteur","title":"<code>_threshold_seconds</code> (flotteur)","text":"<ul> <li>Par d\u00e9faut: <code>0.0</code></li> <li> <p>Description : Sp\u00e9cifie le temps en secondes pour le seuil de mise en m\u00e9moire tampon, ce qui a un impact sur la douceur et la continuit\u00e9 de la lecture audio.</p> </li> <li> <p>Comment \u00e7a marche : Avant de synth\u00e9tiser une nouvelle phrase, le syst\u00e8me v\u00e9rifie s'il reste plus de mat\u00e9riel audio dans le tampon que le temps sp\u00e9cifi\u00e9 par <code>buffer_threshold_seconds</code>. Si tel est le cas, il r\u00e9cup\u00e8re une autre phrase du g\u00e9n\u00e9rateur de texte, en supposant qu'il peut r\u00e9cup\u00e9rer et synth\u00e9tiser cette nouvelle phrase dans la fen\u00eatre temporelle fournie par l'audio restant dans le tampon. Ce processus permet au moteur de synth\u00e8se vocale d'avoir plus de contexte pour une meilleure synth\u00e8se, am\u00e9liorant ainsi l'exp\u00e9rience utilisateur.</p> </li> </ul> <p>Une valeur plus \u00e9lev\u00e9e garantit qu'il y a plus d'audio pr\u00e9-tamponn\u00e9, r\u00e9duisant ainsi le risque de silence ou de lacunes pendant la lecture. Si vous rencontrez des pauses ou des pauses, envisagez d'augmenter cette valeur.</p>"},{"location":"fr/fr/api/#_sentence_length-int","title":"<code>_sentence_length</code> (int)","text":"<ul> <li>Par d\u00e9faut: <code>10</code></li> <li>Description : D\u00e9finit la longueur minimale des caract\u00e8res pour consid\u00e9rer une cha\u00eene comme une phrase \u00e0 synth\u00e9tiser. Cela affecte la fa\u00e7on dont les morceaux de texte sont trait\u00e9s et lus.</li> </ul>"},{"location":"fr/fr/api/#_first_fragment_lengthint","title":"<code>_first_fragment_length</code>(int)","text":"<ul> <li>Par d\u00e9faut: <code>10</code></li> <li>Description : Le nombre minimum de caract\u00e8res requis pour le premier fragment de phrase avant de c\u00e9der.</li> </ul>"},{"location":"fr/fr/api/#_synthesized_text-bool","title":"<code>_synthesized_text</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Lorsqu'il est activ\u00e9, enregistre les morceaux de texte au fur et \u00e0 mesure de leur synth\u00e8se en audio. Utile pour l'audit et le d\u00e9bogage.</li> </ul>"},{"location":"fr/fr/api/#reset_generated_text-bool","title":"#reset_generated_text` (bool)","text":"<ul> <li>Par d\u00e9faut: <code>True</code></li> <li>Description : Si Vrai, r\u00e9initialisez le texte g\u00e9n\u00e9r\u00e9 avant le traitement.</li> </ul>"},{"location":"fr/fr/api/#_wavfile-str","title":"<code>_wavfile</code> (str)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Si d\u00e9fini, enregistrez l'audio dans le fichier WAV sp\u00e9cifi\u00e9.</li> </ul>"},{"location":"fr/fr/api/#_sentence_synthesized-appelable","title":"`_sentence_synthesized (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Une fonction de rappel appel\u00e9e apr\u00e8s un seul fragment de phrase a \u00e9t\u00e9 synth\u00e9tis\u00e9e.</li> </ul>"},{"location":"fr/fr/api/#before_sentence_synthesized-appelable","title":"before`_sentence_synthesized (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Une fonction de rappel qui est appel\u00e9e avant qu'un seul fragment de phrase ne soit synth\u00e9tis\u00e9.</li> </ul>"},{"location":"fr/fr/api/#_audio_chunk-appelable","title":"<code>_audio_chunk</code> (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Fonction de rappel qui est appel\u00e9e lorsqu'un seul morceau audio est pr\u00eat.</li> </ul>"},{"location":"fr/fr/api/#str","title":"```(str)","text":"<ul> <li>Par d\u00e9faut:<code>\"nltk\"</code></li> <li>Description : Tokenizer \u00e0 utiliser pour le fractionnement des phrases. Prend actuellement en charge \u00ab nltk \u00bb et \u00ab stroza \u00bb.</li> </ul>"},{"location":"fr/fr/api/#_sentences-appelable","title":"<code>_sentences</code> (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Une fonction personnalis\u00e9e qui tokenise les phrases du texte saisi. Vous pouvez fournir votre propre tokenizer l\u00e9ger si vous n'\u00eates pas satisfait de nltk et stanza. Il doit prendre du texte comme cha\u00eene et renvoyer des phrases divis\u00e9es comme liste de cha\u00eenes.</li> </ul>"},{"location":"fr/fr/api/#angustr","title":"<code>angu</code>(str)","text":"<ul> <li>Par d\u00e9faut:<code>\"en\"</code></li> <li>Description : Langue \u00e0 utiliser pour le fractionnement des phrases.</li> </ul>"},{"location":"fr/fr/api/#_sizeint","title":"<code>_size</code>(int)","text":"<ul> <li>Par d\u00e9faut: <code>12</code></li> <li>Description : Le nombre de caract\u00e8res utilis\u00e9s pour \u00e9tablir le contexte pour la d\u00e9tection des limites de phrase. Un contexte plus large am\u00e9liore la pr\u00e9cision de la d\u00e9tection des limites des phrases.</li> </ul>"},{"location":"fr/fr/api/#_size_look_overhead-int","title":"<code>_size_look_overhead</code> (int)","text":"<ul> <li>Par d\u00e9faut: <code>12</code></li> <li>Description : Taille de contexte suppl\u00e9mentaire pour regarder vers l'avenir lors de la d\u00e9tection des limites des phrases.</li> </ul>"},{"location":"fr/fr/api/#mute-bool","title":"<code>mute</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Si vrai, d\u00e9sactive la lecture audio via des haut-parleurs locaux. Utile lorsque vous souhaitez synth\u00e9tiser dans un fichier ou traiter des morceaux audio sans les lire.</li> </ul>"},{"location":"fr/fr/api/#ence_fragment_delimiters-str","title":"<code>ence_fragment_delimiters</code> (str)","text":"<ul> <li>Par d\u00e9faut:<code>\"?!;::\\n...)]}-</code></li> <li>Description : Une cha\u00eene de caract\u00e8res qui sont consid\u00e9r\u00e9s comme des d\u00e9limiteurs de phrases.</li> </ul>"},{"location":"fr/fr/api/#_first_fragment_after_words-int","title":"<code>_first_fragment_after_</code>words (int)","text":"<ul> <li>Par d\u00e9faut: <code>15</code></li> <li>Description : Le nombre de mots apr\u00e8s lesquels le fragment de la premi\u00e8re phrase est forc\u00e9 d'\u00eatre donn\u00e9.</li> </ul>"},{"location":"fr/fr/contributing/","title":"Contribuer \u00e0 RealtimeTTS","text":"<p>Nous accueillons les contributions \u00e0 RealtimeTTS ! Voici quelques fa\u00e7ons dont vous pouvez contribuer :</p> <ol> <li> <p>Reporting Bugs : Si vous trouvez un bug, veuillez ouvrir un probl\u00e8me sur notre r\u00e9f\u00e9rentiel GitHub.</p> </li> <li> <p>** Suggestion d'am\u00e9liorations** : Vous avez des id\u00e9es de nouvelles fonctionnalit\u00e9s ou d'am\u00e9liorations ? Nous serions ravis de les entendre ! Ouvrez un num\u00e9ro pour sugg\u00e9rer des am\u00e9liorations.</p> </li> <li> <p>Code Contributions : Vous voulez ajouter une nouvelle fonctionnalit\u00e9 ou corriger un bug ? Super ! Veuillez suivre ces \u00e9tapes :</p> </li> <li>Fourcher le d\u00e9p\u00f4t</li> <li>Cr\u00e9ez une nouvelle branche pour votre fonctionnalit\u00e9</li> <li>Faites vos changements</li> <li> <p>Soumettez une demande pull avec une description claire de vos modifications</p> </li> <li> <p>Documentation : Aidez-nous \u00e0 am\u00e9liorer notre documentation en corrigeant les fautes de frappe, en ajoutant des exemples ou en clarifiant les sections d\u00e9routantes.</p> </li> <li> <p>Ajout de nouveaux moteurs : Si vous souhaitez ajouter la prise en charge d'un nouveau moteur TTS, veuillez d'abord ouvrir un num\u00e9ro pour discuter de l'impl\u00e9mentation.</p> </li> </ol> <p>Merci d'avoir contribu\u00e9 \u00e0 rendre RealtimeTTS meilleur !</p>"},{"location":"fr/fr/faq/","title":"Foire aux questions","text":"<p>Pour les r\u00e9ponses aux questions fr\u00e9quemment pos\u00e9es sur RealtimeTTS, veuillez vous r\u00e9f\u00e9rer \u00e0 notre page FAQ sur GitHub.</p> <p>Cette page couvre divers sujets dont</p> <ul> <li>Utilisation de diff\u00e9rents moteurs TTS</li> <li>Manipulation de textes multilingues</li> <li>Optimisation des performances</li> <li>D\u00e9pannage des probl\u00e8mes courants</li> </ul> <p>Pour des informations plus d\u00e9taill\u00e9es, veuillez consulter le lien ci-dessus.</p>"},{"location":"fr/fr/installation/","title":"Fran\u00e7ais","text":"<p>Remarque: Installation de base avec <code>pip install realtimetts</code>s n'est plus recommand\u00e9, utilisez <code>pip install realtimetts[all]</code> \u00e0 la place.</p> <p>La biblioth\u00e8que RealtimeTTS offre des options d'installation pour diverses d\u00e9pendances pour votre cas d'utilisation. Voici les diff\u00e9rentes fa\u00e7ons dont vous pouvez installer RealtimeTTS en fonction de vos besoins :</p>"},{"location":"fr/fr/installation/#installation-complete","title":"Installation compl\u00e8te","text":"<p>Pour installer RealtimeTTS avec prise en charge de tous les moteurs TTS :</p> <p><code>pip install -U realtimetts [tous]</code></p>"},{"location":"fr/fr/installation/#installation-personnalisee","title":"Installation personnalis\u00e9e","text":"<p>RealtimeTTS permet une installation personnalis\u00e9e avec un minimum d'installations de biblioth\u00e8que. Voici les options disponibles : - all : Installation compl\u00e8te avec chaque moteur pris en charge. - ** syst\u00e8me : Inclut les capacit\u00e9s TTS sp\u00e9cifiques au syst\u00e8me (par exemple, pyttsx3). - azure : ajoute le support vocal Azure Cognitive Services. - elevenlabs : Comprend l'int\u00e9gration avec l'API ElevenLabs. - openai : Pour les services vocaux OpenAI. - gtts : Prise en charge de Google Text-to-Speech. - coqui : Installe le moteur Coqui TTS. - minimal** : installe uniquement les exigences de base sans moteur (n\u00e9cessaire uniquement si vous souhaitez d\u00e9velopper votre propre moteur)</p> <p>Supposons que vous souhaitiez installer RealtimeTTS uniquement pour l'utilisation neuronale locale de Coqui TTS, vous devez alors utiliser :</p> <p><code>pip installez realtimetts [coqui]</code></p> <p>Par exemple, si vous souhaitez installer RealtimeTTS avec uniquement Azure Cognitive Services Speech, ElevenLabs et la prise en charge d'OpenAI :</p> <p><code>pip installez realtimetts[azure,elevenlabs,openai]</code></p>"},{"location":"fr/fr/installation/#installation-de-lenvironnement-virtuel","title":"Installation de l'environnement virtuel","text":"<p>Pour ceux qui souhaitent effectuer une installation compl\u00e8te dans un environnement virtuel, proc\u00e9dez comme suit</p> <p><code>python - m venv env_realtimetts env_realtimetts\\Scripts\\activate.bat python.exe - m pip install - upgrade pip pip install -U realtimetts [tous]</code></p> <p>Plus d'informations sur installation CUDA.</p>"},{"location":"fr/fr/installation/#exigences-du-moteur","title":"Exigences du moteur","text":"<p>Diff\u00e9rents moteurs pris en charge par RealtimeTTS ont des exigences uniques. Assurez-vous de remplir ces exigences en fonction du moteur que vous choisissez.</p>"},{"location":"fr/fr/installation/#moteur-systeme","title":"Moteur syst\u00e8me","text":"<p>Le `SystemEngine fonctionne d\u00e8s le d\u00e9part avec les capacit\u00e9s TTS int\u00e9gr\u00e9es de votre syst\u00e8me. Aucune configuration suppl\u00e9mentaire n'est n\u00e9cessaire.</p>"},{"location":"fr/fr/installation/#gttsengine","title":"GTTSEngine","text":"<p>Le <code>GTTSEngine</code> fonctionne d\u00e8s le d\u00e9part \u00e0 l'aide de l'API de synth\u00e8se vocale de Google Translate. Aucune configuration suppl\u00e9mentaire n'est n\u00e9cessaire.</p>"},{"location":"fr/fr/installation/#openaiengine","title":"OpenAIEngine","text":"<p>Pour utiliser le ``(OpenAIE): - d\u00e9finir la variable d'environnement OPENAI_API_KEY - installer ffmpeg (voir installation CUDA point 3)</p>"},{"location":"fr/fr/installation/#azureengine","title":"AzureEngine","text":"<p>Pour utiliser le <code>ine</code>, vous aurez besoin de : - Cl\u00e9 API Microsoft Azure Text-to-Speech (fournie via le param\u00e8tre constructeur AzureEngine \u00ab speech_key \u00bb ou dans la variable d'environnement AZURE_SPEECH_KEY) - R\u00e9gion de service Microsoft Azure.</p> <p>Assurez-vous d'avoir ces informations d'identification disponibles et correctement configur\u00e9es lors de l'initialisation du <code>AzureEngine</code>.</p>"},{"location":"fr/fr/installation/#elevenlabsengine","title":"ElevenlabsEngine","text":"<p>Pour le <code>ElevenlabsEngine</code>, vous avez besoin de: - Cl\u00e9 API Elevenlabs (fournie via le param\u00e8tre constructeur ElevenlabsEngine \u00ab api_key \u00bb ou dans la variable d'environnement ELEVENLABS_API_KEY) - <code>mpv</code> installed on your system (essential for streaming mpeg audio, Elevenlabs ne d\u00e9livre que mpeg).</p>"},{"location":"fr/fr/installation/#elevenlabsengine_1","title":"ElevenlabsEngine","text":"<p>Pour le <code>ElevenlabsEngine</code>, vous avez besoin de: - Cl\u00e9 API Elevenlabs (fournie via le param\u00e8tre constructeur ElevenlabsEngine \u00ab api_key \u00bb ou dans la variable d'environnement ELEVENLABS_API_KEY) - <code>mpv</code> installed on your system (essential for streaming mpeg audio, Elevenlabs ne d\u00e9livre que mpeg).</p> <p>\ud83d\udd39 Installation <code>v</code>:   - macOS:     <code>infuser installer mpv</code></p> <ul> <li>Linux et Windows : Visitez mpv.io pour les instructions d'installation.</li> </ul>"},{"location":"fr/fr/installation/#coquiengine","title":"CoquiEngine","text":"<p>Offre un TTS neuronal local de haute qualit\u00e9 avec clonage vocal.</p> <p>T\u00e9l\u00e9charge d'abord un mod\u00e8le TTS neuronal. Dans la plupart des cas, il est suffisamment rapide pour le temps r\u00e9el utilisant la synth\u00e8se GPU. N\u00e9cessite environ 4 \u00e0 5 Go de VRAM.</p> <ul> <li>pour cloner une voix, soumettez le nom de fichier d'un fichier d'onde contenant la voix source comme param\u00e8tre \u00ab voix \u00bb au constructeur CoquiEngine</li> <li>le clonage vocal fonctionne mieux avec un fichier WAV mono 16 bits de 22 050 Hz contenant un \u00e9chantillon court (~5 \u00e0 30 secondes)</li> </ul> <p>Sur la plupart des syst\u00e8mes, la prise en charge du GPU sera n\u00e9cessaire pour fonctionner suffisamment rapidement en temps r\u00e9el, sinon vous ferez l'exp\u00e9rience du b\u00e9gaiement.</p>"},{"location":"fr/fr/installation/#installation-cuda","title":"Installation CUDA","text":"<p>Ces \u00e9tapes sont recommand\u00e9es pour ceux qui ont besoin de ** meilleures performances ** et disposent d'un GPU NVIDIA compatible.</p> <p>Remarque : pour v\u00e9rifier si votre GPU NVIDIA prend en charge CUDA, visitez la liste officielle des GPU CUDA.</p> <p>Pour utiliser une torche avec support via CUDA, veuillez suivre ces \u00e9tapes :</p> <p>Remarque : les installations de pythorque plus r\u00e9centes peuvent (non v\u00e9rifi\u00e9) n'ont plus besoin d'installation de Toolkit (et \u00e9ventuellement de cuDNN).</p> <ol> <li> <p>Installer NVIDIA CUDA Toolkit:     Par exemple, pour installer Toolkit 12.X, s'il te pla\u00eet</p> <ul> <li>Visitez NVIDIA CUDA T\u00e9l\u00e9chargements.</li> <li>S\u00e9lectionnez votre syst\u00e8me d'exploitation, votre architecture syst\u00e8me et votre version os.</li> <li>T\u00e9l\u00e9chargez et installez le logiciel.</li> </ul> <p>ou pour installer Toolkit 11.8, s'il vous pla\u00eet - Visitez Archive de la bo\u00eete \u00e0 outils CUDA NVIDIA. - S\u00e9lectionnez votre syst\u00e8me d'exploitation, votre architecture syst\u00e8me et votre version os. - T\u00e9l\u00e9chargez et installez le logiciel.</p> </li> <li> <p>Installer NVIDIA cuDNN:</p> <p>Par exemple, pour installer cuDNN 8.7.0 pour CUDA 11. x s'il vous pla\u00eet - Visitez NVIDIA cuDNN Archive. - Cliquez sur \u00ab T\u00e9l\u00e9charger cuDNN v8.7.0 (28 novembre 2022), pour CUDA 11.x \u00bb. - T\u00e9l\u00e9chargez et installez le logiciel.</p> </li> <li> <p>Installer ffmpeg:</p> <p>Vous pouvez t\u00e9l\u00e9charger un programme d'installation pour votre syst\u00e8me d'exploitation \u00e0 partir du site Web deffmpeg.</p> <p>Ou utilisez un gestionnaire de packages :</p> <ul> <li> <p>Sur Ubuntu ou Debian:     <code>sudo apt update &amp; &amp; sudo apt install ffmpeg</code></p> </li> <li> <p>Sur Arch Linux:     <code>sudo pacman -S ffmpeg</code></p> </li> <li> <p>Sur MacOS utilisant Homebrew (https://brew.sh/):     <code>infuser installer ffmpeg</code></p> </li> <li> <p>Sur Windows utilisant Chocolatey (https://chocolatey.org/):     <code>choco installer ffmpeg</code></p> </li> <li> <p>Sur Windows utilisant Scoop (https://scoop.sh/):     <code>scoop installer ffmpeg</code></p> </li> </ul> </li> <li> <p>Installez PyTorch avec le support CUDA :</p> <p>Pour mettre \u00e0 niveau votre installation PyTorch afin d'activer le support GPU avec CUDA, suivez ces instructions en fonction de votre version CUDA sp\u00e9cifique. Ceci est utile si vous souhaitez am\u00e9liorer les performances de RealtimeSTT avec les capacit\u00e9s CUDA.</p> <ul> <li> <p>Pour CUDA 11.8:</p> <p>Pour mettre \u00e0 jour PyTorch et Torchaudio afin de prendre en charge CUDA 11.8, utilisez les commandes suivantes :</p> <p><code>pip installe torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p> </li> <li> <p>Pour CUDA 12.X:</p> <p>Pour mettre \u00e0 jour PyTorch et Torchaudio pour prendre en charge CUDA 12.X, ex\u00e9cutez ce qui suit :</p> <p><code>pip installe torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code></p> </li> </ul> <p>Remplacer <code></code> ` of PyTorch that matching your system and requirements.</p> </li> <li> <p>** Correction pour r\u00e9soudre les probl\u00e8mes de compatibilit\u00e9** :     Si vous rencontrez des probl\u00e8mes de compatibilit\u00e9 de biblioth\u00e8que, essayez de d\u00e9finir ces biblioth\u00e8ques sur des versions fixes :</p> </li> </ol> <p>``` </p> <pre><code>pip install networkx==2.8.8\n\npip install typing_extensions==4.8.0\n\npip install fsspec==2023.6.0\n\npip install imageio==2.31.6\n\npip install networkx==2.8.8\n\npip install numpy==1.24.3\n\npip install requests==2.31.0\n</code></pre> <p>```</p>"},{"location":"fr/fr/usage/","title":"Utilisation","text":""},{"location":"fr/fr/usage/#demarrage-rapide","title":"D\u00e9marrage rapide","text":"<p>Voici un exemple d'utilisation de base :</p> <p><code>```(</code>python depuis RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine</p> <p>moteur = SystemEngine () # remplacer par votre moteur TTS flux = TextToAudioStream(moteur) stream.feed(\"Bonjour le monde! Comment \u00e7a va aujourd'hui ?\") stream.play_async() ``</p>"},{"location":"fr/fr/usage/#flux-texte","title":"Flux Texte","text":"<p>Vous pouvez alimenter des cha\u00eenes individuelles :</p> <p><code>``(`python stream.feed(\u00ab Bonjour, c'est une phrase. \u00bb)</code></p> <p>Ou vous pouvez alimenter des g\u00e9n\u00e9rateurs et des it\u00e9rateurs de caract\u00e8res pour le streaming en temps r\u00e9el :</p> <p><code>```(</code>python def write (prompt : str) :     pour chunk en openai.ChatCompletion.create(         mod\u00e8le=\"gpt-3.5-turbo\",         messages=[{\"role\": \"utilisateur\", \"contenu\" : prompt}],         stream=True     ):         si (text_chunk := chunk[\u00ab choix \u00bb][0][\u00ab delta \u00bb].get(\u00ab contenu \u00bb)) n'est pas Aucun :             produire du texte_chunk</p> <p>text_stream = write (\u00ab Un discours relaxant en trois phrases \u00bb)</p> <p>stream.feed(text_stream) ``</p> <p><code>``(`python char_iterator = iter (\u00ab Diffusion de ce personnage par personnage \u00bb) stream.feed (char_iterator)</code></p>"},{"location":"fr/fr/usage/#layback","title":"Layback","text":"<p>Asynchrone:</p> <p><code>``(`python stream.play_async() pendant que stream.is_playing():     temps.sommeil(0,1)</code></p> <p>Synchronis\u00e9:</p> <p><code>``(`python stream.play()</code></p>"},{"location":"fr/fr/usage/#tester-la-bibliotheque","title":"Tester la biblioth\u00e8que","text":"<p>Le sous-r\u00e9pertoire de test contient un ensemble de scripts pour vous aider \u00e0 \u00e9valuer et comprendre les capacit\u00e9s de la biblioth\u00e8que RealtimeTTS.</p> <p>Notez que la plupart des tests reposent toujours sur l'\u00ab ancienne \u00bb API OpenAI (&lt;1.0.0). L'utilisation de la nouvelle API OpenAI est d\u00e9montr\u00e9e dans openai_1.0_test.py.</p> <ul> <li> <p>simple_test.py</p> <ul> <li>Description : Une d\u00e9monstration de style \u00ab hello world \u00bb de l'usage le plus simple de la biblioth\u00e8que.</li> </ul> </li> <li> <p>complex_test.py</p> <ul> <li>Description : Une d\u00e9monstration compl\u00e8te pr\u00e9sentant la plupart des fonctionnalit\u00e9s fournies par la biblioth\u00e8que.</li> </ul> </li> <li> <p>coqui_test.py</p> <ul> <li>Description : Test du moteur local coqui TTS.</li> </ul> </li> <li> <p>traducteur.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai realtimestt</code>.</li> <li>Description : Traductions en temps r\u00e9el dans six langues diff\u00e9rentes.</li> </ul> </li> <li> <p>openai_voice_interface.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai realtimestt</code>.</li> <li>Description : Interface utilisateur activ\u00e9e par mot de r\u00e9veil et bas\u00e9e sur la voix vers l'API OpenAI.</li> </ul> </li> <li> <p>advanced_talk.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai keyboard realtimestt</code>.</li> <li>Description : Choisissez le moteur et la voix TTS avant de d\u00e9marrer la conversation sur l'IA.</li> </ul> </li> <li> <p>_talkbot.py minimaliste</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai realtimestt</code>.</li> <li>Description : Un talkbot basique en 20 lignes de code.</li> </ul> </li> <li> <p>simple_llm_test.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai</code>.</li> <li>Description : D\u00e9monstration simple de la fa\u00e7on d'int\u00e9grer la biblioth\u00e8que avec de grands mod\u00e8les de langage (LLM).</li> </ul> </li> <li> <p>test_callbacks.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai</code>.</li> <li>Description : pr\u00e9sente les rappels et vous permet de v\u00e9rifier les temps de latence dans un environnement d'application r\u00e9el.</li> </ul> </li> </ul>"},{"location":"fr/fr/usage/#mettre-en-pause-reprendre-et-arreter","title":"Mettre en pause, reprendre et arr\u00eater","text":"<p>Mettre en pause le flux audio :</p> <p><code>``(`python stream.pause()</code></p> <p>Reprendre un flux en pause :</p> <p><code>``(`python stream.reprendre()</code></p> <p>Arr\u00eatez imm\u00e9diatement le flux :</p> <p><code>``(`python stream.stop()</code></p>"},{"location":"fr/fr/usage/#exigences-expliquees","title":"Exigences expliqu\u00e9es","text":"<ul> <li>Version Python:</li> <li>Obligatoire: Python &gt;= 3.9, &lt; 3.13</li> <li> <p>Raison : La biblioth\u00e8que d\u00e9pend de la biblioth\u00e8que GitHub \u00ab TTS \u00bb de coqui, qui n\u00e9cessite des versions Python dans cette gamme.</p> </li> <li> <p>PyAudio : pour cr\u00e9er un flux audio de sortie</p> </li> <li> <p>stream2sent : pour diviser le flux de texte entrant en phrases</p> </li> <li> <p>pyttsx3 : Moteur de conversion texte-parole du syst\u00e8me</p> </li> <li> <p>pydub : pour convertir les formats de morceaux audio</p> </li> <li> <p>azure-cognitiveservices-speech : Moteur de conversion texte-parole azur</p> </li> <li> <p>elevenlabs : Moteur de conversion texte-parole Elevenlabs</p> </li> <li> <p>coqui-TTS : Biblioth\u00e8que de synth\u00e8se vocale XTTS de Coqui pour un TTS neuronal local de haute qualit\u00e9</p> </li> </ul> <p>Criez \u00e0 Idiap Research Institute pour entretenir une fourche de coqui tts.</p> <ul> <li> <p>openai : pour interagir avec l'API TTS d'OpenAI</p> </li> <li> <p>gtts : Google traduit la conversion texte-parole</p> </li> </ul>"},{"location":"es/en/","title":"RealtimeTTS","text":"<p>EN | FR | ES</p> <p>Easy to use, low-latency text-to-speech library for realtime applications</p>"},{"location":"es/en/#about-the-project","title":"About the Project","text":"<p>RealtimeTTS is a state-of-the-art text-to-speech (TTS) library designed for real-time applications. It stands out in its ability to convert text streams fast into high-quality auditory output with minimal latency.</p>"},{"location":"es/en/#key-features","title":"Key Features","text":"<ul> <li>Low Latency: almost instantaneous text-to-speech conversion, compatible with LLM outputs</li> <li>High-Quality Audio: generates clear and natural-sounding speech</li> <li>Multiple TTS Engine Support: supports OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS and System TTS</li> <li>Multilingual</li> <li>Robust and Reliable: ensures continuous operation through a fallback mechanism, switches to alternative engines in case of disruptions guaranteeing consistent performance and reliability</li> </ul> <p>For installation instructions, usage examples, and API reference, please navigate through the documentation using the sidebar.</p>"},{"location":"es/en/api/","title":"English","text":""},{"location":"es/en/api/#configuration","title":"Configuration","text":""},{"location":"es/en/api/#initialization-parameters-for-texttoaudiostream","title":"Initialization Parameters for <code>TextToAudioStream</code>","text":"<p>When you initialize the <code>TextToAudioStream</code> class, you have various options to customize its behavior. Here are the available parameters:</p>"},{"location":"es/en/api/#engine-baseengine","title":"<code>engine</code> (BaseEngine)","text":"<ul> <li>Type: BaseEngine</li> <li>Required: Yes</li> <li>Description: The underlying engine responsible for text-to-audio synthesis. You must provide an instance of <code>BaseEngine</code> or its subclass to enable audio synthesis.</li> </ul>"},{"location":"es/en/api/#on_text_stream_start-callable","title":"<code>on_text_stream_start</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is triggered when the text stream begins. Use it for any setup or logging you may need.</li> </ul>"},{"location":"es/en/api/#on_text_stream_stop-callable","title":"<code>on_text_stream_stop</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is activated when the text stream ends. You can use this for cleanup tasks or logging.</li> </ul>"},{"location":"es/en/api/#on_audio_stream_start-callable","title":"<code>on_audio_stream_start</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is invoked when the audio stream starts. Useful for UI updates or event logging.</li> </ul>"},{"location":"es/en/api/#on_audio_stream_stop-callable","title":"<code>on_audio_stream_stop</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is called when the audio stream stops. Ideal for resource cleanup or post-processing tasks.</li> </ul>"},{"location":"es/en/api/#on_character-callable","title":"<code>on_character</code> (callable)","text":"<ul> <li>Type: Callable function</li> <li>Required: No</li> <li>Description: This optional callback function is called when a single character is processed.</li> </ul>"},{"location":"es/en/api/#output_device_index-int","title":"<code>output_device_index</code> (int)","text":"<ul> <li>Type: Integer</li> <li>Required: No</li> <li>Default: None</li> <li>Description: Specifies the output device index to use. None uses the default device.</li> </ul>"},{"location":"es/en/api/#tokenizer-string","title":"<code>tokenizer</code> (string)","text":"<ul> <li>Type: String</li> <li>Required: No</li> <li>Default: nltk</li> <li>Description: Tokenizer to use for sentence splitting (currently \"nltk\" and \"stanza\" are supported).</li> </ul>"},{"location":"es/en/api/#language-string","title":"<code>language</code> (string)","text":"<ul> <li>Type: String</li> <li>Required: No</li> <li>Default: en</li> <li>Description: Language to use for sentence splitting.</li> </ul>"},{"location":"es/en/api/#muted-bool","title":"<code>muted</code> (bool)","text":"<ul> <li>Type: Bool</li> <li>Required: No</li> <li>Default: False</li> <li>Description: Global muted parameter. If True, no pyAudio stream will be opened. Disables audio playback via local speakers (in case you want to synthesize to file or process audio chunks) and overrides the play parameters muted setting.</li> </ul>"},{"location":"es/en/api/#level-int","title":"<code>level</code> (int)","text":"<ul> <li>Type: Integer</li> <li>Required: No</li> <li>Default: <code>logging.WARNING</code></li> <li>Description: Sets the logging level for the internal logger. This can be any integer constant from Python's built-in <code>logging</code> module.</li> </ul>"},{"location":"es/en/api/#example-usage","title":"Example Usage:","text":"<pre><code>engine = YourEngine()  # Substitute with your engine\nstream = TextToAudioStream(\n    engine=engine,\n    on_text_stream_start=my_text_start_func,\n    on_text_stream_stop=my_text_stop_func,\n    on_audio_stream_start=my_audio_start_func,\n    on_audio_stream_stop=my_audio_stop_func,\n    level=logging.INFO\n)\n</code></pre>"},{"location":"es/en/api/#methods","title":"Methods","text":""},{"location":"es/en/api/#play-and-play_async","title":"<code>play</code> and <code>play_async</code>","text":"<p>These methods are responsible for executing the text-to-audio synthesis and playing the audio stream. The difference is that <code>play</code> is a blocking function, while <code>play_async</code> runs in a separate thread, allowing other operations to proceed.</p>"},{"location":"es/en/api/#parameters","title":"Parameters:","text":""},{"location":"es/en/api/#fast_sentence_fragment-bool","title":"<code>fast_sentence_fragment</code> (bool)","text":"<ul> <li>Default: <code>True</code></li> <li>Description: When set to <code>True</code>, the method will prioritize speed, generating and playing sentence fragments faster. This is useful for applications where latency matters.</li> </ul>"},{"location":"es/en/api/#fast_sentence_fragment_allsentences-bool","title":"<code>fast_sentence_fragment_allsentences</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: When set to <code>True</code>, applies the fast sentence fragment processing to all sentences, not just the first one.</li> </ul>"},{"location":"es/en/api/#fast_sentence_fragment_allsentences_multiple-bool","title":"<code>fast_sentence_fragment_allsentences_multiple</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: When set to <code>True</code>, allows yielding multiple sentence fragments instead of just a single one.</li> </ul>"},{"location":"es/en/api/#buffer_threshold_seconds-float","title":"<code>buffer_threshold_seconds</code> (float)","text":"<ul> <li>Default: <code>0.0</code></li> <li> <p>Description: Specifies the time in seconds for the buffering threshold, which impacts the smoothness and continuity of audio playback.</p> </li> <li> <p>How it Works: Before synthesizing a new sentence, the system checks if there is more audio material left in the buffer than the time specified by <code>buffer_threshold_seconds</code>. If so, it retrieves another sentence from the text generator, assuming that it can fetch and synthesize this new sentence within the time window provided by the remaining audio in the buffer. This process allows the text-to-speech engine to have more context for better synthesis, enhancing the user experience.</p> </li> </ul> <p>A higher value ensures that there's more pre-buffered audio, reducing the likelihood of silence or gaps during playback. If you experience breaks or pauses, consider increasing this value.</p>"},{"location":"es/en/api/#minimum_sentence_length-int","title":"<code>minimum_sentence_length</code> (int)","text":"<ul> <li>Default: <code>10</code></li> <li>Description: Sets the minimum character length to consider a string as a sentence to be synthesized. This affects how text chunks are processed and played.</li> </ul>"},{"location":"es/en/api/#minimum_first_fragment_length-int","title":"<code>minimum_first_fragment_length</code> (int)","text":"<ul> <li>Default: <code>10</code></li> <li>Description: The minimum number of characters required for the first sentence fragment before yielding.</li> </ul>"},{"location":"es/en/api/#log_synthesized_text-bool","title":"<code>log_synthesized_text</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: When enabled, logs the text chunks as they are synthesized into audio. Helpful for auditing and debugging.</li> </ul>"},{"location":"es/en/api/#reset_generated_text-bool","title":"<code>reset_generated_text</code> (bool)","text":"<ul> <li>Default: <code>True</code></li> <li>Description: If True, reset the generated text before processing.</li> </ul>"},{"location":"es/en/api/#output_wavfile-str","title":"<code>output_wavfile</code> (str)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: If set, save the audio to the specified WAV file.</li> </ul>"},{"location":"es/en/api/#on_sentence_synthesized-callable","title":"<code>on_sentence_synthesized</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: A callback function that gets called after a single sentence fragment was synthesized.</li> </ul>"},{"location":"es/en/api/#before_sentence_synthesized-callable","title":"<code>before_sentence_synthesized</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: A callback function that gets called before a single sentence fragment gets synthesized.</li> </ul>"},{"location":"es/en/api/#on_audio_chunk-callable","title":"<code>on_audio_chunk</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: Callback function that gets called when a single audio chunk is ready.</li> </ul>"},{"location":"es/en/api/#tokenizer-str","title":"<code>tokenizer</code> (str)","text":"<ul> <li>Default: <code>\"nltk\"</code></li> <li>Description: Tokenizer to use for sentence splitting. Currently supports \"nltk\" and \"stanza\".</li> </ul>"},{"location":"es/en/api/#tokenize_sentences-callable","title":"<code>tokenize_sentences</code> (callable)","text":"<ul> <li>Default: <code>None</code></li> <li>Description: A custom function that tokenizes sentences from the input text. You can provide your own lightweight tokenizer if you are unhappy with nltk and stanza. It should take text as a string and return split sentences as a list of strings.</li> </ul>"},{"location":"es/en/api/#language-str","title":"<code>language</code> (str)","text":"<ul> <li>Default: <code>\"en\"</code></li> <li>Description: Language to use for sentence splitting.</li> </ul>"},{"location":"es/en/api/#context_size-int","title":"<code>context_size</code> (int)","text":"<ul> <li>Default: <code>12</code></li> <li>Description: The number of characters used to establish context for sentence boundary detection. A larger context improves the accuracy of detecting sentence boundaries.</li> </ul>"},{"location":"es/en/api/#context_size_look_overhead-int","title":"<code>context_size_look_overhead</code> (int)","text":"<ul> <li>Default: <code>12</code></li> <li>Description: Additional context size for looking ahead when detecting sentence boundaries.</li> </ul>"},{"location":"es/en/api/#muted-bool_1","title":"<code>muted</code> (bool)","text":"<ul> <li>Default: <code>False</code></li> <li>Description: If True, disables audio playback via local speakers. Useful when you want to synthesize to a file or process audio chunks without playing them.</li> </ul>"},{"location":"es/en/api/#sentence_fragment_delimiters-str","title":"<code>sentence_fragment_delimiters</code> (str)","text":"<ul> <li>Default: <code>\".?!;:,\\n\u2026)]}\u3002-\"</code></li> <li>Description: A string of characters that are considered sentence delimiters.</li> </ul>"},{"location":"es/en/api/#force_first_fragment_after_words-int","title":"<code>force_first_fragment_after_words</code> (int)","text":"<ul> <li>Default: <code>15</code></li> <li>Description: The number of words after which the first sentence fragment is forced to be yielded.</li> </ul>"},{"location":"es/en/contributing/","title":"Contributing to RealtimeTTS","text":"<p>We welcome contributions to RealtimeTTS! Here are some ways you can contribute:</p> <ol> <li> <p>Reporting Bugs: If you find a bug, please open an issue on our GitHub repository.</p> </li> <li> <p>Suggesting Enhancements: Have ideas for new features or improvements? We'd love to hear them! Open an issue to suggest enhancements.</p> </li> <li> <p>Code Contributions: Want to add a new feature or fix a bug? Great! Please follow these steps:</p> </li> <li>Fork the repository</li> <li>Create a new branch for your feature</li> <li>Make your changes</li> <li> <p>Submit a pull request with a clear description of your changes</p> </li> <li> <p>Documentation: Help us improve our documentation by fixing typos, adding examples, or clarifying confusing sections.</p> </li> <li> <p>Adding New Engines: If you want to add support for a new TTS engine, please open an issue first to discuss the implementation.</p> </li> </ol> <p>Thank you for helping make RealtimeTTS better!</p>"},{"location":"es/en/faq/","title":"Frequently Asked Questions","text":"<p>For answers to frequently asked questions about RealtimeTTS, please refer to our FAQ page on GitHub.</p> <p>This page covers various topics including:</p> <ul> <li>Usage of different TTS engines</li> <li>Handling of multilingual text</li> <li>Performance optimization</li> <li>Troubleshooting common issues</li> </ul> <p>For more detailed information, please visit the link above.</p>"},{"location":"es/en/installation/","title":"English","text":"<p>Note: Basic Installation with <code>pip install realtimetts</code> is not recommended anymore, use <code>pip install realtimetts[all]</code> instead.</p> <p>The RealtimeTTS library provides installation options for various dependencies for your use case. Here are the different ways you can install RealtimeTTS depending on your needs:</p>"},{"location":"es/en/installation/#full-installation","title":"Full Installation","text":"<p>To install RealtimeTTS with support for all TTS engines:</p> <pre><code>pip install -U realtimetts[all]\n</code></pre>"},{"location":"es/en/installation/#custom-installation","title":"Custom Installation","text":"<p>RealtimeTTS allows for custom installation with minimal library installations. Here are the options available: - all: Full installation with every engine supported. - system: Includes system-specific TTS capabilities (e.g., pyttsx3). - azure: Adds Azure Cognitive Services Speech support. - elevenlabs: Includes integration with ElevenLabs API. - openai: For OpenAI voice services. - gtts: Google Text-to-Speech support. - coqui: Installs the Coqui TTS engine. - minimal: Installs only the base requirements with no engine (only needed if you want to develop an own engine)</p> <p>Say you want to install RealtimeTTS only for local neuronal Coqui TTS usage, then you should use:</p> <pre><code>pip install realtimetts[coqui]\n</code></pre> <p>For example, if you want to install RealtimeTTS with only Azure Cognitive Services Speech, ElevenLabs, and OpenAI support:</p> <pre><code>pip install realtimetts[azure,elevenlabs,openai]\n</code></pre>"},{"location":"es/en/installation/#virtual-environment-installation","title":"Virtual Environment Installation","text":"<p>For those who want to perform a full installation within a virtual environment, follow these steps:</p> <pre><code>python -m venv env_realtimetts\nenv_realtimetts\\Scripts\\activate.bat\npython.exe -m pip install --upgrade pip\npip install -U realtimetts[all]\n</code></pre> <p>More information about CUDA installation.</p>"},{"location":"es/en/installation/#engine-requirements","title":"Engine Requirements","text":"<p>Different engines supported by RealtimeTTS have unique requirements. Ensure you fulfill these requirements based on the engine you choose.</p>"},{"location":"es/en/installation/#systemengine","title":"SystemEngine","text":"<p>The <code>SystemEngine</code> works out of the box with your system's built-in TTS capabilities. No additional setup is needed.</p>"},{"location":"es/en/installation/#gttsengine","title":"GTTSEngine","text":"<p>The <code>GTTSEngine</code> works out of the box using Google Translate's text-to-speech API. No additional setup is needed.</p>"},{"location":"es/en/installation/#openaiengine","title":"OpenAIEngine","text":"<p>To use the <code>OpenAIEngine</code>: - set environment variable OPENAI_API_KEY - install ffmpeg (see CUDA installation point 3)</p>"},{"location":"es/en/installation/#azureengine","title":"AzureEngine","text":"<p>To use the <code>AzureEngine</code>, you will need: - Microsoft Azure Text-to-Speech API key (provided via AzureEngine constructor parameter \"speech_key\" or in the environment variable AZURE_SPEECH_KEY) - Microsoft Azure service region.</p> <p>Make sure you have these credentials available and correctly configured when initializing the <code>AzureEngine</code>.</p>"},{"location":"es/en/installation/#elevenlabsengine","title":"ElevenlabsEngine","text":"<p>For the <code>ElevenlabsEngine</code>, you need: - Elevenlabs API key (provided via ElevenlabsEngine constructor parameter \"api_key\" or in the environment variable ELEVENLABS_API_KEY) - <code>mpv</code> installed on your system (essential for streaming mpeg audio, Elevenlabs only delivers mpeg).</p> <p>\ud83d\udd39 Installing <code>mpv</code>:   - macOS:     <code>brew install mpv</code></p> <ul> <li>Linux and Windows: Visit mpv.io for installation instructions.</li> </ul>"},{"location":"es/en/installation/#coquiengine","title":"CoquiEngine","text":"<p>Delivers high quality, local, neural TTS with voice-cloning.</p> <p>Downloads a neural TTS model first. In most cases it be fast enough for Realtime using GPU synthesis. Needs around 4-5 GB VRAM.</p> <ul> <li>to clone a voice submit the filename of a wave file containing the source voice as \"voice\" parameter to the CoquiEngine constructor</li> <li>voice cloning works best with a 22050 Hz mono 16bit WAV file containing a short (~5-30 sec) sample</li> </ul> <p>On most systems GPU support will be needed to run fast enough for realtime, otherwise you will experience stuttering.</p>"},{"location":"es/en/installation/#cuda-installation","title":"CUDA installation","text":"<p>These steps are recommended for those who require better performance and have a compatible NVIDIA GPU.</p> <p>Note: to check if your NVIDIA GPU supports CUDA, visit the official CUDA GPUs list.</p> <p>To use a torch with support via CUDA please follow these steps:</p> <p>Note: newer pytorch installations may (unverified) not need Toolkit (and possibly cuDNN) installation anymore.</p> <ol> <li> <p>Install NVIDIA CUDA Toolkit:     For example, to install Toolkit 12.X, please</p> <ul> <li>Visit NVIDIA CUDA Downloads.</li> <li>Select your operating system, system architecture, and os version.</li> <li>Download and install the software.</li> </ul> <p>or to install Toolkit 11.8, please - Visit NVIDIA CUDA Toolkit Archive. - Select your operating system, system architecture, and os version. - Download and install the software.</p> </li> <li> <p>Install NVIDIA cuDNN:</p> <p>For example, to install cuDNN 8.7.0 for CUDA 11.x please - Visit NVIDIA cuDNN Archive. - Click on \"Download cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x\". - Download and install the software.</p> </li> <li> <p>Install ffmpeg:</p> <p>You can download an installer for your OS from the ffmpeg Website.</p> <p>Or use a package manager:</p> <ul> <li> <p>On Ubuntu or Debian:     <code>sudo apt update &amp;&amp; sudo apt install ffmpeg</code></p> </li> <li> <p>On Arch Linux:     <code>sudo pacman -S ffmpeg</code></p> </li> <li> <p>On MacOS using Homebrew (https://brew.sh/):     <code>brew install ffmpeg</code></p> </li> <li> <p>On Windows using Chocolatey (https://chocolatey.org/):     <code>choco install ffmpeg</code></p> </li> <li> <p>On Windows using Scoop (https://scoop.sh/):     <code>scoop install ffmpeg</code></p> </li> </ul> </li> <li> <p>Install PyTorch with CUDA support:</p> <p>To upgrade your PyTorch installation to enable GPU support with CUDA, follow these instructions based on your specific CUDA version. This is useful if you wish to enhance the performance of RealtimeSTT with CUDA capabilities.</p> <ul> <li> <p>For CUDA 11.8:</p> <p>To update PyTorch and Torchaudio to support CUDA 11.8, use the following commands:</p> <p><code>pip install torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p> </li> <li> <p>For CUDA 12.X:</p> <p>To update PyTorch and Torchaudio to support CUDA 12.X, execute the following:</p> <p><code>pip install torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code></p> </li> </ul> <p>Replace <code>2.3.1</code> with the version of PyTorch that matches your system and requirements.</p> </li> <li> <p>Fix for to resolve compatibility issues:     If you run into library compatibility issues, try setting these libraries to fixed versions:</p> </li> </ol> <p>``` </p> <pre><code>pip install networkx==2.8.8\n\npip install typing_extensions==4.8.0\n\npip install fsspec==2023.6.0\n\npip install imageio==2.31.6\n\npip install networkx==2.8.8\n\npip install numpy==1.24.3\n\npip install requests==2.31.0\n</code></pre> <p>```</p>"},{"location":"es/en/usage/","title":"Usage","text":""},{"location":"es/en/usage/#quick-start","title":"Quick Start","text":"<p>Here's a basic usage example:</p> <pre><code>from RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine\n\nengine = SystemEngine() # replace with your TTS engine\nstream = TextToAudioStream(engine)\nstream.feed(\"Hello world! How are you today?\")\nstream.play_async()\n</code></pre>"},{"location":"es/en/usage/#feed-text","title":"Feed Text","text":"<p>You can feed individual strings:</p> <pre><code>stream.feed(\"Hello, this is a sentence.\")\n</code></pre> <p>Or you can feed generators and character iterators for real-time streaming:</p> <pre><code>def write(prompt: str):\n    for chunk in openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\" : prompt}],\n        stream=True\n    ):\n        if (text_chunk := chunk[\"choices\"][0][\"delta\"].get(\"content\")) is not None:\n            yield text_chunk\n\ntext_stream = write(\"A three-sentence relaxing speech.\")\n\nstream.feed(text_stream)\n</code></pre> <pre><code>char_iterator = iter(\"Streaming this character by character.\")\nstream.feed(char_iterator)\n</code></pre>"},{"location":"es/en/usage/#playback","title":"Playback","text":"<p>Asynchronously:</p> <pre><code>stream.play_async()\nwhile stream.is_playing():\n    time.sleep(0.1)\n</code></pre> <p>Synchronously:</p> <pre><code>stream.play()\n</code></pre>"},{"location":"es/en/usage/#testing-the-library","title":"Testing the Library","text":"<p>The test subdirectory contains a set of scripts to help you evaluate and understand the capabilities of the RealtimeTTS library.</p> <p>Note that most of the tests still rely on the \"old\" OpenAI API (&lt;1.0.0). Usage of the new OpenAI API is demonstrated in openai_1.0_test.py.</p> <ul> <li> <p>simple_test.py</p> <ul> <li>Description: A \"hello world\" styled demonstration of the library's simplest usage.</li> </ul> </li> <li> <p>complex_test.py</p> <ul> <li>Description: A comprehensive demonstration showcasing most of the features provided by the library.</li> </ul> </li> <li> <p>coqui_test.py</p> <ul> <li>Description: Test of local coqui TTS engine.</li> </ul> </li> <li> <p>translator.py</p> <ul> <li>Dependencies: Run <code>pip install openai realtimestt</code>.</li> <li>Description: Real-time translations into six different languages.</li> </ul> </li> <li> <p>openai_voice_interface.py</p> <ul> <li>Dependencies: Run <code>pip install openai realtimestt</code>.</li> <li>Description: Wake word activated and voice based user interface to the OpenAI API.</li> </ul> </li> <li> <p>advanced_talk.py</p> <ul> <li>Dependencies: Run <code>pip install openai keyboard realtimestt</code>.</li> <li>Description: Choose TTS engine and voice before starting AI conversation.</li> </ul> </li> <li> <p>minimalistic_talkbot.py</p> <ul> <li>Dependencies: Run <code>pip install openai realtimestt</code>.</li> <li>Description: A basic talkbot in 20 lines of code.</li> </ul> </li> <li> <p>simple_llm_test.py</p> <ul> <li>Dependencies: Run <code>pip install openai</code>.</li> <li>Description: Simple demonstration of how to integrate the library with large language models (LLMs).</li> </ul> </li> <li> <p>test_callbacks.py</p> <ul> <li>Dependencies: Run <code>pip install openai</code>.</li> <li>Description: Showcases the callbacks and lets you check the latency times in a real-world application environment.</li> </ul> </li> </ul>"},{"location":"es/en/usage/#pause-resume-stop","title":"Pause, Resume &amp; Stop","text":"<p>Pause the audio stream:</p> <pre><code>stream.pause()\n</code></pre> <p>Resume a paused stream:</p> <pre><code>stream.resume()\n</code></pre> <p>Stop the stream immediately:</p> <pre><code>stream.stop()\n</code></pre>"},{"location":"es/en/usage/#requirements-explained","title":"Requirements Explained","text":"<ul> <li>Python Version:</li> <li>Required: Python &gt;= 3.9, &lt; 3.13</li> <li> <p>Reason: The library depends on the GitHub library \"TTS\" from coqui, which requires Python versions in this range.</p> </li> <li> <p>PyAudio: to create an output audio stream</p> </li> <li> <p>stream2sentence: to split the incoming text stream into sentences</p> </li> <li> <p>pyttsx3: System text-to-speech conversion engine</p> </li> <li> <p>pydub: to convert audio chunk formats</p> </li> <li> <p>azure-cognitiveservices-speech: Azure text-to-speech conversion engine</p> </li> <li> <p>elevenlabs: Elevenlabs text-to-speech conversion engine</p> </li> <li> <p>coqui-TTS: Coqui's XTTS text-to-speech library for high-quality local neural TTS</p> </li> </ul> <p>Shoutout to Idiap Research Institute for maintaining a fork of coqui tts.</p> <ul> <li> <p>openai: to interact with OpenAI's TTS API</p> </li> <li> <p>gtts: Google translate text-to-speech conversion</p> </li> </ul>"},{"location":"es/es/","title":"RealtimeTTS","text":"<p>EN | FR | ES</p> <p>*Biblioteca de conversi\u00f3n de texto en voz f\u00e1cil de usar y de baja latencia para aplicaciones en tiempo real.</p>"},{"location":"es/es/#acerca-del-proyecto","title":"Acerca del proyecto","text":"<p>RealtimeTTS es una biblioteca de texto a voz (TTS) de \u00faltima generaci\u00f3n dise\u00f1ada para aplicaciones en tiempo real. Destaca por su capacidad para convertir r\u00e1pidamente flujos de texto en salida auditiva de alta calidad con una latencia m\u00ednima.</p>"},{"location":"es/es/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ul> <li>Baja latencia: conversi\u00f3n de texto a voz casi instant\u00e1nea, compatible con salidas LLM.</li> <li>Audio de alta calidad**: genera un habla clara y natural.</li> <li>Compatible con m\u00faltiples motores TTS**: compatible con OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS y System TTS</li> <li>Multiling\u00fce</li> <li>Robusto y fiable**: garantiza un funcionamiento continuo gracias a un mecanismo de reserva que cambia a motores alternativos en caso de interrupciones, lo que garantiza un rendimiento y una fiabilidad constantes.</li> </ul> <p>Para obtener instrucciones de instalaci\u00f3n, ejemplos de uso y referencias de la API, navegue por la documentaci\u00f3n utilizando la barra lateral.</p>"},{"location":"es/es/api/","title":"TextToAudioStream - Documentaci\u00f3n en Espa\u00f1ol","text":""},{"location":"es/es/api/#configuracion","title":"Configuraci\u00f3n","text":""},{"location":"es/es/api/#parametros-de-inicializacion-para-texttoaudiostream","title":"Par\u00e1metros de Inicializaci\u00f3n para <code>TextToAudioStream</code>","text":"<p>Cuando inicializa la clase <code>TextToAudioStream</code>, tiene varias opciones para personalizar su comportamiento. Aqu\u00ed est\u00e1n los par\u00e1metros disponibles:</p>"},{"location":"es/es/api/#parametros-principales","title":"Par\u00e1metros Principales","text":""},{"location":"es/es/api/#engine-baseengine","title":"<code>engine</code> (BaseEngine)","text":"<ul> <li>Tipo: BaseEngine</li> <li>Requerido: S\u00ed</li> <li>Descripci\u00f3n: El motor subyacente responsable de la s\u00edntesis de texto a audio. Debe proporcionar una instancia de <code>BaseEngine</code> o su subclase para habilitar la s\u00edntesis de audio.</li> </ul>"},{"location":"es/es/api/#on_text_stream_start-callable","title":"<code>on_text_stream_start</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se activa cuando comienza el flujo de texto. Util\u00edcela para cualquier configuraci\u00f3n o registro que pueda necesitar.</li> </ul>"},{"location":"es/es/api/#on_text_stream_stop-callable","title":"<code>on_text_stream_stop</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se activa cuando finaliza el flujo de texto. Puede utilizarla para tareas de limpieza o registro.</li> </ul>"},{"location":"es/es/api/#on_audio_stream_start-callable","title":"<code>on_audio_stream_start</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se invoca cuando comienza el flujo de audio. \u00datil para actualizaciones de UI o registro de eventos.</li> </ul>"},{"location":"es/es/api/#on_audio_stream_stop-callable","title":"<code>on_audio_stream_stop</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se llama cuando se detiene el flujo de audio. Ideal para limpieza de recursos o tareas de post-procesamiento.</li> </ul>"},{"location":"es/es/api/#on_character-callable","title":"<code>on_character</code> (callable)","text":"<ul> <li>Tipo: Funci\u00f3n callable</li> <li>Requerido: No</li> <li>Descripci\u00f3n: Esta funci\u00f3n de callback opcional se llama cuando se procesa un solo car\u00e1cter.</li> </ul>"},{"location":"es/es/api/#output_device_index-int","title":"<code>output_device_index</code> (int)","text":"<ul> <li>Tipo: Entero</li> <li>Requerido: No</li> <li>Valor predeterminado: None</li> <li>Descripci\u00f3n: Especifica el \u00edndice del dispositivo de salida a utilizar. None usa el dispositivo predeterminado.</li> </ul>"},{"location":"es/es/api/#tokenizer-string","title":"<code>tokenizer</code> (string)","text":"<ul> <li>Tipo: String</li> <li>Requerido: No</li> <li>Valor predeterminado: nltk</li> <li>Descripci\u00f3n: Tokenizador a utilizar para la divisi\u00f3n de oraciones (actualmente se admiten \"nltk\" y \"stanza\").</li> </ul>"},{"location":"es/es/api/#language-string","title":"<code>language</code> (string)","text":"<ul> <li>Tipo: String</li> <li>Requerido: No</li> <li>Valor predeterminado: en</li> <li>Descripci\u00f3n: Idioma a utilizar para la divisi\u00f3n de oraciones.</li> </ul>"},{"location":"es/es/api/#muted-bool","title":"<code>muted</code> (bool)","text":"<ul> <li>Tipo: Bool</li> <li>Requerido: No</li> <li>Valor predeterminado: False</li> <li>Descripci\u00f3n: Par\u00e1metro global de silencio. Si es True, no se abrir\u00e1 ning\u00fan flujo pyAudio. Deshabilita la reproducci\u00f3n de audio a trav\u00e9s de los altavoces locales.</li> </ul>"},{"location":"es/es/api/#level-int","title":"<code>level</code> (int)","text":"<ul> <li>Tipo: Entero</li> <li>Requerido: No</li> <li>Valor predeterminado: <code>logging.WARNING</code></li> <li>Descripci\u00f3n: Establece el nivel de registro para el registrador interno. Puede ser cualquier constante entera del m\u00f3dulo <code>logging</code> incorporado de Python.</li> </ul>"},{"location":"es/es/api/#ejemplo-de-uso","title":"Ejemplo de Uso","text":"<pre><code>engine = YourEngine()  # Sustituya con su motor\nstream = TextToAudioStream(\n    engine=engine,\n    on_text_stream_start=my_text_start_func,\n    on_text_stream_stop=my_text_stop_func,\n    on_audio_stream_start=my_audio_start_func,\n    on_audio_stream_stop=my_audio_stop_func,\n    level=logging.INFO\n)\n</code></pre>"},{"location":"es/es/api/#metodos","title":"M\u00e9todos","text":""},{"location":"es/es/api/#play-y-play_async","title":"<code>play</code> y <code>play_async</code>","text":"<p>Estos m\u00e9todos son responsables de ejecutar la s\u00edntesis de texto a audio y reproducir el flujo de audio. La diferencia es que <code>play</code> es una funci\u00f3n bloqueante, mientras que <code>play_async</code> se ejecuta en un hilo separado, permitiendo que otras operaciones contin\u00faen.</p>"},{"location":"es/es/api/#parametros-de-reproduccion","title":"Par\u00e1metros de Reproducci\u00f3n","text":""},{"location":"es/es/api/#fast_sentence_fragment-bool","title":"<code>fast_sentence_fragment</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>True</code></li> <li>Descripci\u00f3n: Cuando se establece en <code>True</code>, el m\u00e9todo priorizar\u00e1 la velocidad, generando y reproduciendo fragmentos de oraciones m\u00e1s r\u00e1pidamente.</li> </ul>"},{"location":"es/es/api/#fast_sentence_fragment_allsentences-bool","title":"<code>fast_sentence_fragment_allsentences</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Cuando se establece en <code>True</code>, aplica el procesamiento r\u00e1pido de fragmentos de oraciones a todas las oraciones.</li> </ul>"},{"location":"es/es/api/#fast_sentence_fragment_allsentences_multiple-bool","title":"<code>fast_sentence_fragment_allsentences_multiple</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Cuando se establece en <code>True</code>, permite generar m\u00faltiples fragmentos de oraciones.</li> </ul>"},{"location":"es/es/api/#buffer_threshold_seconds-float","title":"<code>buffer_threshold_seconds</code> (float)","text":"<ul> <li>Valor predeterminado: <code>0.0</code></li> <li>Descripci\u00f3n: Especifica el tiempo en segundos para el umbral de b\u00fafer.</li> </ul> <p>C\u00f3mo funciona: Antes de sintetizar una nueva oraci\u00f3n, el sistema verifica si queda m\u00e1s material de audio en el b\u00fafer que el tiempo especificado. Un valor m\u00e1s alto asegura que haya m\u00e1s audio pre-almacenado en el b\u00fafer.</p>"},{"location":"es/es/api/#minimum_sentence_length-int","title":"<code>minimum_sentence_length</code> (int)","text":"<ul> <li>Valor predeterminado: <code>10</code></li> <li>Descripci\u00f3n: Establece la longitud m\u00ednima de caracteres para considerar una cadena como una oraci\u00f3n.</li> </ul>"},{"location":"es/es/api/#minimum_first_fragment_length-int","title":"<code>minimum_first_fragment_length</code> (int)","text":"<ul> <li>Valor predeterminado: <code>10</code></li> <li>Descripci\u00f3n: El n\u00famero m\u00ednimo de caracteres requeridos para el primer fragmento de oraci\u00f3n.</li> </ul>"},{"location":"es/es/api/#log_synthesized_text-bool","title":"<code>log_synthesized_text</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Cuando est\u00e1 habilitado, registra los fragmentos de texto sintetizados.</li> </ul>"},{"location":"es/es/api/#reset_generated_text-bool","title":"<code>reset_generated_text</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>True</code></li> <li>Descripci\u00f3n: Si es True, reinicia el texto generado antes del procesamiento.</li> </ul>"},{"location":"es/es/api/#output_wavfile-str","title":"<code>output_wavfile</code> (str)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Si se establece, guarda el audio en el archivo WAV especificado.</li> </ul>"},{"location":"es/es/api/#funciones-de-callback","title":"Funciones de Callback","text":""},{"location":"es/es/api/#on_sentence_synthesized-callable","title":"<code>on_sentence_synthesized</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Se llama despu\u00e9s de sintetizar un fragmento de oraci\u00f3n.</li> </ul>"},{"location":"es/es/api/#before_sentence_synthesized-callable","title":"<code>before_sentence_synthesized</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Se llama antes de sintetizar un fragmento de oraci\u00f3n.</li> </ul>"},{"location":"es/es/api/#on_audio_chunk-callable","title":"<code>on_audio_chunk</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Se llama cuando un fragmento de audio est\u00e1 listo.</li> </ul>"},{"location":"es/es/api/#configuracion-de-tokenizacion","title":"Configuraci\u00f3n de Tokenizaci\u00f3n","text":""},{"location":"es/es/api/#tokenizer-str","title":"<code>tokenizer</code> (str)","text":"<ul> <li>Valor predeterminado: <code>\"nltk\"</code></li> <li>Descripci\u00f3n: Tokenizador para la divisi\u00f3n de oraciones. Admite \"nltk\" y \"stanza\".</li> </ul>"},{"location":"es/es/api/#tokenize_sentences-callable","title":"<code>tokenize_sentences</code> (callable)","text":"<ul> <li>Valor predeterminado: <code>None</code></li> <li>Descripci\u00f3n: Funci\u00f3n personalizada para tokenizar oraciones del texto de entrada.</li> </ul>"},{"location":"es/es/api/#language-str","title":"<code>language</code> (str)","text":"<ul> <li>Valor predeterminado: <code>\"en\"</code></li> <li>Descripci\u00f3n: Idioma para la divisi\u00f3n de oraciones.</li> </ul>"},{"location":"es/es/api/#parametros-de-contexto","title":"Par\u00e1metros de Contexto","text":""},{"location":"es/es/api/#context_size-int","title":"<code>context_size</code> (int)","text":"<ul> <li>Valor predeterminado: <code>12</code></li> <li>Descripci\u00f3n: Caracteres utilizados para establecer el contexto de l\u00edmites de oraciones.</li> </ul>"},{"location":"es/es/api/#context_size_look_overhead-int","title":"<code>context_size_look_overhead</code> (int)","text":"<ul> <li>Valor predeterminado: <code>12</code></li> <li>Descripci\u00f3n: Tama\u00f1o de contexto adicional para mirar hacia adelante.</li> </ul>"},{"location":"es/es/api/#otros-parametros","title":"Otros Par\u00e1metros","text":""},{"location":"es/es/api/#muted-bool_1","title":"<code>muted</code> (bool)","text":"<ul> <li>Valor predeterminado: <code>False</code></li> <li>Descripci\u00f3n: Deshabilita la reproducci\u00f3n de audio local si es True.</li> </ul>"},{"location":"es/es/api/#sentence_fragment_delimiters-str","title":"<code>sentence_fragment_delimiters</code> (str)","text":"<ul> <li>Valor predeterminado: <code>\".?!;:,\\n\u2026)]}\u3002-\"</code></li> <li>Descripci\u00f3n: Caracteres considerados como delimitadores de oraciones.</li> </ul>"},{"location":"es/es/api/#force_first_fragment_after_words-int","title":"<code>force_first_fragment_after_words</code> (int)","text":"<ul> <li>Valor predeterminado: <code>15</code></li> <li>Descripci\u00f3n: N\u00famero de palabras despu\u00e9s de las cuales se fuerza el primer fragmento.</li> </ul>"},{"location":"es/es/contributing/","title":"Contribuir a RealtimeTTS","text":"<p>Agradecemos cualquier contribuci\u00f3n a RealtimeTTS. Aqu\u00ed tienes algunas formas de contribuir:</p> <ol> <li> <p>Informar de errores: Si encuentras un error, por favor abre una incidencia en nuestro repositorio GitHub.</p> </li> <li> <p>Sugerir mejoras: \u00bfTienes ideas para nuevas funciones o mejoras? Nos encantar\u00eda escucharlas. Abre una incidencia para sugerir mejoras.</p> </li> <li> <p>Contribuciones de c\u00f3digo: \u00bfQuieres a\u00f1adir una nueva funci\u00f3n o corregir un error? \u00a1Perfecto! Sigue estos pasos:</p> </li> <li>Abre el repositorio</li> <li>Crea una nueva rama para tu funci\u00f3n</li> <li>Realice los cambios</li> <li> <p>Env\u00eda un pull request con una descripci\u00f3n clara de tus cambios</p> </li> <li> <p>Documentaci\u00f3n: Ay\u00fadanos a mejorar nuestra documentaci\u00f3n corrigiendo erratas, a\u00f1adiendo ejemplos o aclarando secciones confusas.</p> </li> <li> <p>A\u00f1adir nuevos motores: Si quieres a\u00f1adir soporte para un nuevo motor TTS, por favor abre una incidencia primero para discutir la implementaci\u00f3n.</p> </li> </ol> <p>Gracias por ayudarnos a mejorar RealtimeTTS.</p>"},{"location":"es/es/faq/","title":"Preguntas frecuentes","text":"<p>Para obtener respuestas a las preguntas m\u00e1s frecuentes sobre RealtimeTTS, consulta nuestra p\u00e1gina de preguntas frecuentes en GitHub.</p> <p>Esta p\u00e1gina cubre varios temas, entre ellos</p> <ul> <li>Uso de diferentes motores TTS</li> <li>Tratamiento de textos multiling\u00fces</li> <li>Optimizaci\u00f3n del rendimiento</li> <li>Soluci\u00f3n de problemas comunes</li> </ul> <p>Para obtener informaci\u00f3n m\u00e1s detallada, visite el enlace anterior.</p>"},{"location":"es/es/installation/","title":"Espa\u00f1ol","text":"<p>Nota: Ya no se recomienda la instalaci\u00f3n b\u00e1sica con <code>pip install realtimetts</code>, use <code>pip install realtimetts[all]</code> en su lugar.</p> <p>La biblioteca RealtimeTTS proporciona opciones de instalaci\u00f3n para varias dependencias seg\u00fan su caso de uso. Aqu\u00ed est\u00e1n las diferentes formas en que puede instalar RealtimeTTS seg\u00fan sus necesidades:</p>"},{"location":"es/es/installation/#instalacion-completa","title":"Instalaci\u00f3n Completa","text":"<p>Para instalar RealtimeTTS con soporte para todos los motores de TTS:</p> <pre><code>pip install -U realtimetts[all]\n</code></pre>"},{"location":"es/es/installation/#instalacion-personalizada","title":"Instalaci\u00f3n Personalizada","text":"<p>RealtimeTTS permite una instalaci\u00f3n personalizada con instalaciones m\u00ednimas de bibliotecas. Estas son las opciones disponibles: - all: Instalaci\u00f3n completa con todos los motores soportados. - system: Incluye capacidades de TTS espec\u00edficas del sistema (por ejemplo, pyttsx3). - azure: Agrega soporte para Azure Cognitive Services Speech. - elevenlabs: Incluye integraci\u00f3n con la API de ElevenLabs. - openai: Para servicios de voz de OpenAI. - gtts: Soporte para Google Text-to-Speech. - coqui: Instala el motor Coqui TTS. - minimal: Instala solo los requisitos base sin motor (solo necesario si desea desarrollar un motor propio)</p> <p>Por ejemplo, si desea instalar RealtimeTTS solo para uso local de Coqui TTS neuronal, debe usar:</p> <pre><code>pip install realtimetts[coqui]\n</code></pre> <p>Si desea instalar RealtimeTTS solo con Azure Cognitive Services Speech, ElevenLabs y soporte de OpenAI:</p> <pre><code>pip install realtimetts[azure,elevenlabs,openai]\n</code></pre>"},{"location":"es/es/installation/#instalacion-en-entorno-virtual","title":"Instalaci\u00f3n en Entorno Virtual","text":"<p>Para aquellos que deseen realizar una instalaci\u00f3n completa dentro de un entorno virtual, sigan estos pasos:</p> <pre><code>python -m venv env_realtimetts\nenv_realtimetts\\Scripts\\activate.bat\npython.exe -m pip install --upgrade pip\npip install -U realtimetts[all]\n</code></pre> <p>M\u00e1s informaci\u00f3n sobre instalaci\u00f3n de CUDA.</p>"},{"location":"es/es/installation/#requisitos-de-los-motores","title":"Requisitos de los Motores","text":"<p>Los diferentes motores soportados por RealtimeTTS tienen requisitos \u00fanicos. Aseg\u00farese de cumplir con estos requisitos seg\u00fan el motor que elija.</p>"},{"location":"es/es/installation/#systemengine","title":"SystemEngine","text":"<p>El <code>SystemEngine</code> funciona de inmediato con las capacidades de TTS incorporadas en su sistema. No se necesita configuraci\u00f3n adicional.</p>"},{"location":"es/es/installation/#gttsengine","title":"GTTSEngine","text":"<p>El <code>GTTSEngine</code> funciona de inmediato usando la API de texto a voz de Google Translate. No se necesita configuraci\u00f3n adicional.</p>"},{"location":"es/es/installation/#openaiengine","title":"OpenAIEngine","text":"<p>Para usar el <code>OpenAIEngine</code>: - configure la variable de entorno OPENAI_API_KEY - instale ffmpeg (ver instalaci\u00f3n de CUDA punto 3)</p>"},{"location":"es/es/installation/#azureengine","title":"AzureEngine","text":"<p>Para usar el <code>AzureEngine</code>, necesitar\u00e1: - Clave API de Microsoft Azure Text-to-Speech (proporcionada a trav\u00e9s del par\u00e1metro \"speech_key\" del constructor AzureEngine o en la variable de entorno AZURE_SPEECH_KEY) - Regi\u00f3n de servicio de Microsoft Azure.</p> <p>Aseg\u00farese de tener estas credenciales disponibles y correctamente configuradas al inicializar el <code>AzureEngine</code>.</p>"},{"location":"es/es/installation/#elevenlabsengine","title":"ElevenlabsEngine","text":"<p>Para el <code>ElevenlabsEngine</code>, necesita: - Clave API de Elevenlabs (proporcionada a trav\u00e9s del par\u00e1metro \"api_key\" del constructor ElevenlabsEngine o en la variable de entorno ELEVENLABS_API_KEY) - <code>mpv</code> instalado en su sistema (esencial para transmitir audio mpeg, Elevenlabs solo entrega mpeg).</p> <p>\ud83d\udd39 Instalaci\u00f3n de <code>mpv</code>:   - macOS:     <code>brew install mpv</code></p> <ul> <li>Linux y Windows: Visite mpv.io para instrucciones de instalaci\u00f3n.</li> </ul>"},{"location":"es/es/installation/#coquiengine","title":"CoquiEngine","text":"<p>Proporciona TTS neuronal local de alta calidad con clonaci\u00f3n de voz.</p> <p>Descarga primero un modelo neuronal TTS. En la mayor\u00eda de los casos, ser\u00e1 lo suficientemente r\u00e1pido para tiempo real usando s\u00edntesis GPU. Necesita alrededor de 4-5 GB de VRAM.</p> <ul> <li>para clonar una voz, env\u00ede el nombre del archivo de un archivo wave que contenga la voz fuente como par\u00e1metro \"voice\" al constructor CoquiEngine</li> <li>la clonaci\u00f3n de voz funciona mejor con un archivo WAV mono de 16 bits a 22050 Hz que contenga una muestra corta (~5-30 seg)</li> </ul> <p>En la mayor\u00eda de los sistemas, se necesitar\u00e1 soporte de GPU para ejecutarse lo suficientemente r\u00e1pido en tiempo real, de lo contrario experimentar\u00e1 tartamudeo.</p>"},{"location":"es/es/installation/#instalacion-de-cuda","title":"Instalaci\u00f3n de CUDA","text":"<p>Estos pasos son recomendados para aquellos que requieren mejor rendimiento y tienen una GPU NVIDIA compatible.</p> <p>Nota: para verificar si su GPU NVIDIA es compatible con CUDA, visite la lista oficial de GPUs CUDA.</p> <p>Para usar torch con soporte v\u00eda CUDA, siga estos pasos:</p> <p>Nota: las instalaciones m\u00e1s nuevas de pytorch pueden (no verificado) no necesitar la instalaci\u00f3n de Toolkit (y posiblemente cuDNN).</p> <ol> <li> <p>Instalar NVIDIA CUDA Toolkit:     Por ejemplo, para instalar Toolkit 12.X, por favor</p> <ul> <li>Visite NVIDIA CUDA Downloads.</li> <li>Seleccione su sistema operativo, arquitectura del sistema y versi\u00f3n del sistema operativo.</li> <li>Descargue e instale el software.</li> </ul> <p>o para instalar Toolkit 11.8, por favor - Visite NVIDIA CUDA Toolkit Archive. - Seleccione su sistema operativo, arquitectura del sistema y versi\u00f3n del sistema operativo. - Descargue e instale el software.</p> </li> <li> <p>Instalar NVIDIA cuDNN:</p> <p>Por ejemplo, para instalar cuDNN 8.7.0 para CUDA 11.x por favor - Visite NVIDIA cuDNN Archive. - Haga clic en \"Download cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x\". - Descargue e instale el software.</p> </li> <li> <p>Instalar ffmpeg:</p> <p>Puede descargar un instalador para su sistema operativo desde el sitio web de ffmpeg.</p> <p>O usar un gestor de paquetes:</p> <ul> <li> <p>En Ubuntu o Debian:     <code>sudo apt update &amp;&amp; sudo apt install ffmpeg</code></p> </li> <li> <p>En Arch Linux:     <code>sudo pacman -S ffmpeg</code></p> </li> <li> <p>En MacOS usando Homebrew (https://brew.sh/):     <code>brew install ffmpeg</code></p> </li> <li> <p>En Windows usando Chocolatey (https://chocolatey.org/):     <code>choco install ffmpeg</code></p> </li> <li> <p>En Windows usando Scoop (https://scoop.sh/):     <code>scoop install ffmpeg</code></p> </li> </ul> </li> <li> <p>Instalar PyTorch con soporte CUDA:</p> <p>Para actualizar su instalaci\u00f3n de PyTorch y habilitar el soporte de GPU con CUDA, siga estas instrucciones seg\u00fan su versi\u00f3n espec\u00edfica de CUDA. Esto es \u00fatil si desea mejorar el rendimiento de RealtimeSTT con capacidades CUDA.</p> <ul> <li> <p>Para CUDA 11.8:</p> <p>Para actualizar PyTorch y Torchaudio para soportar CUDA 11.8, use los siguientes comandos:</p> <p><code>pip install torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p> </li> <li> <p>Para CUDA 12.X:</p> <p>Para actualizar PyTorch y Torchaudio para soportar CUDA 12.X, ejecute lo siguiente:</p> <p><code>pip install torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code></p> </li> </ul> <p>Reemplace <code>2.3.1</code> con la versi\u00f3n de PyTorch que coincida con su sistema y requisitos.</p> </li> <li> <p>Soluci\u00f3n para resolver problemas de compatibilidad:     Si encuentra problemas de compatibilidad de bibliotecas, intente establecer estas bibliotecas en versiones fijas:</p> <p><code>pip install networkx==2.8.8 pip install typing_extensions==4.8.0 pip install fsspec==2023.6.0 pip install imageio==2.31.6 pip install networkx==2.8.8 pip install numpy==1.24.3 pip install requests==2.31.0</code></p> </li> </ol>"},{"location":"es/es/usage/","title":"Uso","text":""},{"location":"es/es/usage/#inicio-rapido","title":"Inicio R\u00e1pido","text":"<p>Aqu\u00ed hay un ejemplo b\u00e1sico de uso:</p> <pre><code>from RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine\n\nengine = SystemEngine() # replace with your TTS engine\nstream = TextToAudioStream(engine)\nstream.feed(\"Hello world! How are you today?\")\nstream.play_async()\n</code></pre>"},{"location":"es/es/usage/#alimentar-texto","title":"Alimentar Texto","text":"<p>Puede alimentar cadenas individuales:</p> <pre><code>stream.feed(\"Hello, this is a sentence.\")\n</code></pre> <p>O puede alimentar generadores e iteradores de caracteres para la transmisi\u00f3n en tiempo real:</p> <pre><code>def write(prompt: str):\n    for chunk in openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\" : prompt}],\n        stream=True\n    ):\n        if (text_chunk := chunk[\"choices\"][0][\"delta\"].get(\"content\")) is not None:\n            yield text_chunk\n\ntext_stream = write(\"A three-sentence relaxing speech.\")\n\nstream.feed(text_stream)\n</code></pre> <pre><code>char_iterator = iter(\"Streaming this character by character.\")\nstream.feed(char_iterator)\n</code></pre>"},{"location":"es/es/usage/#reproduccion","title":"Reproducci\u00f3n","text":"<p>De forma as\u00edncrona:</p> <pre><code>stream.play_async()\nwhile stream.is_playing():\n    time.sleep(0.1)\n</code></pre> <p>De forma s\u00edncrona:</p> <pre><code>stream.play()\n</code></pre>"},{"location":"es/es/usage/#prueba-de-la-biblioteca","title":"Prueba de la Biblioteca","text":"<p>El subdirectorio de pruebas contiene un conjunto de scripts para ayudarte a evaluar y comprender las capacidades de la biblioteca RealtimeTTS.</p> <p>Ten en cuenta que la mayor\u00eda de las pruebas a\u00fan dependen de la API \"antigua\" de OpenAI (&lt;1.0.0). El uso de la nueva API de OpenAI se demuestra en openai_1.0_test.py.</p> <ul> <li> <p>simple_test.py</p> <ul> <li>Descripci\u00f3n: Una demostraci\u00f3n tipo \"hola mundo\" del uso m\u00e1s simple de la biblioteca.</li> </ul> </li> <li> <p>complex_test.py</p> <ul> <li>Descripci\u00f3n: Una demostraci\u00f3n completa que muestra la mayor\u00eda de las caracter\u00edsticas proporcionadas por la biblioteca.</li> </ul> </li> <li> <p>coqui_test.py</p> <ul> <li>Descripci\u00f3n: Prueba del motor local coqui TTS.</li> </ul> </li> <li> <p>translator.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai realtimestt</code>.</li> <li>Descripci\u00f3n: Traducciones en tiempo real a seis idiomas diferentes.</li> </ul> </li> <li> <p>openai_voice_interface.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai realtimestt</code>.</li> <li>Descripci\u00f3n: Interfaz de usuario activada por palabra clave y basada en voz para la API de OpenAI.</li> </ul> </li> <li> <p>advanced_talk.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai keyboard realtimestt</code>.</li> <li>Descripci\u00f3n: Elija el motor TTS y la voz antes de iniciar la conversaci\u00f3n con IA.</li> </ul> </li> <li> <p>minimalistic_talkbot.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai realtimestt</code>.</li> <li>Descripci\u00f3n: Un talkbot b\u00e1sico en 20 l\u00edneas de c\u00f3digo.</li> </ul> </li> <li> <p>simple_llm_test.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai</code>.</li> <li>Descripci\u00f3n: Demostraci\u00f3n simple de c\u00f3mo integrar la biblioteca con modelos de lenguaje grande (LLMs).</li> </ul> </li> <li> <p>test_callbacks.py</p> <ul> <li>Dependencias: Ejecutar <code>pip install openai</code>.</li> <li>Descripci\u00f3n: Muestra los callbacks y te permite verificar los tiempos de latencia en un entorno de aplicaci\u00f3n del mundo real.</li> </ul> </li> </ul>"},{"location":"es/es/usage/#pausar-reanudar-y-detener","title":"Pausar, Reanudar y Detener","text":"<p>Pausar el flujo de audio:</p> <pre><code>stream.pause()\n</code></pre> <p>Reanudar un flujo pausado:</p> <pre><code>stream.resume()\n</code></pre> <p>Detener el flujo inmediatamente:</p> <pre><code>stream.stop()\n</code></pre>"},{"location":"es/es/usage/#requisitos-explicados","title":"Requisitos Explicados","text":"<ul> <li>Versi\u00f3n de Python:</li> <li>Requerido: Python &gt;= 3.9, &lt; 3.13</li> <li> <p>Raz\u00f3n: La biblioteca depende de la biblioteca GitHub \"TTS\" de coqui, que requiere versiones de Python en este rango.</p> </li> <li> <p>PyAudio: para crear un flujo de audio de salida</p> </li> <li> <p>stream2sentence: para dividir el flujo de texto entrante en oraciones</p> </li> <li> <p>pyttsx3: Motor de conversi\u00f3n de texto a voz del sistema</p> </li> <li> <p>pydub: para convertir formatos de fragmentos de audio</p> </li> <li> <p>azure-cognitiveservices-speech: Motor de conversi\u00f3n de texto a voz de Azure</p> </li> <li> <p>elevenlabs: Motor de conversi\u00f3n de texto a voz de Elevenlabs</p> </li> <li> <p>coqui-TTS: Biblioteca de texto a voz XTTS de Coqui para TTS neuronal local de alta calidad</p> </li> </ul> <p>Agradecimiento especial al Instituto de Investigaci\u00f3n Idiap por mantener un fork de coqui tts.</p> <ul> <li> <p>openai: para interactuar con la API TTS de OpenAI</p> </li> <li> <p>gtts: Conversi\u00f3n de texto a voz de Google translate</p> </li> </ul>"},{"location":"es/fr/","title":"RealtimeTTS","text":"<p>EN | FR | ES</p> <p>Biblioth\u00e8que de synth\u00e8se vocale \u00e0 faible latence et facile \u00e0 utiliser pour les applications en temps r\u00e9el</p>"},{"location":"es/fr/#a-propos-du-projet","title":"\u00c0 propos du projet","text":"<p>RealtimeTTS est une biblioth\u00e8que de synth\u00e8se vocale (TTS) de pointe con\u00e7ue pour les applications en temps r\u00e9el. Elle se distingue par sa capacit\u00e9 \u00e0 convertir des flux de texte en sortie auditive de haute qualit\u00e9 avec une latence minimale.</p>"},{"location":"es/fr/#caracteristiques-cles","title":"Caract\u00e9ristiques cl\u00e9s","text":"<ul> <li>Faible latence : conversion text-to-speech quasi-instantan\u00e9e, compatible avec les sorties LLM</li> <li>Audio de haute qualit\u00e9 : g\u00e9n\u00e8re un discours clair et naturel</li> <li>Support de plusieurs moteurs TTS : prend en charge OpenAI TTS, Elevenlabs, Azure Speech Services, Coqui TTS, gTTS et System TTS</li> <li>Multilingue</li> <li>Robuste et fiable : garantit une op\u00e9ration continue gr\u00e2ce \u00e0 un m\u00e9canisme de fallback, bascule vers des moteurs alternatifs en cas de perturbations, garantissant une performance et une fiabilit\u00e9 coh\u00e9rentes</li> </ul> <p>Pour les instructions d'installation, les exemples d'utilisation et la r\u00e9f\u00e9rence de l'API, veuillez naviguer \u00e0 travers la documentation \u00e0 l'aide du sidebar.</p>"},{"location":"es/fr/api/","title":"Fran\u00e7ais","text":""},{"location":"es/fr/api/#configuration","title":"Configuration","text":""},{"location":"es/fr/api/#parametres-dinitialisation-pour-texttoaudiostream","title":"Param\u00e8tres d'initialisation pour `TextToAudioStream","text":"<p>Lorsque vous initialisez la classe <code>TextToAudioStream</code>, vous disposez de diverses options pour personnaliser son comportement. Voici les param\u00e8tres disponibles :</p>"},{"location":"es/fr/api/#baseengine","title":"`(BaseEngine)","text":"<ul> <li>Type: BaseEngine</li> <li>Obligatoire: Oui</li> <li>Description : Le moteur sous-jacent responsable de la synth\u00e8se texte-audio. Vous devez fournir une instance de <code>ine</code> ou sa sous-classe pour permettre la synth\u00e8se audio.</li> </ul>"},{"location":"es/fr/api/#_text_stream_start-appelable","title":"<code>_text_stream_start</code> (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est d\u00e9clench\u00e9e lorsque le flux de texte commence. Utilisez-le pour toute configuration ou journalisation dont vous pourriez avoir besoin.</li> </ul>"},{"location":"es/fr/api/#_text_stream_stop-appelable","title":"<code>_text_stream_stop</code> (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est activ\u00e9e \u00e0 la fin du flux de texte. Vous pouvez l'utiliser pour des t\u00e2ches de nettoyage ou de journalisation.</li> </ul>"},{"location":"es/fr/api/#_audio_stream_start-appelable","title":"_audio_stream_start` (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel facultative est invoqu\u00e9e au d\u00e9marrage du flux audio. Utile pour les mises \u00e0 jour de l'interface utilisateur ou la journalisation des \u00e9v\u00e9nements.</li> </ul>"},{"location":"es/fr/api/#_audio_stream_stop-appelable","title":"<code>_audio_stream_stop</code> (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est appel\u00e9e lorsque le flux audio s'arr\u00eate. Id\u00e9al pour les t\u00e2ches de nettoyage des ressources ou de post-traitement.</li> </ul>"},{"location":"es/fr/api/#on_character-appelable","title":"on_character` (appelable)","text":"<ul> <li>Type: Fonction appelable</li> <li>Obligatoire: Non</li> <li>Description : Cette fonction de rappel optionnelle est appel\u00e9e lorsqu'un seul caract\u00e8re est trait\u00e9.</li> </ul>"},{"location":"es/fr/api/#_device_index-int","title":"<code>_device_index</code> (int)","text":"<ul> <li>Type: Entier</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: Aucun</li> <li>Description : Sp\u00e9cifie l'index du p\u00e9riph\u00e9rique de sortie \u00e0 utiliser. Aucun n'utilise le p\u00e9riph\u00e9rique par d\u00e9faut.</li> </ul>"},{"location":"es/fr/api/#tokenizerchaine","title":"<code>(tokenizer</code>(cha\u00eene)","text":"<ul> <li>Type: Cha\u00eene</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: nltk</li> <li>Description : Tokenizer \u00e0 utiliser pour le fractionnement des phrases (actuellement \u00ab nltk \u00bb et \u00ab stroza \u00bb sont pris en charge).</li> </ul>"},{"location":"es/fr/api/#languagechaine","title":"`language(cha\u00eene)","text":"<ul> <li>Type: Cha\u00eene</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: fr</li> <li>Description : Langue \u00e0 utiliser pour le fractionnement des phrases.</li> </ul>"},{"location":"es/fr/api/#mutedbool","title":"<code>muted</code>(bool)","text":"<ul> <li>Type: Bool</li> <li>Obligatoire: Non</li> <li>Par d\u00e9faut: Faux</li> <li>Description : Param\u00e8tre global coup\u00e9. Si True, aucun flux pyAudio ne sera ouvert. D\u00e9sactive la lecture audio via des haut-parleurs locaux (au cas o\u00f9 vous souhaitez synth\u00e9tiser dans un fichier ou traiter des morceaux audio) et remplace le param\u00e8tre de mise en sourdine des param\u00e8tres de lecture.</li> </ul>"},{"location":"es/fr/api/#level-int","title":"<code>level</code> (int)","text":"<ul> <li>Type: Entier</li> <li>Obligatoire: Non</li> <li>D\u00e9faut:<code>logging.AVERTISSEMENT</code></li> <li>Description : D\u00e9finit le niveau de journalisation pour l'enregistreur interne. Cela peut \u00eatre n'importe quelle constante enti\u00e8re du module <code>ging</code> int\u00e9gr\u00e9 de Python.</li> </ul>"},{"location":"es/fr/api/#exemple-dutilisation","title":"Exemple d'utilisation :","text":"<p><code>``(`python moteur = YourEngine () # Remplacez-vous par votre moteur flux = TextToAudioStream(     moteur=engine,     on_text_stream_start=my_text_start_func,     on_text_stream_stop=my_text_stop_func,     on_audio_stream_start=my_audio_start_func,     on_audio_stream_stop=my_audio_stop_func,     niveau=logging.INFO )</code></p>"},{"location":"es/fr/api/#methodes","title":"M\u00e9thodes","text":""},{"location":"es/fr/api/#play-etplay_async","title":"<code>play et</code>play_async`","text":"<p>Ces m\u00e9thodes sont responsables de l'ex\u00e9cution de la synth\u00e8se texte-audio et de la lecture du flux audio. La diff\u00e9rence est que <code>play</code> est une fonction de blocage, tandis que <code>play_async</code> s'ex\u00e9cute dans un thread s\u00e9par\u00e9, permettant \u00e0 d'autres op\u00e9rations de se poursuivre.</p>"},{"location":"es/fr/api/#parametres","title":"Param\u00e8tres :","text":""},{"location":"es/fr/api/#fast_sentence_fragment-bool","title":"fast<code>_sentence_fragment</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>True</code></li> <li>Description : Lorsqu'elle est d\u00e9finie sur <code>True</code>, la m\u00e9thode donnera la priorit\u00e9 \u00e0 la vitesse, g\u00e9n\u00e9rant et jouant plus rapidement des fragments de phrases. Ceci est utile pour les applications o\u00f9 la latence est importante.</li> </ul>"},{"location":"es/fr/api/#fast_sentence_fragment_allsentencesbool","title":"fast<code>_sentence_fragment_allsentences</code>(bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Lorsqu'il est d\u00e9fini sur <code>True</code>, applique le traitement rapide des fragments de phrase \u00e0 toutes les phrases, pas seulement \u00e0 la premi\u00e8re.</li> </ul>"},{"location":"es/fr/api/#fast_sentence_fragment_allsentences_multiple-bool","title":"fast<code>_sentence_fragment_allsentences_multiple</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Lorsqu'il est d\u00e9fini sur <code>True</code>, permet de produire plusieurs fragments de phrase au lieu d'un seul.</li> </ul>"},{"location":"es/fr/api/#_threshold_seconds-flotteur","title":"<code>_threshold_seconds</code> (flotteur)","text":"<ul> <li>Par d\u00e9faut: <code>0.0</code></li> <li> <p>Description : Sp\u00e9cifie le temps en secondes pour le seuil de mise en m\u00e9moire tampon, ce qui a un impact sur la douceur et la continuit\u00e9 de la lecture audio.</p> </li> <li> <p>Comment \u00e7a marche : Avant de synth\u00e9tiser une nouvelle phrase, le syst\u00e8me v\u00e9rifie s'il reste plus de mat\u00e9riel audio dans le tampon que le temps sp\u00e9cifi\u00e9 par <code>buffer_threshold_seconds</code>. Si tel est le cas, il r\u00e9cup\u00e8re une autre phrase du g\u00e9n\u00e9rateur de texte, en supposant qu'il peut r\u00e9cup\u00e9rer et synth\u00e9tiser cette nouvelle phrase dans la fen\u00eatre temporelle fournie par l'audio restant dans le tampon. Ce processus permet au moteur de synth\u00e8se vocale d'avoir plus de contexte pour une meilleure synth\u00e8se, am\u00e9liorant ainsi l'exp\u00e9rience utilisateur.</p> </li> </ul> <p>Une valeur plus \u00e9lev\u00e9e garantit qu'il y a plus d'audio pr\u00e9-tamponn\u00e9, r\u00e9duisant ainsi le risque de silence ou de lacunes pendant la lecture. Si vous rencontrez des pauses ou des pauses, envisagez d'augmenter cette valeur.</p>"},{"location":"es/fr/api/#_sentence_length-int","title":"<code>_sentence_length</code> (int)","text":"<ul> <li>Par d\u00e9faut: <code>10</code></li> <li>Description : D\u00e9finit la longueur minimale des caract\u00e8res pour consid\u00e9rer une cha\u00eene comme une phrase \u00e0 synth\u00e9tiser. Cela affecte la fa\u00e7on dont les morceaux de texte sont trait\u00e9s et lus.</li> </ul>"},{"location":"es/fr/api/#_first_fragment_lengthint","title":"<code>_first_fragment_length</code>(int)","text":"<ul> <li>Par d\u00e9faut: <code>10</code></li> <li>Description : Le nombre minimum de caract\u00e8res requis pour le premier fragment de phrase avant de c\u00e9der.</li> </ul>"},{"location":"es/fr/api/#_synthesized_text-bool","title":"<code>_synthesized_text</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Lorsqu'il est activ\u00e9, enregistre les morceaux de texte au fur et \u00e0 mesure de leur synth\u00e8se en audio. Utile pour l'audit et le d\u00e9bogage.</li> </ul>"},{"location":"es/fr/api/#reset_generated_text-bool","title":"#reset_generated_text` (bool)","text":"<ul> <li>Par d\u00e9faut: <code>True</code></li> <li>Description : Si Vrai, r\u00e9initialisez le texte g\u00e9n\u00e9r\u00e9 avant le traitement.</li> </ul>"},{"location":"es/fr/api/#_wavfile-str","title":"<code>_wavfile</code> (str)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Si d\u00e9fini, enregistrez l'audio dans le fichier WAV sp\u00e9cifi\u00e9.</li> </ul>"},{"location":"es/fr/api/#_sentence_synthesized-appelable","title":"`_sentence_synthesized (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Une fonction de rappel appel\u00e9e apr\u00e8s un seul fragment de phrase a \u00e9t\u00e9 synth\u00e9tis\u00e9e.</li> </ul>"},{"location":"es/fr/api/#before_sentence_synthesized-appelable","title":"before`_sentence_synthesized (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Une fonction de rappel qui est appel\u00e9e avant qu'un seul fragment de phrase ne soit synth\u00e9tis\u00e9.</li> </ul>"},{"location":"es/fr/api/#_audio_chunk-appelable","title":"<code>_audio_chunk</code> (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Fonction de rappel qui est appel\u00e9e lorsqu'un seul morceau audio est pr\u00eat.</li> </ul>"},{"location":"es/fr/api/#str","title":"```(str)","text":"<ul> <li>Par d\u00e9faut:<code>\"nltk\"</code></li> <li>Description : Tokenizer \u00e0 utiliser pour le fractionnement des phrases. Prend actuellement en charge \u00ab nltk \u00bb et \u00ab stroza \u00bb.</li> </ul>"},{"location":"es/fr/api/#_sentences-appelable","title":"<code>_sentences</code> (appelable)","text":"<ul> <li>Par d\u00e9faut: <code>None</code></li> <li>Description : Une fonction personnalis\u00e9e qui tokenise les phrases du texte saisi. Vous pouvez fournir votre propre tokenizer l\u00e9ger si vous n'\u00eates pas satisfait de nltk et stanza. Il doit prendre du texte comme cha\u00eene et renvoyer des phrases divis\u00e9es comme liste de cha\u00eenes.</li> </ul>"},{"location":"es/fr/api/#angustr","title":"<code>angu</code>(str)","text":"<ul> <li>Par d\u00e9faut:<code>\"en\"</code></li> <li>Description : Langue \u00e0 utiliser pour le fractionnement des phrases.</li> </ul>"},{"location":"es/fr/api/#_sizeint","title":"<code>_size</code>(int)","text":"<ul> <li>Par d\u00e9faut: <code>12</code></li> <li>Description : Le nombre de caract\u00e8res utilis\u00e9s pour \u00e9tablir le contexte pour la d\u00e9tection des limites de phrase. Un contexte plus large am\u00e9liore la pr\u00e9cision de la d\u00e9tection des limites des phrases.</li> </ul>"},{"location":"es/fr/api/#_size_look_overhead-int","title":"<code>_size_look_overhead</code> (int)","text":"<ul> <li>Par d\u00e9faut: <code>12</code></li> <li>Description : Taille de contexte suppl\u00e9mentaire pour regarder vers l'avenir lors de la d\u00e9tection des limites des phrases.</li> </ul>"},{"location":"es/fr/api/#mute-bool","title":"<code>mute</code> (bool)","text":"<ul> <li>Par d\u00e9faut: <code>False</code></li> <li>Description : Si vrai, d\u00e9sactive la lecture audio via des haut-parleurs locaux. Utile lorsque vous souhaitez synth\u00e9tiser dans un fichier ou traiter des morceaux audio sans les lire.</li> </ul>"},{"location":"es/fr/api/#ence_fragment_delimiters-str","title":"<code>ence_fragment_delimiters</code> (str)","text":"<ul> <li>Par d\u00e9faut:<code>\"?!;::\\n...)]}-</code></li> <li>Description : Une cha\u00eene de caract\u00e8res qui sont consid\u00e9r\u00e9s comme des d\u00e9limiteurs de phrases.</li> </ul>"},{"location":"es/fr/api/#_first_fragment_after_words-int","title":"<code>_first_fragment_after_</code>words (int)","text":"<ul> <li>Par d\u00e9faut: <code>15</code></li> <li>Description : Le nombre de mots apr\u00e8s lesquels le fragment de la premi\u00e8re phrase est forc\u00e9 d'\u00eatre donn\u00e9.</li> </ul>"},{"location":"es/fr/contributing/","title":"Contribuer \u00e0 RealtimeTTS","text":"<p>Nous accueillons les contributions \u00e0 RealtimeTTS ! Voici quelques fa\u00e7ons dont vous pouvez contribuer :</p> <ol> <li> <p>Reporting Bugs : Si vous trouvez un bug, veuillez ouvrir un probl\u00e8me sur notre r\u00e9f\u00e9rentiel GitHub.</p> </li> <li> <p>** Suggestion d'am\u00e9liorations** : Vous avez des id\u00e9es de nouvelles fonctionnalit\u00e9s ou d'am\u00e9liorations ? Nous serions ravis de les entendre ! Ouvrez un num\u00e9ro pour sugg\u00e9rer des am\u00e9liorations.</p> </li> <li> <p>Code Contributions : Vous voulez ajouter une nouvelle fonctionnalit\u00e9 ou corriger un bug ? Super ! Veuillez suivre ces \u00e9tapes :</p> </li> <li>Fourcher le d\u00e9p\u00f4t</li> <li>Cr\u00e9ez une nouvelle branche pour votre fonctionnalit\u00e9</li> <li>Faites vos changements</li> <li> <p>Soumettez une demande pull avec une description claire de vos modifications</p> </li> <li> <p>Documentation : Aidez-nous \u00e0 am\u00e9liorer notre documentation en corrigeant les fautes de frappe, en ajoutant des exemples ou en clarifiant les sections d\u00e9routantes.</p> </li> <li> <p>Ajout de nouveaux moteurs : Si vous souhaitez ajouter la prise en charge d'un nouveau moteur TTS, veuillez d'abord ouvrir un num\u00e9ro pour discuter de l'impl\u00e9mentation.</p> </li> </ol> <p>Merci d'avoir contribu\u00e9 \u00e0 rendre RealtimeTTS meilleur !</p>"},{"location":"es/fr/faq/","title":"Foire aux questions","text":"<p>Pour les r\u00e9ponses aux questions fr\u00e9quemment pos\u00e9es sur RealtimeTTS, veuillez vous r\u00e9f\u00e9rer \u00e0 notre page FAQ sur GitHub.</p> <p>Cette page couvre divers sujets dont</p> <ul> <li>Utilisation de diff\u00e9rents moteurs TTS</li> <li>Manipulation de textes multilingues</li> <li>Optimisation des performances</li> <li>D\u00e9pannage des probl\u00e8mes courants</li> </ul> <p>Pour des informations plus d\u00e9taill\u00e9es, veuillez consulter le lien ci-dessus.</p>"},{"location":"es/fr/installation/","title":"Fran\u00e7ais","text":"<p>Remarque: Installation de base avec <code>pip install realtimetts</code>s n'est plus recommand\u00e9, utilisez <code>pip install realtimetts[all]</code> \u00e0 la place.</p> <p>La biblioth\u00e8que RealtimeTTS offre des options d'installation pour diverses d\u00e9pendances pour votre cas d'utilisation. Voici les diff\u00e9rentes fa\u00e7ons dont vous pouvez installer RealtimeTTS en fonction de vos besoins :</p>"},{"location":"es/fr/installation/#installation-complete","title":"Installation compl\u00e8te","text":"<p>Pour installer RealtimeTTS avec prise en charge de tous les moteurs TTS :</p> <p><code>pip install -U realtimetts [tous]</code></p>"},{"location":"es/fr/installation/#installation-personnalisee","title":"Installation personnalis\u00e9e","text":"<p>RealtimeTTS permet une installation personnalis\u00e9e avec un minimum d'installations de biblioth\u00e8que. Voici les options disponibles : - all : Installation compl\u00e8te avec chaque moteur pris en charge. - ** syst\u00e8me : Inclut les capacit\u00e9s TTS sp\u00e9cifiques au syst\u00e8me (par exemple, pyttsx3). - azure : ajoute le support vocal Azure Cognitive Services. - elevenlabs : Comprend l'int\u00e9gration avec l'API ElevenLabs. - openai : Pour les services vocaux OpenAI. - gtts : Prise en charge de Google Text-to-Speech. - coqui : Installe le moteur Coqui TTS. - minimal** : installe uniquement les exigences de base sans moteur (n\u00e9cessaire uniquement si vous souhaitez d\u00e9velopper votre propre moteur)</p> <p>Supposons que vous souhaitiez installer RealtimeTTS uniquement pour l'utilisation neuronale locale de Coqui TTS, vous devez alors utiliser :</p> <p><code>pip installez realtimetts [coqui]</code></p> <p>Par exemple, si vous souhaitez installer RealtimeTTS avec uniquement Azure Cognitive Services Speech, ElevenLabs et la prise en charge d'OpenAI :</p> <p><code>pip installez realtimetts[azure,elevenlabs,openai]</code></p>"},{"location":"es/fr/installation/#installation-de-lenvironnement-virtuel","title":"Installation de l'environnement virtuel","text":"<p>Pour ceux qui souhaitent effectuer une installation compl\u00e8te dans un environnement virtuel, proc\u00e9dez comme suit</p> <p><code>python - m venv env_realtimetts env_realtimetts\\Scripts\\activate.bat python.exe - m pip install - upgrade pip pip install -U realtimetts [tous]</code></p> <p>Plus d'informations sur installation CUDA.</p>"},{"location":"es/fr/installation/#exigences-du-moteur","title":"Exigences du moteur","text":"<p>Diff\u00e9rents moteurs pris en charge par RealtimeTTS ont des exigences uniques. Assurez-vous de remplir ces exigences en fonction du moteur que vous choisissez.</p>"},{"location":"es/fr/installation/#moteur-systeme","title":"Moteur syst\u00e8me","text":"<p>Le `SystemEngine fonctionne d\u00e8s le d\u00e9part avec les capacit\u00e9s TTS int\u00e9gr\u00e9es de votre syst\u00e8me. Aucune configuration suppl\u00e9mentaire n'est n\u00e9cessaire.</p>"},{"location":"es/fr/installation/#gttsengine","title":"GTTSEngine","text":"<p>Le <code>GTTSEngine</code> fonctionne d\u00e8s le d\u00e9part \u00e0 l'aide de l'API de synth\u00e8se vocale de Google Translate. Aucune configuration suppl\u00e9mentaire n'est n\u00e9cessaire.</p>"},{"location":"es/fr/installation/#openaiengine","title":"OpenAIEngine","text":"<p>Pour utiliser le ``(OpenAIE): - d\u00e9finir la variable d'environnement OPENAI_API_KEY - installer ffmpeg (voir installation CUDA point 3)</p>"},{"location":"es/fr/installation/#azureengine","title":"AzureEngine","text":"<p>Pour utiliser le <code>ine</code>, vous aurez besoin de : - Cl\u00e9 API Microsoft Azure Text-to-Speech (fournie via le param\u00e8tre constructeur AzureEngine \u00ab speech_key \u00bb ou dans la variable d'environnement AZURE_SPEECH_KEY) - R\u00e9gion de service Microsoft Azure.</p> <p>Assurez-vous d'avoir ces informations d'identification disponibles et correctement configur\u00e9es lors de l'initialisation du <code>AzureEngine</code>.</p>"},{"location":"es/fr/installation/#elevenlabsengine","title":"ElevenlabsEngine","text":"<p>Pour le <code>ElevenlabsEngine</code>, vous avez besoin de: - Cl\u00e9 API Elevenlabs (fournie via le param\u00e8tre constructeur ElevenlabsEngine \u00ab api_key \u00bb ou dans la variable d'environnement ELEVENLABS_API_KEY) - <code>mpv</code> installed on your system (essential for streaming mpeg audio, Elevenlabs ne d\u00e9livre que mpeg).</p>"},{"location":"es/fr/installation/#elevenlabsengine_1","title":"ElevenlabsEngine","text":"<p>Pour le <code>ElevenlabsEngine</code>, vous avez besoin de: - Cl\u00e9 API Elevenlabs (fournie via le param\u00e8tre constructeur ElevenlabsEngine \u00ab api_key \u00bb ou dans la variable d'environnement ELEVENLABS_API_KEY) - <code>mpv</code> installed on your system (essential for streaming mpeg audio, Elevenlabs ne d\u00e9livre que mpeg).</p> <p>\ud83d\udd39 Installation <code>v</code>:   - macOS:     <code>infuser installer mpv</code></p> <ul> <li>Linux et Windows : Visitez mpv.io pour les instructions d'installation.</li> </ul>"},{"location":"es/fr/installation/#coquiengine","title":"CoquiEngine","text":"<p>Offre un TTS neuronal local de haute qualit\u00e9 avec clonage vocal.</p> <p>T\u00e9l\u00e9charge d'abord un mod\u00e8le TTS neuronal. Dans la plupart des cas, il est suffisamment rapide pour le temps r\u00e9el utilisant la synth\u00e8se GPU. N\u00e9cessite environ 4 \u00e0 5 Go de VRAM.</p> <ul> <li>pour cloner une voix, soumettez le nom de fichier d'un fichier d'onde contenant la voix source comme param\u00e8tre \u00ab voix \u00bb au constructeur CoquiEngine</li> <li>le clonage vocal fonctionne mieux avec un fichier WAV mono 16 bits de 22 050 Hz contenant un \u00e9chantillon court (~5 \u00e0 30 secondes)</li> </ul> <p>Sur la plupart des syst\u00e8mes, la prise en charge du GPU sera n\u00e9cessaire pour fonctionner suffisamment rapidement en temps r\u00e9el, sinon vous ferez l'exp\u00e9rience du b\u00e9gaiement.</p>"},{"location":"es/fr/installation/#installation-cuda","title":"Installation CUDA","text":"<p>Ces \u00e9tapes sont recommand\u00e9es pour ceux qui ont besoin de ** meilleures performances ** et disposent d'un GPU NVIDIA compatible.</p> <p>Remarque : pour v\u00e9rifier si votre GPU NVIDIA prend en charge CUDA, visitez la liste officielle des GPU CUDA.</p> <p>Pour utiliser une torche avec support via CUDA, veuillez suivre ces \u00e9tapes :</p> <p>Remarque : les installations de pythorque plus r\u00e9centes peuvent (non v\u00e9rifi\u00e9) n'ont plus besoin d'installation de Toolkit (et \u00e9ventuellement de cuDNN).</p> <ol> <li> <p>Installer NVIDIA CUDA Toolkit:     Par exemple, pour installer Toolkit 12.X, s'il te pla\u00eet</p> <ul> <li>Visitez NVIDIA CUDA T\u00e9l\u00e9chargements.</li> <li>S\u00e9lectionnez votre syst\u00e8me d'exploitation, votre architecture syst\u00e8me et votre version os.</li> <li>T\u00e9l\u00e9chargez et installez le logiciel.</li> </ul> <p>ou pour installer Toolkit 11.8, s'il vous pla\u00eet - Visitez Archive de la bo\u00eete \u00e0 outils CUDA NVIDIA. - S\u00e9lectionnez votre syst\u00e8me d'exploitation, votre architecture syst\u00e8me et votre version os. - T\u00e9l\u00e9chargez et installez le logiciel.</p> </li> <li> <p>Installer NVIDIA cuDNN:</p> <p>Par exemple, pour installer cuDNN 8.7.0 pour CUDA 11. x s'il vous pla\u00eet - Visitez NVIDIA cuDNN Archive. - Cliquez sur \u00ab T\u00e9l\u00e9charger cuDNN v8.7.0 (28 novembre 2022), pour CUDA 11.x \u00bb. - T\u00e9l\u00e9chargez et installez le logiciel.</p> </li> <li> <p>Installer ffmpeg:</p> <p>Vous pouvez t\u00e9l\u00e9charger un programme d'installation pour votre syst\u00e8me d'exploitation \u00e0 partir du site Web deffmpeg.</p> <p>Ou utilisez un gestionnaire de packages :</p> <ul> <li> <p>Sur Ubuntu ou Debian:     <code>sudo apt update &amp; &amp; sudo apt install ffmpeg</code></p> </li> <li> <p>Sur Arch Linux:     <code>sudo pacman -S ffmpeg</code></p> </li> <li> <p>Sur MacOS utilisant Homebrew (https://brew.sh/):     <code>infuser installer ffmpeg</code></p> </li> <li> <p>Sur Windows utilisant Chocolatey (https://chocolatey.org/):     <code>choco installer ffmpeg</code></p> </li> <li> <p>Sur Windows utilisant Scoop (https://scoop.sh/):     <code>scoop installer ffmpeg</code></p> </li> </ul> </li> <li> <p>Installez PyTorch avec le support CUDA :</p> <p>Pour mettre \u00e0 niveau votre installation PyTorch afin d'activer le support GPU avec CUDA, suivez ces instructions en fonction de votre version CUDA sp\u00e9cifique. Ceci est utile si vous souhaitez am\u00e9liorer les performances de RealtimeSTT avec les capacit\u00e9s CUDA.</p> <ul> <li> <p>Pour CUDA 11.8:</p> <p>Pour mettre \u00e0 jour PyTorch et Torchaudio afin de prendre en charge CUDA 11.8, utilisez les commandes suivantes :</p> <p><code>pip installe torch==2.3.1+cu118 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118</code></p> </li> <li> <p>Pour CUDA 12.X:</p> <p>Pour mettre \u00e0 jour PyTorch et Torchaudio pour prendre en charge CUDA 12.X, ex\u00e9cutez ce qui suit :</p> <p><code>pip installe torch==2.3.1+cu121 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121</code></p> </li> </ul> <p>Remplacer <code></code> ` of PyTorch that matching your system and requirements.</p> </li> <li> <p>** Correction pour r\u00e9soudre les probl\u00e8mes de compatibilit\u00e9** :     Si vous rencontrez des probl\u00e8mes de compatibilit\u00e9 de biblioth\u00e8que, essayez de d\u00e9finir ces biblioth\u00e8ques sur des versions fixes :</p> </li> </ol> <p>``` </p> <pre><code>pip install networkx==2.8.8\n\npip install typing_extensions==4.8.0\n\npip install fsspec==2023.6.0\n\npip install imageio==2.31.6\n\npip install networkx==2.8.8\n\npip install numpy==1.24.3\n\npip install requests==2.31.0\n</code></pre> <p>```</p>"},{"location":"es/fr/usage/","title":"Utilisation","text":""},{"location":"es/fr/usage/#demarrage-rapide","title":"D\u00e9marrage rapide","text":"<p>Voici un exemple d'utilisation de base :</p> <p><code>```(</code>python depuis RealtimeTTS import TextToAudioStream, SystemEngine, AzureEngine, ElevenlabsEngine</p> <p>moteur = SystemEngine () # remplacer par votre moteur TTS flux = TextToAudioStream(moteur) stream.feed(\"Bonjour le monde! Comment \u00e7a va aujourd'hui ?\") stream.play_async() ``</p>"},{"location":"es/fr/usage/#flux-texte","title":"Flux Texte","text":"<p>Vous pouvez alimenter des cha\u00eenes individuelles :</p> <p><code>``(`python stream.feed(\u00ab Bonjour, c'est une phrase. \u00bb)</code></p> <p>Ou vous pouvez alimenter des g\u00e9n\u00e9rateurs et des it\u00e9rateurs de caract\u00e8res pour le streaming en temps r\u00e9el :</p> <p><code>```(</code>python def write (prompt : str) :     pour chunk en openai.ChatCompletion.create(         mod\u00e8le=\"gpt-3.5-turbo\",         messages=[{\"role\": \"utilisateur\", \"contenu\" : prompt}],         stream=True     ):         si (text_chunk := chunk[\u00ab choix \u00bb][0][\u00ab delta \u00bb].get(\u00ab contenu \u00bb)) n'est pas Aucun :             produire du texte_chunk</p> <p>text_stream = write (\u00ab Un discours relaxant en trois phrases \u00bb)</p> <p>stream.feed(text_stream) ``</p> <p><code>``(`python char_iterator = iter (\u00ab Diffusion de ce personnage par personnage \u00bb) stream.feed (char_iterator)</code></p>"},{"location":"es/fr/usage/#layback","title":"Layback","text":"<p>Asynchrone:</p> <p><code>``(`python stream.play_async() pendant que stream.is_playing():     temps.sommeil(0,1)</code></p> <p>Synchronis\u00e9:</p> <p><code>``(`python stream.play()</code></p>"},{"location":"es/fr/usage/#tester-la-bibliotheque","title":"Tester la biblioth\u00e8que","text":"<p>Le sous-r\u00e9pertoire de test contient un ensemble de scripts pour vous aider \u00e0 \u00e9valuer et comprendre les capacit\u00e9s de la biblioth\u00e8que RealtimeTTS.</p> <p>Notez que la plupart des tests reposent toujours sur l'\u00ab ancienne \u00bb API OpenAI (&lt;1.0.0). L'utilisation de la nouvelle API OpenAI est d\u00e9montr\u00e9e dans openai_1.0_test.py.</p> <ul> <li> <p>simple_test.py</p> <ul> <li>Description : Une d\u00e9monstration de style \u00ab hello world \u00bb de l'usage le plus simple de la biblioth\u00e8que.</li> </ul> </li> <li> <p>complex_test.py</p> <ul> <li>Description : Une d\u00e9monstration compl\u00e8te pr\u00e9sentant la plupart des fonctionnalit\u00e9s fournies par la biblioth\u00e8que.</li> </ul> </li> <li> <p>coqui_test.py</p> <ul> <li>Description : Test du moteur local coqui TTS.</li> </ul> </li> <li> <p>traducteur.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai realtimestt</code>.</li> <li>Description : Traductions en temps r\u00e9el dans six langues diff\u00e9rentes.</li> </ul> </li> <li> <p>openai_voice_interface.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai realtimestt</code>.</li> <li>Description : Interface utilisateur activ\u00e9e par mot de r\u00e9veil et bas\u00e9e sur la voix vers l'API OpenAI.</li> </ul> </li> <li> <p>advanced_talk.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai keyboard realtimestt</code>.</li> <li>Description : Choisissez le moteur et la voix TTS avant de d\u00e9marrer la conversation sur l'IA.</li> </ul> </li> <li> <p>_talkbot.py minimaliste</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai realtimestt</code>.</li> <li>Description : Un talkbot basique en 20 lignes de code.</li> </ul> </li> <li> <p>simple_llm_test.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai</code>.</li> <li>Description : D\u00e9monstration simple de la fa\u00e7on d'int\u00e9grer la biblioth\u00e8que avec de grands mod\u00e8les de langage (LLM).</li> </ul> </li> <li> <p>test_callbacks.py</p> <ul> <li>D\u00e9pendances: Ex\u00e9cuter <code>pip install openai</code>.</li> <li>Description : pr\u00e9sente les rappels et vous permet de v\u00e9rifier les temps de latence dans un environnement d'application r\u00e9el.</li> </ul> </li> </ul>"},{"location":"es/fr/usage/#mettre-en-pause-reprendre-et-arreter","title":"Mettre en pause, reprendre et arr\u00eater","text":"<p>Mettre en pause le flux audio :</p> <p><code>``(`python stream.pause()</code></p> <p>Reprendre un flux en pause :</p> <p><code>``(`python stream.reprendre()</code></p> <p>Arr\u00eatez imm\u00e9diatement le flux :</p> <p><code>``(`python stream.stop()</code></p>"},{"location":"es/fr/usage/#exigences-expliquees","title":"Exigences expliqu\u00e9es","text":"<ul> <li>Version Python:</li> <li>Obligatoire: Python &gt;= 3.9, &lt; 3.13</li> <li> <p>Raison : La biblioth\u00e8que d\u00e9pend de la biblioth\u00e8que GitHub \u00ab TTS \u00bb de coqui, qui n\u00e9cessite des versions Python dans cette gamme.</p> </li> <li> <p>PyAudio : pour cr\u00e9er un flux audio de sortie</p> </li> <li> <p>stream2sent : pour diviser le flux de texte entrant en phrases</p> </li> <li> <p>pyttsx3 : Moteur de conversion texte-parole du syst\u00e8me</p> </li> <li> <p>pydub : pour convertir les formats de morceaux audio</p> </li> <li> <p>azure-cognitiveservices-speech : Moteur de conversion texte-parole azur</p> </li> <li> <p>elevenlabs : Moteur de conversion texte-parole Elevenlabs</p> </li> <li> <p>coqui-TTS : Biblioth\u00e8que de synth\u00e8se vocale XTTS de Coqui pour un TTS neuronal local de haute qualit\u00e9</p> </li> </ul> <p>Criez \u00e0 Idiap Research Institute pour entretenir une fourche de coqui tts.</p> <ul> <li> <p>openai : pour interagir avec l'API TTS d'OpenAI</p> </li> <li> <p>gtts : Google traduit la conversion texte-parole</p> </li> </ul>"}]}